
<!doctype html>
<html lang="fi" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../rnn/">
      
      
        <link rel="next" href="../../aikasarjat/ideat/">
      
      
        
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.7.2">
    
    
      
        <title>Transformers - Syväoppiminen I</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.484c7ddc.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.ab4e12ef.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#transformers" class="md-skip">
          Hyppää sisältöön
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Ylätunniste">
    <a href="../.." title="Syväoppiminen I" class="md-header__button md-logo" aria-label="Syväoppiminen I" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Syväoppiminen I
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Transformers
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Hae" placeholder="Hae" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Haku">
        
        <button type="reset" class="md-search__icon md-icon" title="Tyhjää" aria-label="Tyhjää" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Aloitetaan hakua
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigaatio" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Syväoppiminen I" class="md-nav__button md-logo" aria-label="Syväoppiminen I" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Syväoppiminen I
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Tervetuloa kurssille
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    1. Neuroverkot
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    1. Neuroverkot
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../neuroverkot/neuroverkot_101/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Neuroverkot
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../neuroverkot/syvaoppiminen_FC/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Syvät neuroverkot
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    2. Tensorit
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            
  
    2. Tensorit
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tensorit/vektorointi/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Vektorointi
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tensorit/pytorch/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    PyTorch
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    3. Vastavirta
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            
  
    3. Vastavirta
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../vastavirta/backpropagation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Vastavirta (Backprop)
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    4. Mallinnus
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            
  
    4. Mallinnus
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../mallinnus/yleiskatsaus/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Yleiskatsaus
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../mallinnus/datanlataus/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Datan lataus
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../mallinnus/kaytannot/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Kouluttamisen käytännöt
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    5. Konvoluutio
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            
  
    5. Konvoluutio
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../konvoluutio/cnn/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Konvoluutioverkot
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    6. Siirtovaikutus
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            
  
    6. Siirtovaikutus
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../siirtovaikutus/pretrained/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Koulutetun mallin käyttö
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../siirtovaikutus/transferlearning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Siirtovaikutus
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_8" checked>
        
          
          <label class="md-nav__link" for="__nav_8" id="__nav_8_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    7. Kieli
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_8_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_8">
            <span class="md-nav__icon md-icon"></span>
            
  
    7. Kieli
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../nlp/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Luonnollinen kieli
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../rnn/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    RNN ja jälkeläiset
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    
  
    Transformers
  

    
  </span>
  
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    
  
    Transformers
  

    
  </span>
  
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Sisällysluettelo">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Sisällysluettelo
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#motivaatio-rnnn-haasteet" class="md-nav__link">
    <span class="md-ellipsis">
      
        Motivaatio: RNN:n haasteet
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#attention" class="md-nav__link">
    <span class="md-ellipsis">
      
        Attention
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Attention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#query-key-ja-value-q-k-v" class="md-nav__link">
    <span class="md-ellipsis">
      
        Query, Key ja Value (Q, K, V)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#transformer-arkkitehtuuri" class="md-nav__link">
    <span class="md-ellipsis">
      
        Transformer-arkkitehtuuri
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Transformer-arkkitehtuuri">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#self-attention" class="md-nav__link">
    <span class="md-ellipsis">
      
        Self-Attention
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multi-head-self-attention" class="md-nav__link">
    <span class="md-ellipsis">
      
        Multi-Head Self-Attention
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#positional-encoding" class="md-nav__link">
    <span class="md-ellipsis">
      
        Positional Encoding
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#toiminta-kokonaisuutena" class="md-nav__link">
    <span class="md-ellipsis">
      
        Toiminta kokonaisuutena
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#arkkitehtuurin-variaatiot" class="md-nav__link">
    <span class="md-ellipsis">
      
        Arkkitehtuurin variaatiot
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mallien-arviointi-metriikat" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mallien arviointi (Metriikat)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Mallien arviointi (Metriikat)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#perplexity" class="md-nav__link">
    <span class="md-ellipsis">
      
        Perplexity
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bleu" class="md-nav__link">
    <span class="md-ellipsis">
      
        BLEU
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rouge" class="md-nav__link">
    <span class="md-ellipsis">
      
        ROUGE
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bertscore" class="md-nav__link">
    <span class="md-ellipsis">
      
        BERTScore
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#tehtavat" class="md-nav__link">
    <span class="md-ellipsis">
      
        Tehtävät
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lahteet" class="md-nav__link">
    <span class="md-ellipsis">
      
        Lähteet
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_9" >
        
          
          <label class="md-nav__link" for="__nav_9" id="__nav_9_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    8. Aikasarjat
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_9_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_9">
            <span class="md-nav__icon md-icon"></span>
            
  
    8. Aikasarjat
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../aikasarjat/ideat/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Aikasarjat
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../exercises/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Tehtäväkooste
  

    
  </span>
  
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Sisällysluettelo">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Sisällysluettelo
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#motivaatio-rnnn-haasteet" class="md-nav__link">
    <span class="md-ellipsis">
      
        Motivaatio: RNN:n haasteet
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#attention" class="md-nav__link">
    <span class="md-ellipsis">
      
        Attention
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Attention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#query-key-ja-value-q-k-v" class="md-nav__link">
    <span class="md-ellipsis">
      
        Query, Key ja Value (Q, K, V)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#transformer-arkkitehtuuri" class="md-nav__link">
    <span class="md-ellipsis">
      
        Transformer-arkkitehtuuri
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Transformer-arkkitehtuuri">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#self-attention" class="md-nav__link">
    <span class="md-ellipsis">
      
        Self-Attention
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multi-head-self-attention" class="md-nav__link">
    <span class="md-ellipsis">
      
        Multi-Head Self-Attention
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#positional-encoding" class="md-nav__link">
    <span class="md-ellipsis">
      
        Positional Encoding
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#toiminta-kokonaisuutena" class="md-nav__link">
    <span class="md-ellipsis">
      
        Toiminta kokonaisuutena
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#arkkitehtuurin-variaatiot" class="md-nav__link">
    <span class="md-ellipsis">
      
        Arkkitehtuurin variaatiot
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mallien-arviointi-metriikat" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mallien arviointi (Metriikat)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Mallien arviointi (Metriikat)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#perplexity" class="md-nav__link">
    <span class="md-ellipsis">
      
        Perplexity
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bleu" class="md-nav__link">
    <span class="md-ellipsis">
      
        BLEU
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rouge" class="md-nav__link">
    <span class="md-ellipsis">
      
        ROUGE
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bertscore" class="md-nav__link">
    <span class="md-ellipsis">
      
        BERTScore
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#tehtavat" class="md-nav__link">
    <span class="md-ellipsis">
      
        Tehtävät
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lahteet" class="md-nav__link">
    <span class="md-ellipsis">
      
        Lähteet
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="transformers">Transformers</h1>
<p>Aloitetaan töksäyttämällä heti alkuun määritelmä siitä, mikä transformer on: se on <a href="https://arxiv.org/abs/1706.03762">Attention is All You Need</a>-artikkelissa vuonna 2017 esitelty neuroverkkopohjainen arkkitehtuuri, joka on suunniteltu erityisesti käsittelemään kieltä, mutta arkkitehtuuri on sittemmin taipunut myös muun datan käsittelyyn. Artikkelin tiivistelmä kertoo paljon:</p>
<blockquote>
<p>"We propose a new simple network architecture, the Transformer,
based solely on attention mechanisms, dispensing with recurrence and convolutions
entirely. Experiments on two machine translation tasks show these models to
be superior in quality while being more parallelizable and requiring significantly
less time to train."</p>
<p>– Ashish Vaswani et. al. <sup id="fnref3:attention2017"><a class="footnote-ref" href="#fn:attention2017">1</a></sup></p>
</blockquote>
<div class="admonition danger">
<p class="admonition-title">Danger</p>
<p>Termeissä sekaannusvaara. Yleisesti käytetty Hugging Facen Python-kirjasto on nimeltään <code>transformers</code>, mutta se ei ole sama asia kuin transformer-arkkitehtuuri. Hugging Facen <code>transformers</code>-kirjasto on nimetty sen mukaan, että se tarjoaa työkaluja transformer-pohjaisten mallien rakentamiseen ja hyödyntämiseen.</p>
</div>
<h2 id="motivaatio-rnnn-haasteet">Motivaatio: RNN:n haasteet</h2>
<blockquote>
<p>"The big limitation of encoder–decoder RNNs is that the RNN can’t directly access earlier hidden states from the encoder during the decoding phase. Consequently, it relies solely on the current hidden state, which encapsulates all relevant information. This can lead to a loss of context, especially in complex sentences where dependencies might span long distances."</p>
<p>— Sebastian Raschka <sup id="fnref3:llmfromscratch"><a class="footnote-ref" href="#fn:llmfromscratch">2</a></sup></p>
</blockquote>
<p>Edellisessä luvussa opit, että LSTM ja GRU mallit paikkasivat RNN:n ongelmia, mutta eivät täysin ratkaisseet niitä. Erityisesti pitkät riippuvuudet ja kontekstin vaikutus sanan merkitykseen aiheuttivat haasteita. Palaako se kuusi vai palaako kuusi? Jääneitä ongelmia olivat siis ainakin <sup id="fnref:buildingaiagents"><a class="footnote-ref" href="#fn:buildingaiagents">3</a></sup>:</p>
<ul>
<li><strong>Kohdistus (engl. <em>alignment</em>):</strong> Käännöksessä eri kielten sanajärjestys ja rakenteet vaativat kohdennusta. Esimerkiksi <code>the cat</code> kummatkin sanat viittaavat suomenkielisessä käännöksessä many-to-one -hengessä sanaan <code>kissa</code>.</li>
<li><strong>Katoavat tai räjähtävät gradientit:</strong> Vaikka LSTM toi ratkaisuja, ongelma ei poistunut.</li>
<li><strong>Ei-parallelisoitavuus:</strong> RNN:n aikaisemmat tilat vaikuttavat nykyiseen tilaan, mikä estää tehokkaan rinnakkaisprosessoinnin.</li>
</ul>
<p>Transformer-arkkitehtuuri, esitelty vuonna 2017, mullisti tätä siten, että <em>attention</em>-mekanismilla korvattiin <em>recurrence</em> eli RNN:stä tuttu syklisyys kokonaan. <sup id="fnref:dlwithpython"><a class="footnote-ref" href="#fn:dlwithpython">4</a></sup> Attention ei itsessään ole uusi keksintö: voit kurkata siihen liittyvää historiaa <a href="https://en.wikipedia.org/wiki/Attention_(machine_learning)">Attention (machine learning)</a>-Wikipedia-artikkelista. Tutustutaan alla tarkemmin siihen, mitä <em>attention</em> on.</p>
<h2 id="attention">Attention</h2>
<p>On hyvä pohtia heti alkuun motivaatiota: miksi tarvitsemme attentionia? Olet tutustunut RNN:n kohdalla encoder-decoder-arkkitehtuuriin, jossa encoderin tehtävänä on tiivistää koko syötesekvenssi yhdeksi vektoriksi, joka sitten dekoodataan tulosteeksi. Tämän tiivistyksen tulisi sisältää <mark>kaikki oleennainen informaatio</mark> syötteestä. <sup id="fnref:ldl"><a class="footnote-ref" href="#fn:ldl">5</a></sup></p>
<pre class="mermaid"><code>graph LR
    A[Input Sequence] --&gt; B[Encoder RNN]
    B --&gt; C[Context Vector]
    C --&gt; D[Decoder RNN]
    D --&gt; E[Output Sequence]

    %% styling
    classDef orange fill:#F39C12,color:#000,stroke:#F39C12;
    class C orange;</code></pre>
<p>Sanan kohdalla <em>embedding</em> tai <em>context vector</em> on jo tämän kurssin harjoituksissa osoittautunut hyödylliseksi, viitaten siihen, että Zellig Harrisin <em>distribution hypothesis</em>-teorialla on paikkansa maailmassa. Entäpä kun pitää tiivistää kokonaisuus, joka on merkittävästi monimutkaisempi kuin yksittäinen sana (tai sanan osatekijä, wordpiece)? Tarkastellaan seuraavaa kaoottista virkettä:</p>
<blockquote>
<p>"Nimikirjassa oleva merkintä virantoimituksesta pidättämisestä on poistettava, jos virantoimituksesta pidättämistä koskeva päätös tai virantoimituksesta pidättämisen perusteena ollut irtisanominen tai virkasuhteen purkamista koskeva päätös kumotaan taikka virkamiehen virantoimituksesta pidättämisen syynä olleessa oikeudenkäynnissä ei ole todettu syyllistyneen rangaistavaan tekoon."</p>
<p>— 1322/89, § 3 <sup id="fnref:nimikirja"><a class="footnote-ref" href="#fn:nimikirja">6</a></sup></p>
</blockquote>
<p>Kykenetkö tulkitsemaan lauseen sisällön kertalukemalla vasemmalta oikealle, vai joudutko välillä palaamaan taaksepäin? Enkooder-dekooder-malli ja sen <em>context embedding</em> ei välttämättä juuri edusta sitä, kuinka ihminen tulkitsee tekstikokonaisuuden. Ihminen esimerkiksi silmäilee edes-takaisin ja tulkitsee yksittäisten sanojen merkityksen kontekstin valossa. <mark>Attention voidaan nähdä tämän prosessin mimikointina</mark>. <sup id="fnref2:ldl"><a class="footnote-ref" href="#fn:ldl">5</a></sup></p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Kyseinen lause on poimittu Kielikellon artikkelista, jossa käsitellään säädöskielen virke- ja lauserakenteen ongelmia. Käy kurkkaamassa, kuinka Kielikellon artikkelissa sama lause on selkeytetty listan avulla. <sup id="fnref:kielikello"><a class="footnote-ref" href="#fn:kielikello">7</a></sup></p>
</div>
<p>Tavallisen, aiemmin kurssilta tutun seq2seq RNN:n <strong>kiinteän pituuden kontekstivektorista</strong> tulee siis pullonkaula <sup id="fnref:ml-algos-depth"><a class="footnote-ref" href="#fn:ml-algos-depth">8</a></sup> <sup id="fnref:llmfromscratch"><a class="footnote-ref" href="#fn:llmfromscratch">2</a></sup>. Attention-mekanismi tarjoaa tähän ratkaisuna <strong>dynaamisen kontekstivektorin</strong>, joka lasketaan uudestaan joka ikiselle dekooderin aika-askeleelle <sup id="fnref2:buildingaiagents"><a class="footnote-ref" href="#fn:buildingaiagents">3</a></sup>. Tällöin Attention Decoder voi itsenäisesti päättää, mihin se keskittää huomionsa dekoodatessaan käännöstä.</p>
<blockquote>
<p>"In simple words, during translation, we have a context vector that is dynamically updated and tells us how much attention we should give to each part of the input sequence."</p>
<p>— Raieli &amp; Iuculano <sup id="fnref5:buildingaiagents"><a class="footnote-ref" href="#fn:buildingaiagents">3</a></sup></p>
</blockquote>
<p><img alt="" src="../../images/720_attention_unrolled.png" /></p>
<p><strong>Kuva 1:</strong> <em>Kuvassa näkyy vasemmalla enkooderi-RNN ja oikealla dekooderi-RNN. Näiden välissä on attention-mekanismi, joka saa syötteenä kaikkien aika-askeleiden piilotetut tilat (ks. koodi yllä). Kullekin dekooderin aika-askeleelle lasketaan kohdistuspisteet (alignment scores), jotka kuvaavat sitä, kuinka hyvin dekooderin nykyinen tila vastaa kutakin enkooderin tilaa. Kohdistuspisteet voidaan laskea monella tavalla, joista yksi on pistetulo (eli cosine similarity ilman magnitude-jakajaa).</em> <sup id="fnref3:ldl"><a class="footnote-ref" href="#fn:ldl">5</a></sup></p>
<h4 id="query-key-ja-value-q-k-v">Query, Key ja Value (Q, K, V)</h4>
<p>Attention-mekanismissa käytetyt termit <em>Query</em>, <em>Key</em> ja <em>Value</em> on lainattu tiedonhaun ja tietokantojen maailmasta, joissa niitä käytetään informaation järjestämiseen ja hakemiseen. Kuten Géron kirjoittaa, nämä termit ovat modernin implementaation mukaisia termejä aiemmin esitellylle. <sup id="fnref:geronpytorch"><a class="footnote-ref" href="#fn:geronpytorch">9</a></sup></p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="k">def</span><span class="w"> </span><span class="nf">attention</span><span class="p">(</span><span class="n">QUERY</span><span class="p">,</span> <span class="n">KEY</span><span class="p">,</span> <span class="n">VALUE</span><span class="p">):</span>
</span><span id="__span-0-2"><a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>    <span class="c1"># Calculate alignment scores (e.g., dot product)</span>
</span><span id="__span-0-3"><a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>    <span class="n">scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">QUERY</span><span class="p">,</span> <span class="n">KEY</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="c1"># (1)!</span>
</span><span id="__span-0-4"><a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>
</span><span id="__span-0-5"><a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>    <span class="c1"># Normalize scores to get attention weights</span>
</span><span id="__span-0-6"><a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a>    <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
</span><span id="__span-0-7"><a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a>
</span><span id="__span-0-8"><a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a>    <span class="c1"># Compute the attention vector as a weighted sum of values</span>
</span><span id="__span-0-9"><a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a>    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">,</span> <span class="n">VALUE</span><span class="p">)</span> <span class="c1"># (2)!</span>
</span></code></pre></div>
<ol>
<li>Query on sama kuin <span class="arithmatex">\(h_t\)</span> dekooderin nykyisessä aika-askeleessa. Key on sama kuin <span class="arithmatex">\(h_s\)</span> enkooderin aika-askeleissa.</li>
<li>Value on <span class="arithmatex">\(h_s\)</span> enkooderin aika-askeleissa, aivan kuten Key, mutta se edustaa sitä informaatiota, jota dekooderi hyödyntää.</li>
</ol>
<p>Intuition tasolla nämä kolme termiä voidaan ymmärtää seuraavasti:</p>
<ul>
<li><strong>Query (Kysely):</strong> Vastaa tietokantahakua. <sup id="fnref4:llmfromscratch"><a class="footnote-ref" href="#fn:llmfromscratch">2</a></sup></li>
<li><strong>Key (Avain):</strong> Toimii kuten tietokannan avain indeksoinnissa. Sekvenssin kaikilla jäsenillä on oma avain. <sup id="fnref5:llmfromscratch"><a class="footnote-ref" href="#fn:llmfromscratch">2</a></sup></li>
<li><strong>Value (Arvo):</strong>  Vastaa avain-arvo -parin varsinaista sisältöä tietokannassa. <sup id="fnref6:llmfromscratch"><a class="footnote-ref" href="#fn:llmfromscratch">2</a></sup></li>
</ul>
<p>Value edustaa siis haluttua informaatiota. Kun malli on avainten perusteella päätellyt, mitkä syötteen osat ovat olennaisimpia kyselylle, se hakee hyödynnettäväksi niitä vastaavat arvot. (BY-NC-ND) <sup id="fnref2:llmfromscratch"><a class="footnote-ref" href="#fn:llmfromscratch">2</a></sup></p>
<p><img alt="" src="../../images/720_TransformerBlockSACross.svg" /></p>
<p><strong>Kuva 2:</strong> <em>Kuvan Q, V ja K laskelmat ovat Transformers-arkkitehtuurista, mutta sama logiikka pätee myös RNN:n enkooderi-dekooderi-arkkitehtuurissa. Huomaa, että Q, K ja V lasketaan lineaarisella projektiolla syöte-embeddingeistä. Tässä laskelma on matriisimuodossa. <span class="arithmatex">\(N\)</span> on input-vektorin dimensio eli sanojen määrä. <span class="arithmatex">\(D\)</span> on embedding-koko. Softmax ajetaan sarakekohtaisesti.</em> <sup id="fnref:udlbook"><a class="footnote-ref" href="#fn:udlbook">10</a></sup></p>
<p>Näillä kaikilla kolmella – Q, K, V – on erilaiset roolit, mutta niiden kaikkien laskenta on samankaltaista: ne saadaan lineaarisella projektiolla syöte-embeddingeistä. Tämä tarkoittaa, että syötteen embeddingit muunnetaan kolmeen eri avaruuteen (Q, K ja V) käyttämällä kolmea erillistä painomatriisia. Näin malli oppii erottamaan, miten se hakee tietoa (Q), miten se vertaa sitä syötteeseen (K) ja mitä tietoa se lopulta hyödyntää (V). <sup id="fnref:transformers-def-guide"><a class="footnote-ref" href="#fn:transformers-def-guide">11</a></sup></p>
<h2 id="transformer-arkkitehtuuri">Transformer-arkkitehtuuri</h2>
<p>Alkuperäisen Attention is All you Need -julkaisun transformer-arkkitehtuuri on pähkinänkuoressa <sup id="fnref4:ldl"><a class="footnote-ref" href="#fn:ldl">5</a></sup> <sup id="fnref:attention2017"><a class="footnote-ref" href="#fn:attention2017">1</a></sup>:</p>
<ul>
<li>Encoder-Decoder-arkkitehtuuri, jossa on attention-mekanismi.</li>
<li>... tarkemmin multi-head self-attention.</li>
<li>Positionaalinen koodaus mallintaa sanajärjestystä.</li>
<li>Ei rekurrenssia, vaan kaikki laskelmat tehdään rinnakkain.</li>
</ul>
<p>Tutustutaan alla sen pariin merkittävimpään osatekijään otsikko kerrallaan.</p>
<h3 id="self-attention">Self-Attention</h3>
<p>Attention-mekanismin menestys on synnyttänyt useita variaatioita eri tappiofunktioita käyttäen. Näistä erityisesti <em>self-attention</em> on merkittävä, sillä se poimii informaatiota suoraan syötteestä itsestään ilman tarvetta verrata sitä mihinkään ulkoiseen tietoon. <sup id="fnref3:buildingaiagents"><a class="footnote-ref" href="#fn:buildingaiagents">3</a></sup> Toisin sanoen linkki dekooderiin katkaistaan: myös <span class="arithmatex">\(Q\)</span> tulee syötteestä; ei enkooderin piilotiloista.</p>
<blockquote>
<p>"Self-attention is the key component of transformer architecture. A transformer is a Seq2Seq model that uses attention in the encoder as well as the decoder, thus eliminating the need for RNNs"</p>
<p>— Smolyakov <sup id="fnref2:ml-algos-depth"><a class="footnote-ref" href="#fn:ml-algos-depth">8</a></sup></p>
</blockquote>
<p><img alt="" src="../../images/720_TransformerBlockSA.svg" /></p>
<p><strong>Kuva 3:</strong> <em>Self-attention-mekanismissa sama sekvenssi toimii sekä queryn, keyn että valuen laskennan perustana. Tämä katkaisee kytköksen decoderiin. (BY-NC-ND)</em> <sup id="fnref2:udlbook"><a class="footnote-ref" href="#fn:udlbook">10</a></sup></p>
<p>Self-attentionin perusajatusta voidaan havainnollistaa kirjastometaforalla. Kuvittele, että etsit kirjastosta tietoa <strong>Marsin kolonialisointia</strong> käsittelevää esseetä varten (<em>Query</em>). Sinun ei tarvitse lukea jokaista hyllyssä olevaa kirjaa kannesta kanteen (<em>Value</em>) löytääksesi oikean teoksen. Sen sijaan selaat kirjojen selkämyksiä ja otsikoita (<em>Key</em>) löytääksesi ne, jotka vastaavat hakua. Toisin sanoen, self-attention on menetelmä, jonka avulla malli voi etsiä kontekstista juuri sen tiedon tai "edustuksen", jota se sillä hetkellä tarvitsee. <sup id="fnref4:buildingaiagents"><a class="footnote-ref" href="#fn:buildingaiagents">3</a></sup></p>
<div class="admonition danger">
<p class="admonition-title">Danger</p>
<p>Termien sekaannusvaara. <mark>Attention</mark>-mekanismit mahdollistavat mallien keskittymisen eri osiin sekvenssejä, joita kutsutaan queryksi, keyksi ja valueksi. <mark>Self-attention</mark> puolestaan viittaa erityisesti mekanismiin, jossa sama sekvenssi toimii queryn, keyn ja valuen laskennan perustana, mikä mahdollistaa sisäisten suhteiden ymmärtämisen sekvenssin sisällä. <sup id="fnref:azure-book"><a class="footnote-ref" href="#fn:azure-book">12</a></sup></p>
</div>
<h3 id="multi-head-self-attention">Multi-Head Self-Attention</h3>
<p>Jos kytket samaan inputtiin monta paralleelia self-attention blokkia, sinulla on <strong>multi-head</strong>-ratkaisu käsissäsi. Kullakin niistä on oma <span class="arithmatex">\(Q\)</span>, <span class="arithmatex">\(K\)</span> ja <span class="arithmatex">\(V\)</span> matriisi ja kukin oppii omat painonsa. Lopuksi lähdöt ketjutetaan (engl. concatenate) ja yhdistetään taas kerran uudella lineaarisella projisiolla – eli tarvitaan uusia koulutettavia parametreja. Kurssikirjassa multi-headin merkitystä kuvataan näin: <em>"Multiple heads seem to be necessary to make self-attention work well. It has been speculated that they make the self-attention network more robust to bad initializations."</em> <sup id="fnref3:udlbook"><a class="footnote-ref" href="#fn:udlbook">10</a></sup></p>
<p>Voita ja kumppanit tutkivat 2019 asiaa, ja päättelivät, että <em>"only a small subset of heads appear to be important for the translation task. Important heads have one or more interpretable functions in the model, including attending to adjacent
words and tracking specific syntactic relations.</em> <sup id="fnref:voita2019"><a class="footnote-ref" href="#fn:voita2019">13</a></sup>. Eli eri päät löytävät erilaisia tokeneiden välisiä kytköksiä, mutta kaikki niistä eivät ole välttämättä merkityksellisiä, joten mallin parametrikokoa voi pienentää <em>pruning</em>-tekniikalla: eli katkomalla päitä kuin Hercules Hydralta.</p>
<p><img alt="" src="../../images/720_TransformerBlock.svg" /></p>
<p><strong>Kuva 4:</strong> <em>Tähän mennessä vastaan tulleet osatekijät kun kytkee yhteen, ja ujuttaa väliin layer normalizationit regulaatoimaan, samme kuvassa näkyvän transformer-blokin: multi-head self-attention, layer normalization, MLP sekä vielä yksi layer normalization. Näitä voi kytkeä peräkkäin samalla tavalla kuin vaikkapa aiemmin nähtyjä Conv2D-blokkeja. (BY-NC-ND) <sup id="fnref4:udlbook"><a class="footnote-ref" href="#fn:udlbook">10</a></sup></em></p>
<p>Se, mitä tässä materiaaleissa ei käsitellä, on <mark>masked</mark> multi-head self-attention, joka on osa Transformer-dekoorin rakennetta.</p>
<h3 id="positional-encoding">Positional Encoding</h3>
<p>Transformer-arkkitehtuurissa kaikki laskennat tehdään rinnakkain, paralleelisti, mikä ajaa siihen, että RNN:n sekvenssijärjestyksen tuoma sanajärjestys kadotetaan. Ongelma ratkaistaan lisäämällä <em>positional encoding</em> kuhunkin embedding-vektoriin. Tämä encoding on vektori, joka ynnätään elementti elementiltä syötteeseen. Eli jos syötteen embedding on <span class="arithmatex">\(e\)</span> ja positionaalinen encoding on <span class="arithmatex">\(p\)</span>, syötteen embedding muunnetaan <span class="arithmatex">\(e' = e + p\)</span>. <sup id="fnref5:ldl"><a class="footnote-ref" href="#fn:ldl">5</a></sup> Tämän järjestystä kuvaavan vektorin voi joko oppia koulutuksessa tai laskea. Alkuperäisessä artikkelissa käytettiin laskukaavaa, joka hyödyntää sinin ja kosinin funktioita eri taajuuksilla. <sup id="fnref2:attention2017"><a class="footnote-ref" href="#fn:attention2017">1</a></sup> Tämä kaava on poikkeaa hieman parillisten ja parittomien indeksien osalta, mutta perusidea on sama: eri taajuuksilla olevat sinit ja kosinit luovat uniikkeja positionaalisia koodeja, jotka auttavat mallia erottamaan sanojen järjestyksen. Kaava on seuraava:</p>
<div class="arithmatex">\[
element_{(pos, i)} = \begin{cases}
\sin\left(\frac{pos}{10000^{\frac{i}{d}}}\right) &amp; \text{if } i \text{ is even} \\
\cos\left(\frac{pos}{10000^{\frac{i-1}{d}}}\right) &amp; \text{if } i \text{ is odd}
\end{cases}
\]</div>
<p>Jossa:</p>
<ul>
<li><span class="arithmatex">\(pos\)</span> on sanan paikka sekvenssissä (esim. 0, 1, 2, ...)</li>
<li><span class="arithmatex">\(i\)</span> on embedding-vektorin elementin indeksi (esim. 0, 1, 2, ...)</li>
<li><span class="arithmatex">\(d\)</span> on embedding-vektorin koko (esim. 512)</li>
</ul>
<h3 id="toiminta-kokonaisuutena">Toiminta kokonaisuutena</h3>
<p>3Blue1Brown on tehnyt sen verran hyvää työtä transformer-arkkitehtuurin kokonaisuuden toiminnasta, joten viiton sinut suoraan katsomaan hänen videotaan. Video on upotettu alle.</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/eMlx5fFNoYc?si=998SG9QHp6ENmjuQ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

<p><strong>Video 1:</strong> <em>3Blue1Brownin video, Attention in transformers, step-by-step | Deep Learning Chapter 6, selventää hyvin attention-mekanismia transformer-arkkitehtuurissa</em></p>
<p>On äärimmäisen suositeltavaa katsoa myös soittolistan seuraava video, <a href="https://youtu.be/9-Jl0dxWQs8?si=1tflE3dKxDw7baCv">How might LLMs store facts | Deep Learning Chapter 7</a>, joka selventää, mihin (kenties) tieto tallentuu transformer-mallissa. Eli siis: kuinka malli kykenee jatkamaan lausetta, joka vaatii tosielämän faktatietoa, kuten <em>"Mika Häkkinen on kuuluisa urheilija lajissa..."</em>.</p>
<p>On syytä huomata, että vaikka Transformer ei sisällä rekursiivisia tai konvoluutiokerroksia perinteisessä mielessä, dekooderi toimii autoregressiivisesti – se tuottaa sanoja yksi kerrallaan ja syöttää edellisen tulosteen takaisin seuraavan askeleen syötteeksi. Tämä muistuttaa RNN:n takaisinkytkentää, mutta liittyy mallin käyttötapaan eikä itse arkkitehtuuriin. <sup id="fnref6:ldl"><a class="footnote-ref" href="#fn:ldl">5</a></sup></p>
<p>Jos arkkitehtuuria vertaa konvoluutioverkkoihin, on kiintoisaa, että vaikka Transformer ei eksplisiittisesti hyödynnä konvoluutioita, self-attention-kerrokset oppivat käytännössä usein konvoluutiomaisia operaatioita painojen jakamisen kautta. Oleellinen ero kuitenkin on, että self-attention voi kohdistua mihin tahansa syötteen kohtaan, kun taas konvoluutio rajautuu vain ytimen (engl. <em>kernel</em>) kattamiin naapuripositioihin. <sup id="fnref7:ldl"><a class="footnote-ref" href="#fn:ldl">5</a></sup></p>
<p>Lopulta tätä kokonaisuutta voi tiivistää siten, että kyseessä on yhä enkooderi-dekooderi-arkkitehtuuri, mutta rinnakkaistettavuus erottaa sen RNN-pohjaisista malleista. Alkuperäisessä julkaisussa arkkitehtuuria käytettiin nimenomaan kääntämiseen, mutta Transformer suoriutuu myös muista tehtävästä. Pelkästään kielen käsittelyssä siitä löytyy muunnelmia, kuten GPT ja BERT. <sup id="fnref8:ldl"><a class="footnote-ref" href="#fn:ldl">5</a></sup></p>
<h2 id="arkkitehtuurin-variaatiot">Arkkitehtuurin variaatiot</h2>
<p>!TODO! Nämä voisi käydä lyhyesti läpi, ehkä. Encoder-only BERT, Decoder-only GPT, Encoder-Decoder T5 eli thö öriginääl thing.</p>
<h2 id="mallien-arviointi-metriikat">Mallien arviointi (Metriikat)</h2>
<p>Tekstiä tuottavien tai kääntävien mallien laadun mittaaminen on vaikeampaa kuin luokittelun, sillä "oikeita" vastauksia voi olla useita, ja siksi yksinkertainen tarkkuusprosentti (accuracy) ei riitä – tai ei ole edes määriteltävissä. Ihminen voi arvioida tuotetun tekstin laatua käsin, mutta tämä on aikaa vievää ja subjektiivista. <sup id="fnref:ml-q-ai"><a class="footnote-ref" href="#fn:ml-q-ai">14</a></sup> Joitakin suoritus kykymittareita siis tarvitaan, mutta on ymmärrettävä, että ne eivät tulkitse laatua kuten ihminen. Suorituskykymittarit voidaan jakaa kahteen kategoriaan:</p>
<ul>
<li><strong>Ulkoiset (engl. extrinsic)</strong>: Mittaavat mallin suorituskykyä todellisessa tehtävässä vertaamalla mallin tuottamaa tulosta ihmisen tuottamaan referenssiin. Esimerkkejä ovat BLEU, ROUGE ja BertSCORE.</li>
<li><strong>Sisäiset (engl. intrinsic)</strong>: Mittaavat mallin tuottaman tekstin sisäisten ominaisuuksien perusteella, ilman ulkoista sovellusta tai tehtävää. Tätä edustaa alla Perplexity.</li>
</ul>
<p>Perplexity, sisäinen suorituskykymittari, voidaan rinnastaa luokittelussa käytettävään ristientropia-tappiofunktioon. Mitä matalampi perplexity, sitä vähemmän malli "hämmentyy" ennustaessaan seuraavaa sanaa. Se ei kuitenkaan suoraan kerro, kuinka hyvä malli on tuottamaan ihmismäistä tekstiä tietyssä tehtävässä.</p>
<p>Vastaavasti BLEU ja ROUGE ovat <strong>ulkoisia mittareita</strong>, jotka vertautuvat kuvantunnistuksen tarkkuusprosenttiin (accuracy). Ne mittaavat mallin suorituskykyä loppukäyttäjän näkökulmasta vertaamalla tulosta ihmisen tekemään mallivastaukseen. BLEU on luonteeltaan tarkkuuspainotteinen (precision) ja soveltuu hyvin konekäännösten arviointiin, kun taas ROUGE on saantipainotteinen (recall) ja on yleinen tiivistelmien laadun mittari. Vaikka mallia koulutetaan minimoimaan perplexityä (tai tappiofunktiota), lopullinen tavoite on usein maksimoida nämä ulkoiset laatumittarit. <sup id="fnref2:ml-q-ai"><a class="footnote-ref" href="#fn:ml-q-ai">14</a></sup></p>
<h3 id="perplexity">Perplexity</h3>
<p>Perplexity on sukua koulutuksen aikana minimoitavaan ristientropiaan (cross-entropy). Käytännössä se mittaa mallin epävarmuutta. Toisin päin sanottuna: se mittaa, kuinka yllättynyt (engl. <em>surprised</em> tai.. wait for it.. <em>perplexed</em>) malli on nähdessään oikean sanan. <sup id="fnref2:transformers-def-guide"><a class="footnote-ref" href="#fn:transformers-def-guide">11</a></sup> Se on IBM:n tutkijoiden vuonna 1977 julkaisema mittari, mutta: <em>"Perplexity remains a primary benchmark to this day and is a popular metric for evaluating sequential neural networks (including the GPT family of models)."</em> <sup id="fnref:comet"><a class="footnote-ref" href="#fn:comet">15</a></sup></p>
<p>Matemaattisesti perplexity lasketaan ennustettujen todennäköisyyksien perusteella ja normalisoidaan lauseen pituudella. Jos malli antaa oikeille sanoille korkeita todennäköisyyksiä, perplexity on matala (lähellä ykköstä). Kaava sille on seuraava: <sup id="fnref3:ml-q-ai"><a class="footnote-ref" href="#fn:ml-q-ai">14</a></sup></p>
<div class="arithmatex">\[
Perplexity(s) = 2^{-\frac{1}{N} \log_2 p(s)}
\]</div>
<p>...jossa <span class="arithmatex">\(s\)</span> on syötesekvenssi, <span class="arithmatex">\(N\)</span> on sekvenssin pituus ja <span class="arithmatex">\(p(s)\)</span> on mallin ennustama todennäköisyys sekvenssille. Tuo kaava ei paljasta, kuinka mallin ennustama todennäköisyys sekvenssille itsessään lasketaan. Se on:</p>
<div class="arithmatex">\[
p(s) = \prod_{i=1}^{N} P(w_i | w_1, w_2, ..., w_{i-1})
\]</div>
<p>Eli lause <code>s = "Pizza may be life"</code> voidaan purkaa sanoiksi <code>w_1 = "Pizza"</code>, <code>w_2 = "may"</code>, <code>w_3 = "be"</code> ja <code>w_4 = "life"</code>. Malli ennustaa todennäköisyyden jokaiselle sanalle, ottaen huomioon kaikki aiemmat sanat. Eli siis:</p>
<div class="arithmatex">\[
\begin{align*}
P(w_1 | \text{start}) &amp; = P(\text{"Pizza"} | \text{start}) \\
P(w_2 | w_1) &amp; = P(\text{"may"} | \text{"Pizza"}) \\
P(w_3 | w_1, w_2) &amp; = P(\text{"be"} | \text{"Pizza"}, \text{"may"}) \\
P(w_4 | w_1, w_2, w_3) &amp; = P(\text{"life"} | \text{"Pizza"}, \text{"may"}, \text{"be"})
\end{align*}
\]</div>
<p>Nämä todennäköisyydet saadaan mallin outputista (softmax). Merkintä <code>P(word|context)</code>, joka esiintyy Hugging Facen <a href="https://huggingface.co/docs/transformers/en/perplexity">Perplexity of fixed-length models</a>-dokumentissa, muistuttanee sinua Naive Bayes -mallin todennäköisyyslausekkeista, kuten myös <em>epävarmuus</em> tai <em>entropia</em>. Perplexity on siis eräänlainen "käänteinen" todennäköisyys, joka mittaa mallin epävarmuutta ennustuksistaan.</p>
<p>Suosittelen tutustumaan Machine Learning Q and AI -kirjan repositoriosta löytyvään <a href="https://github.com/rasbt/MachineLearning-QandAI-book/blob/main/supplementary/q19-evaluation-llms/perplexity.ipynb">perplexity.ipynb</a>-tiedostoon, jossa on esimerkkejä siitä, kuinka perplexity lasketaan käytännössä. <sup id="fnref4:ml-q-ai"><a class="footnote-ref" href="#fn:ml-q-ai">14</a></sup>. Jos haluat selkeästi kommentoidun koodiesimerkin, katso Cometin blogilta <a href="https://www.comet.com/site/blog/perplexity-for-llm-evaluation/">Perplexity for LLM Evaluation</a>.</p>
<h3 id="bleu">BLEU</h3>
<p>BLEU (Bilingual Evaluation Understudy) on kenties tunnetuin ja laajimmin käytetty mittari konekäännösten laadun arviointiin. Sitä hyödyntävät lähes kaikki kielen kääntämiseen kykenevät kielimallit, mukaan lukien OpenAI:n Whisper ja GPT-mallit. Menetelmä perustuu vertailuun: mallin tuottamaa tekstiä verrataan ihmisen tekemään referenssikäännökseen. Käytännössä BLEU mittaa sanastollista päällekkäisyyttä laskemalla tarkkuutta (precision) eli sitä, kuinka moni mallin tuottama sana tai sanajono (n-grammi) esiintyy myös referenssit tekstissä <sup id="fnref5:ml-q-ai"><a class="footnote-ref" href="#fn:ml-q-ai">14</a></sup>. Sen kaava on:</p>
<div class="arithmatex">\[
\begin{align*}
BP &amp; = \begin{cases}
1 &amp; \text{if } c &gt; r \\
e^{(1 - \frac{r}{c})} &amp; \text{if } c \leq r
\end{cases} \\
BLEU &amp; = BP \cdot \exp\left( \sum_{n=1}^{N} w_n \log p_n \right)
\end{align*}
\]</div>
<p>...jossa:</p>
<ul>
<li><span class="arithmatex">\(c\)</span> on mallin tuottaman tekstin pituus</li>
<li><span class="arithmatex">\(r\)</span> on referenssitekstin pituus</li>
<li><span class="arithmatex">\(N\)</span> on n-grammin range (usein 4-grammi)</li>
<li><span class="arithmatex">\(w_n\)</span> on painoarvo n-grammille (usein tasapainotettu, eli <span class="arithmatex">\(w_n = \frac{1}{N}\)</span>)</li>
<li><span class="arithmatex">\(p_n\)</span> on <code>correct_n-grams / tota_n_grams</code> eli mallin tuottamien n-grammien tarkkuus</li>
</ul>
<p>Alla Pekka Huttusen aiemmasta kurssitoteutuksesta lainattu esimerkki:</p>
<ul>
<li>Original: Rakastan oppia uusia asioita tekoälystä.</li>
<li>Reference: I love to learn new things about AI.</li>
<li>Candidate 1: <mark>I love</mark> love <mark>new</mark> <mark>AI.</mark></li>
<li>Candidate 2: <mark>I love to learn</mark> ride a bike <mark>.</mark></li>
<li>Candidate 3: <mark>I love</mark> learning <mark>about</mark> artificial intelligence <mark>.</mark></li>
</ul>
<div class="admonition info">
<p class="admonition-title">Info</p>
<p>Lukuohje: keltaisella korostetut osat ovat n-grammeja, jotka esiintyvät referenssissä. Ensimmäisen lauseen toista <code>love</code>-sanaa ei lasketa, koska se on jo laskettu ensimmäisen <code>love</code>-sanan kohdalla. Täten <code>len(I, love, new, AI, .</code>) on 5 ja <code>len(Candidate_1)</code> on 6, joten 1-grammien tarkkuus on 5/6.</p>
</div>
<table>
<thead>
<tr>
<th>Metric</th>
<th>Candidate 1</th>
<th>Candidate 2</th>
<th>Candidate 3</th>
</tr>
</thead>
<tbody>
<tr>
<td>1-grams</td>
<td>5/6</td>
<td>5/8</td>
<td>4/7</td>
</tr>
<tr>
<td>2-grams</td>
<td>2/5</td>
<td>3/7</td>
<td>1/6</td>
</tr>
<tr>
<td>3-grams</td>
<td>0/4</td>
<td>2/6</td>
<td>0/5</td>
</tr>
<tr>
<td>4-grams</td>
<td>0/3</td>
<td>1/5</td>
<td>0/4</td>
</tr>
<tr>
<td>lenght <span class="arithmatex">\(c\)</span></td>
<td>6</td>
<td>8</td>
<td>7</td>
</tr>
</tbody>
</table>
<details>
<summary>Tarkemmat laskelmat (klikkaa auki)</summary>
<p>Alla vielä laskennat:</p>
<p><strong>Candidate 1</strong></p>
<div class="arithmatex">\[
\begin{align*}
BP &amp; = e^{(1 - \frac{9}{6})} = 0.6065 \\
BLEU &amp; = 0.6065 \cdot \exp\left( \frac{1}{4} (\log \frac{5}{6} + \log \frac{2}{5} + \log \frac{0}{4} + \log \frac{0}{3}) \right) \\
BLEU &amp; = 0.6065 \cdot \exp\left( -0.1823 - 0.9162 - \infty - \infty \right) \\
BLEU &amp; = 0.6065 \cdot 0 = 0
\end{align*}
\]</div>
<p><strong>Candidate 2</strong></p>
<div class="arithmatex">\[
\begin{align*}
BP &amp; = e^{(1 - \frac{9}{8})} = 0.8825 \\
BLEU &amp; = 0.6065 \cdot \exp\left( \frac{1}{4} (\log \frac{5}{8} + \log \frac{3}{7} + \log \frac{2}{6} + \log \frac{1}{5}) \right) \\
BLEU &amp; = 0.6065 \cdot \exp\left(\frac{1}{4} (-0.4700 - 0.8472 -1.0986 - 1.6094) \right) \\
BLEU &amp; = 0.6065 \cdot 0.3655 = 0.323
\end{align*}
\]</div>
<p><strong>Candidate 3</strong></p>
<div class="arithmatex">\[
\begin{align*}
BP &amp; = e^{(1 - \frac{9}{7})} = 0.7515 \\
BLEU &amp; = 0.6065 \cdot \exp\left( \frac{1}{4} (\log \frac{4}{7} + \log \frac{1}{6} - \infty - \infty) \right) \\
BLEU &amp; = 0.7515 \cdot 0 = 0
\end{align*}
\]</div>
</details>
<p>Kyseessä on merkkijonojen vertailuun perustuva mittari. Se ei ymmärrä sanojen merkityksiä tai kielioppia. Esimerkiksi synonyymien käyttö tai sanajärjestyksen muutos voi laskea pisteitä, vaikka käännös olisi sisällöllisesti oikein. Toisaalta merkitykseltään väärä lause voi saada korkeat pisteet, jos se sisältää oikeat sanat. <sup id="fnref6:ml-q-ai"><a class="footnote-ref" href="#fn:ml-q-ai">14</a></sup></p>
<p>Nykykäsityksen mukaan BLEU onkin hyödyllinen työkalu ensisijaisesti mallin kehityksen seurantaan (model selection) koulutuksen aikana, jossa se toimii sujuvuuden indikaattorina. Lopulliseen laadunvarmistukseen (model evaluation) tai virheiden etsintään se ei sovellu yhtä hyvin, ja nykyään sen rinnalle tai tilalle on noussut kehittyneempiä vaihtoehtoja, kuten METEOR ja COMET.<sup id="fnref7:ml-q-ai"><a class="footnote-ref" href="#fn:ml-q-ai">14</a></sup>.</p>
<p>Aivan kuten Perplexity, myös BLEU:n käytöstä löytyy <a href="https://github.com/rasbt/MachineLearning-QandAI-book/blob/main/supplementary/q19-evaluation-llms/bleu.ipynb">bleu.ipynb</a>-tiedostosta lisätietoa aiemmin mainitun kirjan reposta.</p>
<h3 id="rouge">ROUGE</h3>
<p>TODO! Obvious.</p>
<h3 id="bertscore">BERTScore</h3>
<p>TODO! Obvious.</p>
<h2 id="tehtavat">Tehtävät</h2>
<div class="admonition question">
<p class="admonition-title">Tehtävä: Perplexity</p>
<p>Aja tiedosto <code>720_perplexity.py</code>. Kyseessä on <strong>Transformers - The definitive Guide</strong> -kirjan <sup id="fnref3:transformers-def-guide"><a class="footnote-ref" href="#fn:transformers-def-guide">11</a></sup> esimerkki, joka on toteutettu PyTorchilla. Koodi on muutoin samaa, mutta se on saatettu yhteen Suomen kielen kanssa. Tutustu koodiin ja käytä sitä leikkikenttenä. Lopussa on soluja, jotka sallivat sinun selvittää jonkin keksimäsi lauseen PPL-arvon, näin:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-1-1"><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a><span class="n">Perplexity</span><span class="p">(</span><span class="s2">&quot;Ainola on säveltäjä Jean Sibeliuksen ja hänen puolisonsa Aino Sibeliuksen asuintalo, joka on vuodesta 1974 alkaen toiminut kotimuseona&quot;</span><span class="p">)</span>
</span></code></pre></div>
<p>Lause on lainaus <a href="https://fi.wikipedia.org/wiki/Ainola">Ainola</a>-artikkelista suomenkielisestä Wikipediasta. Wikipediaa on käytetty yhtenä lähteenä käyttämämme mallin <code>LumiOpen/Viking-7B</code> koulutuksessa. Huomaa, että mallin ajaminen GPU:lla vaatii noin 16 GB VRAM:ia. CPU:lla ajettaessa et saa Out Of Memory -varoituksia, mutta malli on toki hieman hidas.</p>
</div>
<div class="admonition question">
<p class="admonition-title">Tehtävä: ???</p>
<p>Tutustu <code>721_neural_machine_translation_transformer.py</code>-tiedostoon. Tämä on muokattu versio NVIDIA:n Learning Deep Learning -kirjan (ja videosarjan) esimerkistä. Tehtävässä koulutetaan Transformers-malli kääntämään yhdestä kielestä toiseen.</p>
<p>20 epookin koulutus kesti opettajan Macbook:lla noin 12.5 minuuttia. Huomaa, että MPS ei ole tuettu, joten koulutus tapahtui CPU:lla. Ubuntu-koneella meni noin 1.5 minuuttia GPU:lla.</p>
</div>
<h2 id="lahteet">Lähteet</h2>
<div class="footnote">
<hr />
<ol>
<li id="fn:attention2017">
<p>Vaswani, A. et. al. <em>Attention is All You Need</em>. 2017. https://arxiv.org/abs/1706.03762&#160;<a class="footnote-backref" href="#fnref:attention2017" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:attention2017" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:attention2017" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:llmfromscratch">
<p>Raschka, S. <em>Build a Large Language Model (From Scratch)</em>. Manning. 2024.&#160;<a class="footnote-backref" href="#fnref:llmfromscratch" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:llmfromscratch" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:llmfromscratch" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:llmfromscratch" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:llmfromscratch" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:llmfromscratch" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
<li id="fn:buildingaiagents">
<p>Raieli, S. &amp; Iuculano, G. <em>Building AI Agents with LLMs, RAG, and Knowledge Graphs</em>. Packt. 2025.&#160;<a class="footnote-backref" href="#fnref:buildingaiagents" title="Jump back to footnote 3 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:buildingaiagents" title="Jump back to footnote 3 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:buildingaiagents" title="Jump back to footnote 3 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:buildingaiagents" title="Jump back to footnote 3 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:buildingaiagents" title="Jump back to footnote 3 in the text">&#8617;</a></p>
</li>
<li id="fn:dlwithpython">
<p>Watson, M &amp; Chollet, F. <em>Deep Learning with Python, Third Edition</em>. Manning. 2025.&#160;<a class="footnote-backref" href="#fnref:dlwithpython" title="Jump back to footnote 4 in the text">&#8617;</a></p>
</li>
<li id="fn:ldl">
<p>Ekman, M. <em>Learning Deep Learning: Theory and Practice of Neural Networks, Computer Vision, NLP, and Transformers using TensorFlow</em>. Addison-Wesley. 2025.&#160;<a class="footnote-backref" href="#fnref:ldl" title="Jump back to footnote 5 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:ldl" title="Jump back to footnote 5 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:ldl" title="Jump back to footnote 5 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:ldl" title="Jump back to footnote 5 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:ldl" title="Jump back to footnote 5 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:ldl" title="Jump back to footnote 5 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:ldl" title="Jump back to footnote 5 in the text">&#8617;</a><a class="footnote-backref" href="#fnref8:ldl" title="Jump back to footnote 5 in the text">&#8617;</a></p>
</li>
<li id="fn:nimikirja">
<p>1322/1989. <em>Nimikirjan pitäminen eräistä henkilöstöryhmistä</em>. https://www.finlex.fi/fi/lainsaadanto/1989/1322&#160;<a class="footnote-backref" href="#fnref:nimikirja" title="Jump back to footnote 6 in the text">&#8617;</a></p>
</li>
<li id="fn:kielikello">
<p>Virtaniemi, A. <em>Kiiloja ja sokkeloita. Säädöskielen virke- ja lauserakenteen ongelmia</em>. 1992. https://kielikello.fi/kiiloja-ja-sokkeloita-saadoskielen-virke-ja-lauserakenteen-ongelmia/&#160;<a class="footnote-backref" href="#fnref:kielikello" title="Jump back to footnote 7 in the text">&#8617;</a></p>
</li>
<li id="fn:ml-algos-depth">
<p>Smolyakov, V. <em>Machine Learning Algorithms in Depth</em>. Manning. 2025.&#160;<a class="footnote-backref" href="#fnref:ml-algos-depth" title="Jump back to footnote 8 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:ml-algos-depth" title="Jump back to footnote 8 in the text">&#8617;</a></p>
</li>
<li id="fn:geronpytorch">
<p>Géron, A. <em>Hands-On Machine Learning with Scikit-Learn and PyTorch</em>. O'Reilly. 2025.&#160;<a class="footnote-backref" href="#fnref:geronpytorch" title="Jump back to footnote 9 in the text">&#8617;</a></p>
</li>
<li id="fn:udlbook">
<p>Prince, S. <em>Understanding Deep Learning</em>. The MIT Press. 2023. https://udlbook.github.io/udlbook/&#160;<a class="footnote-backref" href="#fnref:udlbook" title="Jump back to footnote 10 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:udlbook" title="Jump back to footnote 10 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:udlbook" title="Jump back to footnote 10 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:udlbook" title="Jump back to footnote 10 in the text">&#8617;</a></p>
</li>
<li id="fn:transformers-def-guide">
<p>Koenigstein, N. <em>Transformers: The Definitive Guide</em>. O'Reilly. 2026.&#160;<a class="footnote-backref" href="#fnref:transformers-def-guide" title="Jump back to footnote 11 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:transformers-def-guide" title="Jump back to footnote 11 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:transformers-def-guide" title="Jump back to footnote 11 in the text">&#8617;</a></p>
</li>
<li id="fn:azure-book">
<p>Esposito, F. <em>Programming Large Language Models with Azure Open AI: Conversational programming and prompt engineering with LLMs</em>. Microsoft Press. 2024.&#160;<a class="footnote-backref" href="#fnref:azure-book" title="Jump back to footnote 12 in the text">&#8617;</a></p>
</li>
<li id="fn:voita2019">
<p>Voita, E., Talbot, D., Moiseev, F., Sennrich, R. &amp; Titov, I. <em>Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned</em>. 2019. https://arxiv.org/abs/1905.09418&#160;<a class="footnote-backref" href="#fnref:voita2019" title="Jump back to footnote 13 in the text">&#8617;</a></p>
</li>
<li id="fn:ml-q-ai">
<p>Raschka, S. <em>Machine Learning Q and AI</em>. No Starch Press. 2024.&#160;<a class="footnote-backref" href="#fnref:ml-q-ai" title="Jump back to footnote 14 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:ml-q-ai" title="Jump back to footnote 14 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:ml-q-ai" title="Jump back to footnote 14 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:ml-q-ai" title="Jump back to footnote 14 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:ml-q-ai" title="Jump back to footnote 14 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:ml-q-ai" title="Jump back to footnote 14 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:ml-q-ai" title="Jump back to footnote 14 in the text">&#8617;</a></p>
</li>
<li id="fn:comet">
<p>Morgan, A. <em>Perplexity for LLM Evaluation</em>. 2024. https://www.comet.com/site/blog/perplexity-for-llm-evaluation/&#160;<a class="footnote-backref" href="#fnref:comet" title="Jump back to footnote 15 in the text">&#8617;</a></p>
</li>
</ol>
</div>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2026 <a href="https://www.kamk.fi">Kajaanin Ammattikorkeakoulu Oy</a>. 
Licenced under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">BY-NC-SA 4.0</a>
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      
      <script id="__config" type="application/json">{"annotate": null, "base": "../..", "features": ["content.code.copy", "content.code.annotate", "content.tabs.link"], "search": "../../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Kopioitu leikep\u00f6yd\u00e4lle", "clipboard.copy": "Kopioi leikep\u00f6yd\u00e4lle", "search.result.more.one": "1 lis\u00e4\u00e4 t\u00e4ll\u00e4 sivulla", "search.result.more.other": "# lis\u00e4\u00e4 t\u00e4ll\u00e4 sivulla", "search.result.none": "Ei t\u00e4sm\u00e4\u00e4vi\u00e4 dokumentteja", "search.result.one": "1 t\u00e4sm\u00e4\u00e4v\u00e4 dokumentti", "search.result.other": "# t\u00e4sm\u00e4\u00e4v\u00e4\u00e4 dokumenttia", "search.result.placeholder": "Kirjoita aloittaaksesi haun", "search.result.term.missing": "Puuttuu", "select.version": "Valitse versio"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.79ae519e.min.js"></script>
      
        <script src="../../javascripts/mathjax.js"></script>
      
        <script src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>