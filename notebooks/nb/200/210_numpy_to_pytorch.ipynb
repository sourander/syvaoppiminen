{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6394fe53",
   "metadata": {},
   "source": [
    "# From NumPy to PyTorch\n",
    "\n",
    "In the previous notebook, we implemented a 2-2-1 neural network from scratch using NumPy. We had to:\n",
    "\n",
    "1. Manually implement the forward pass\n",
    "2. Manually calculate gradients for backpropagation\n",
    "3. Manually implement the weight updates\n",
    "\n",
    "In this notebook, we'll see how PyTorch makes this much simpler by:\n",
    "\n",
    "1. Automatically computing gradients (autograd)\n",
    "2. Providing built-in layers and loss functions\n",
    "3. Handling the optimization automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491d4af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d199739",
   "metadata": {},
   "source": [
    "## Define the PyTorch Model\n",
    "\n",
    "Notice how much simpler this is compared to the NumPy implementation:\n",
    "\n",
    "* We don't need to manually implement forward pass matrix operations\n",
    "* We don't need to manually compute gradients\n",
    "* We don't need to manually update weights\n",
    "\n",
    "PyTorch's `nn.Module` handles the architecture, and autograd handles the backpropagation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8848d193",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyTorchNN(nn.Module):\n",
    "    \"\"\"\n",
    "    2-2-1 Neural Network with Binary Cross-Entropy loss and Sigmoid activations.\n",
    "    This is the PyTorch equivalent of NumpyNNwithBCE.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(PyTorchNN, self).__init__()\n",
    "        \n",
    "        # Define the layers\n",
    "        # Layer 0 to Layer 1: 2 inputs -> 2 hidden neurons\n",
    "        self.fc1 = nn.Linear(2, 2)\n",
    "        \n",
    "        # Layer 1 to Layer 2: 2 hidden neurons -> 1 output\n",
    "        self.fc2 = nn.Linear(2, 1)\n",
    "        \n",
    "        # Activation function\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Forward pass through the network\n",
    "        x = self.sigmoid(self.fc1(x))  # Input -> Hidden (with sigmoid)\n",
    "        x = self.sigmoid(self.fc2(x))  # Hidden -> Output (with sigmoid)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def25fe7",
   "metadata": {},
   "source": [
    "## Training Function\n",
    "\n",
    "The training loop in PyTorch follows a standard pattern:\n",
    "\n",
    "1. **Forward pass**: Compute predictions\n",
    "2. **Compute loss**: Calculate how wrong the predictions are\n",
    "3. **Backward pass**: PyTorch's autograd computes all gradients automatically!\n",
    "4. **Update weights**: The optimizer updates weights using the computed gradients\n",
    "\n",
    "Compare this to our NumPy implementation where we had to manually:\n",
    "- Calculate `dZ2 = A2 - target`\n",
    "- Calculate `dA1 = dZ2.dot(W1.T)`\n",
    "- Calculate `dZ1 = dA1 * sigmoid_derivative(A1)`\n",
    "- Update all weights manually\n",
    "\n",
    "PyTorch does all of this with just `loss.backward()`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a20c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X, y, epochs=1000, learning_rate=0.1, print_every=100):\n",
    "    \"\"\"\n",
    "    Train the PyTorch model.\n",
    "    \n",
    "    Args:\n",
    "        model: The neural network model\n",
    "        X: Input data (numpy array or torch tensor)\n",
    "        y: Target labels (numpy array or torch tensor)\n",
    "        epochs: Number of training iterations\n",
    "        learning_rate: Learning rate for optimization\n",
    "        print_every: Print loss every N epochs\n",
    "    \"\"\"\n",
    "    # Convert numpy arrays to PyTorch tensors if needed\n",
    "    if isinstance(X, np.ndarray):\n",
    "        X = torch.FloatTensor(X)\n",
    "    if isinstance(y, np.ndarray):\n",
    "        y = torch.FloatTensor(y)\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        # Zero the gradients from previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass: compute predictions\n",
    "        outputs = model(X)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, y)\n",
    "        \n",
    "        # Backward pass: compute gradients automatically!\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print progress\n",
    "        if epoch == 0 or (epoch + 1) % print_every == 0:\n",
    "            print(f\"Epoch {epoch + 1}: loss={loss.item():.7f}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bba0c39",
   "metadata": {},
   "source": [
    "## Test with Simple XOR Dataset\n",
    "\n",
    "Let's test with the same simple 4-sample XOR dataset we used in the NumPy notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f3f69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the simple XOR dataset\n",
    "X = np.array([[0, 0], \n",
    "              [0, 1], \n",
    "              [1, 0], \n",
    "              [1, 1]], dtype=np.float32)\n",
    "y = np.array([[0], \n",
    "              [1], \n",
    "              [1], \n",
    "              [0]], dtype=np.float32)\n",
    "\n",
    "# Initialize and train the model\n",
    "model = PyTorchNN()\n",
    "model = train_model(model, X, y, epochs=20_000, learning_rate=0.03, print_every=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52fbb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "with torch.no_grad():  # No need to compute gradients during inference\n",
    "    X_tensor = torch.FloatTensor(X)\n",
    "    y_pred = model(X_tensor).numpy()\n",
    "    y_pred_classes = (y_pred > 0.5).astype(int)\n",
    "\n",
    "print(\"Predictions:\")\n",
    "print(y_pred)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y, y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507337ac",
   "metadata": {},
   "source": [
    "## Test with More Complex XOR Dataset\n",
    "\n",
    "Now let's test with a more complex dataset with noise, just like in the NumPy notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01440202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate XOR data with noise (same as NumPy notebook)\n",
    "np.random.seed(0)\n",
    "n_points = 100\n",
    "noise = 0.1\n",
    "\n",
    "X_complex = np.vstack((\n",
    "    np.random.normal(loc=[0.0, 0.0], scale=noise, size=(n_points, 2)),\n",
    "    np.random.normal(loc=[1.0, 1.0], scale=noise, size=(n_points, 2)),\n",
    "    np.random.normal(loc=[0.0, 1.0], scale=noise, size=(n_points, 2)),\n",
    "    np.random.normal(loc=[1.0, 0.0], scale=noise, size=(n_points, 2))\n",
    ")).astype(np.float32)\n",
    "\n",
    "y_complex = np.array([0]*n_points + [0]*n_points + [1]*n_points + [1]*n_points).reshape(-1, 1).astype(np.float32)\n",
    "\n",
    "# Apply Z-score normalization\n",
    "X_complex = (X_complex - X_complex.mean(axis=0)) / X_complex.std(axis=0)\n",
    "\n",
    "# Visualize the data\n",
    "sns.scatterplot(x=X_complex[:,0], y=X_complex[:,1], hue=y_complex.flatten())\n",
    "plt.title(\"XOR Dataset with Noise\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbdb1a8",
   "metadata": {},
   "source": [
    "## Train and Visualize Decision Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5c9f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train a new model\n",
    "model_complex = PyTorchNN()\n",
    "model_complex = train_model(model_complex, X_complex, y_complex, \n",
    "                            epochs=20_000, learning_rate=0.03, print_every=1_000)\n",
    "\n",
    "# Create a mesh grid for decision boundary visualization\n",
    "x_min, x_max = X_complex[:, 0].min() - 0.5, X_complex[:, 0].max() + 0.5\n",
    "y_min, y_max = X_complex[:, 1].min() - 0.5, X_complex[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                     np.linspace(y_min, y_max, 200))\n",
    "\n",
    "# Predict on the mesh grid\n",
    "with torch.no_grad():\n",
    "    grid_tensor = torch.FloatTensor(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = model_complex(grid_tensor).numpy()\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot decision boundary\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.contourf(xx, yy, Z, levels=20, cmap='RdYlBu', alpha=0.6)\n",
    "plt.colorbar(label='Predicted Probability')\n",
    "plt.contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=2)\n",
    "\n",
    "# Plot the data points\n",
    "scatter = plt.scatter(X_complex[:, 0], X_complex[:, 1], c=y_complex.flatten(), \n",
    "                     cmap='RdYlBu', edgecolors='black', s=50)\n",
    "plt.xlabel('Feature 1 (normalized)')\n",
    "plt.ylabel('Feature 2 (normalized)')\n",
    "plt.title('PyTorch Model: Decision Boundary')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994811d9",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### What We Had to Implement in NumPy:\n",
    "1. **Manual forward pass**: Matrix multiplications, activations\n",
    "2. **Manual backward pass**: Computing all gradients\n",
    "   - `dZ2 = A2 - target`\n",
    "   - `dA1 = dZ2.dot(W1.T)`\n",
    "   - `dZ1 = dA1 * sigmoid_derivative(A1)`\n",
    "3. **Manual weight updates**: Gradient descent\n",
    "   - `W1 -= learning_rate * A1.T.dot(dZ2)`\n",
    "   - `W0 -= learning_rate * A0.T.dot(dZ1)`\n",
    "   - Plus biases\n",
    "\n",
    "### What PyTorch Provides:\n",
    "1. **Automatic forward pass**: Just define layers in `__init__` and call them in `forward()`\n",
    "2. **Automatic backward pass**: Just call `loss.backward()` - PyTorch computes ALL gradients!\n",
    "3. **Automatic optimization**: Optimizer handles weight updates\n",
    "\n",
    "### The Magic of Autograd:\n",
    "- PyTorch tracks all operations on tensors\n",
    "- Builds a computational graph automatically\n",
    "- Computes gradients via reverse-mode automatic differentiation\n",
    "- **No manual gradient calculations needed!**\n",
    "\n",
    "This is why PyTorch (and other deep learning frameworks) are so powerful - they handle the hard part (backpropagation) correctly and efficiently, letting you focus on architecture and experimentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Syvaoppiminen I",
   "language": "python",
   "name": "nb-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
