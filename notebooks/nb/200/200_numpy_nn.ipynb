{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f966401",
   "metadata": {},
   "source": [
    "# NumPy Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db66ceb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a34866",
   "metadata": {},
   "source": [
    "The model will have the 2-2-1 architecture, allowing us to solve the XOR problem. Architecture and starting weights are follow the same pattern as in Matt Mazur's [A Step by Step Backpropagation Example](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/), but with following differences: \n",
    "\n",
    "* we have only one output\n",
    "* we are using a different output activation function. \n",
    "\n",
    "Why the difference in activation function? Matt is doing regression, while we are doing binary classification. Thus, Matt is using a (halved) squared error loss and linear output activation, while we are using binary cross-entropy loss and sigmoid output activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97357ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyNNwithBCE:\n",
    "    \"\"\"\n",
    "    Modularized 2-2-1 Neural Network with Binary Cross-Entropy loss and Sigmoid activations.\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate:float=0.1, every_nth_to_print:int=100):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.every_nth_to_print = every_nth_to_print # Check print in fit()\n",
    "        \n",
    "        # Initialize weights for 2-2-1 architecture\n",
    "        # W0: Input layer (2 inputs) to hidden layer (2 neurons)\n",
    "        # Shape: (2, 2)\n",
    "        self.W0 = np.random.uniform(-0.5, 0.5, size=(2, 2))\n",
    "        self.b0 = np.zeros((1, 2))\n",
    "        \n",
    "        # W1: Hidden layer (2 neurons) to output layer (1 neuron)\n",
    "        self.W1 = np.random.uniform(-0.5, 0.5, size=(2, 1))\n",
    "        self.b1 = np.zeros((1, 1))\n",
    "        \n",
    "        # Cache for storing intermediate values during forward pass\n",
    "        self.A0 = np.empty((1, 2))  # Input layer activations\n",
    "        self.A1 = np.empty((1, 2))  # Hidden layer activations\n",
    "        self.A2 = np.empty((1, 1))  # Output layer activations\n",
    "\n",
    "        # Cache for storing gradients during backward pass\n",
    "        self.dZ1 = np.empty((1, 1))  # Gradient of loss w.r.t. hidden layer\n",
    "        self.dZ2 = np.empty((1, 1))     # Gradient of loss w.r.t. output layer\n",
    "\n",
    "        # Counter for manual train_step\n",
    "        self.manual_epoch: int = 0\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Reshape input\n",
    "        x = np.atleast_2d(x)\n",
    "        \n",
    "        # Layer 0 (input layer)\n",
    "        self.A0 = x\n",
    "        \n",
    "        # Layer 1 (hidden layer)\n",
    "        Z1 = self.A0.dot(self.W0) + self.b0\n",
    "        self.A1 = self.sigmoid(Z1)\n",
    "\n",
    "        # Layer 2 (output layer)\n",
    "        Z2 = self.A1.dot(self.W1) + self.b1\n",
    "        self.A2 = self.sigmoid(Z2)\n",
    "        \n",
    "        return self.A2\n",
    "    \n",
    "    def backward(self, target):\n",
    "        # === Output layer ===\n",
    "        # Gradient of loss wrt Z2 (pre-activation of output neuron)\n",
    "        # For sigmoid + BCE, this simplifies beautifully to (A2 - y)\n",
    "        self.dZ2 = self.A2 - target\n",
    "        \n",
    "        # === Hidden layer ===\n",
    "        # Gradient of loss wrt Z1 (pre-activation of hidden layer)\n",
    "        # Step 1: propagate error back through W1\n",
    "        dA1 = self.dZ2.dot(self.W1.T)\n",
    "        # Step 2: apply derivative of activation (sigmoid)\n",
    "        self.dZ1 = dA1 * self.sigmoid_derivative(self.A1)\n",
    "    \n",
    "    def optimize(self):\n",
    "        \"\"\"\n",
    "        Update weights and biases using computed gradients.\n",
    "        Uses the cached activations and deltas from forward and backward passes.\n",
    "        \"\"\"\n",
    "        # Update W1 and b1 (hidden to output)\n",
    "        self.W1 -= self.learning_rate * self.A1.T.dot(self.dZ2)\n",
    "        self.b1 -= self.learning_rate * self.dZ2\n",
    "        \n",
    "        # Update W0 and b0 (input to hidden)\n",
    "        self.W0 -= self.learning_rate * self.A0.T.dot(self.dZ1)\n",
    "        self.b0 -= self.learning_rate * self.dZ1\n",
    "    \n",
    "    def train_step(self, X, y, learning_rate:float|None=None):\n",
    "        self.learning_rate = learning_rate or self.learning_rate\n",
    "        self.manual_epoch = self.manual_epoch + 1\n",
    "        \n",
    "        # Loop through each sample\n",
    "        for (x, target) in zip(X, y):\n",
    "            self.forward(x)\n",
    "            self.backward(target)\n",
    "            self.optimize()\n",
    "\n",
    "        # For manual train_step, we want to be verbose each time\n",
    "        loss = self.calculate_loss(X, y)\n",
    "        print(\"Manual epoch {}: loss={:.7f}\".format(self.manual_epoch + 1, loss))\n",
    "    \n",
    "    def fit(self, X, y, epochs=1000):\n",
    "        for epoch in np.arange(0, epochs):\n",
    "            # Loop through each sample\n",
    "            for (x, target) in zip(X, y):\n",
    "                self.forward(x)\n",
    "                self.backward(target)\n",
    "                self.optimize()\n",
    "            \n",
    "            if epoch == 0 or (epoch + 1) % self.every_nth_to_print == 0:\n",
    "                loss = self.calculate_loss(X, y)\n",
    "                print(\"Epoch {}: loss={:.7f}\".format(epoch + 1, loss))\n",
    "    \n",
    "    def predict(self, X):\n",
    "\n",
    "        p = np.atleast_2d(X)\n",
    "        \n",
    "        # Layer 0 to Layer 1\n",
    "        p = self.sigmoid(np.dot(p, self.W0) + self.b0)\n",
    "\n",
    "        # Layer 1 to Layer 2\n",
    "        return self.sigmoid(np.dot(p, self.W1) + self.b1)\n",
    "    \n",
    "    def calculate_loss(self, X, targets):\n",
    "        targets = np.atleast_2d(targets)\n",
    "        \n",
    "        # Get predictions (clipped to avoid log(0))\n",
    "        predictions = self.predict(X)\n",
    "        predictions = np.clip(predictions, 1e-15, 1 - 1e-15)\n",
    "        \n",
    "        # Binary cross-entropy loss\n",
    "        # L = -sum(y * log(y_pred) + (1-y) * log(1-y_pred))\n",
    "        loss = -np.sum(\n",
    "            targets * np.log(predictions) \n",
    "            + \n",
    "            (1 - targets) * np.log(1 - predictions)\n",
    "        )\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb2a795",
   "metadata": {},
   "source": [
    "## Simple data\n",
    "\n",
    "Let's use a dataset consisting of only four samples. Note that this will not always find the solution; if not, run again. If we would want to improve this, we could try multiple different ideas:\n",
    "\n",
    "* Initialize weights using some other method (e.g. Xavier initialization)\n",
    "* Use more neurons in the hidden layer\n",
    "* Use learning rate decay or momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a05bb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NumpyNNwithBCE(learning_rate=0.03, every_nth_to_print=2000)\n",
    "\n",
    "X = np.array([[0, 0], \n",
    "              [0, 1], \n",
    "              [1, 0], \n",
    "              [1, 1]])\n",
    "y = np.array([[0], \n",
    "              [1], \n",
    "              [1], \n",
    "              [0]])\n",
    "model.fit(X, y, epochs=20_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46985734",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X)\n",
    "y_pred_classes = (y_pred > 0.5).astype(int)\n",
    "print(classification_report(y, y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6163e0a4",
   "metadata": {},
   "source": [
    "# Slightly more complex data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4d5b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate XOR data using numpy\n",
    "# We will generate four \"blobs\" of point around the mathematically correct XOR points (0,0), (0,1), (1,0), (1,1)\n",
    "np.random.seed(0)\n",
    "n_points = 100\n",
    "noise = 0.1\n",
    "\n",
    "X = np.vstack((\n",
    "    np.random.normal(loc=[0.0, 0.0], scale=noise, size=(n_points, 2)),\n",
    "    np.random.normal(loc=[1.0, 1.0], scale=noise, size=(n_points, 2)),\n",
    "    np.random.normal(loc=[0.0, 1.0], scale=noise, size=(n_points, 2)),\n",
    "    np.random.normal(loc=[1.0, 0.0], scale=noise, size=(n_points, 2))\n",
    "))\n",
    "y = np.array([0]*n_points + [0]*n_points + [1]*n_points + [1]*n_points).reshape(-1, 1)\n",
    "\n",
    "# Apply Z-score normalization\n",
    "X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "\n",
    "# Visualize the data\n",
    "sns.scatterplot(x=X[:,0], y=X[:,1], hue=y.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139c4cd8",
   "metadata": {},
   "source": [
    "## Train and plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7674032",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NumpyNNwithBCE(learning_rate=0.01, every_nth_to_print=500)\n",
    "\n",
    "# Train complete model instead\n",
    "model.fit(X, y, epochs=2_000)\n",
    "\n",
    "# Create a mesh grid for decision boundary visualization\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                     np.linspace(y_min, y_max, 200))\n",
    "\n",
    "# Predict on the mesh grid\n",
    "Z = model.forward(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot decision boundary\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.contourf(xx, yy, Z, levels=20, cmap='RdYlBu', alpha=0.6)\n",
    "plt.colorbar(label='Predicted Probability')\n",
    "plt.contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=2)\n",
    "\n",
    "# Plot the data points\n",
    "scatter = plt.scatter(X[:, 0], X[:, 1], c=y.flatten(), \n",
    "                     cmap='RdYlBu', edgecolors='black', s=50)\n",
    "plt.xlabel('Feature 1 (normalized)')\n",
    "plt.ylabel('Feature 2 (normalized)')\n",
    "plt.title('Decision Boundary after Training Step')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Syvaoppiminen I",
   "language": "python",
   "name": "nb-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
