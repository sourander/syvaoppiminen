{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6394fe53",
   "metadata": {},
   "source": [
    "# From NumPy to PyTorch\n",
    "\n",
    "In the previous notebook, we implemented a 2-2-1 neural network from scratch using NumPy. We had to:\n",
    "\n",
    "1. Manually implement the forward pass\n",
    "2. Manually calculate gradients for backpropagation\n",
    "3. Manually implement the weight updates\n",
    "\n",
    "In this notebook, we'll take a closer look at step 2: calculating gradients for backpropagation. We will use PyTorch to automatically compute these gradients for us, but, we will print out all the intermediate results and compare those to the manual calculations we did in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491d4af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d199739",
   "metadata": {},
   "source": [
    "## Define the PyTorch Model\n",
    "\n",
    "Note that this is from the previous notebook. Last time, we trained it and compared it to NumpyNN. Now, we no longer want to fully train it: we simply inspect how PyTorch handles the forward and backward passes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a39100b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyTorchNNInspectable(nn.Module):\n",
    "    \"\"\"Same as PyTorchNN but stores intermediate activations. \n",
    "    \n",
    "    We also needed to fiddle with the sigmoid slightly to access the pre-activation values.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(2, 2)\n",
    "        self.fc2 = nn.Linear(2, 1)\n",
    "        \n",
    "        # Store intermediate values\n",
    "        self.Z1 = None\n",
    "        self.A1 = None\n",
    "        self.Z2 = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.Z1 = self.fc1(x)\n",
    "        self.A1 = torch.sigmoid(self.Z1)\n",
    "        self.Z2 = self.fc2(self.A1)\n",
    "        output = torch.sigmoid(self.Z2)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbcabc7",
   "metadata": {},
   "source": [
    "## Step 1: Create a Fresh Model and Single Sample\n",
    "\n",
    "Let's use a single sample from our XOR dataset to trace the backward pass clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d26c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a single sample: [0, 1] -> 1 (should output 1 for XOR)\n",
    "x_sample = torch.tensor([[0.0, 1.0]], requires_grad=False)\n",
    "y_sample = torch.tensor([[1.0]], requires_grad=False)\n",
    "\n",
    "# Instantiate the inspectable model\n",
    "inspect_model = PyTorchNNInspectable()\n",
    "A0 = x_sample\n",
    "W1 = inspect_model.fc2.weight\n",
    "A2 = inspect_model(x_sample) # Forward pass to populate A1, Z1, Z2\n",
    "\n",
    "A1 = inspect_model.A1\n",
    "Z1 = inspect_model.Z1\n",
    "Z2 = inspect_model.Z2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe85f39",
   "metadata": {},
   "source": [
    "## Step 2: Compute Loss and Backward Pass\n",
    "\n",
    "Now let's compute the loss and call `backward()` to let PyTorch compute all the gradients.\n",
    "\n",
    "Running this cell will populate the `.grad` attributes of all parameters in `inspect_model`. This will allow us to later on compare these gradients to our manual calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97f16ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute loss\n",
    "criterion = nn.BCELoss()\n",
    "loss = criterion(A2, y_sample)\n",
    "\n",
    "print(f\"Loss: {loss.item():.7f}\")\n",
    "\n",
    "# Perform backward pass\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3592333b",
   "metadata": {},
   "source": [
    "### Step 3: Mimic our NumPy Implementation\n",
    "\n",
    "Let's manually compute the gradients to verify PyTorch got it right! Or maybe.. that we got it right. \n",
    "\n",
    "For reference, the relating functions are:\n",
    "\n",
    "```python\n",
    "    def backward(self, target):\n",
    "        self.dZ2 = self.A2 - target\n",
    "        dA1 = self.dZ2.dot(self.W1.T)\n",
    "        self.dZ1 = dA1 * self.sigmoid_derivative(self.A1)\n",
    "    \n",
    "    def optimize(self):\n",
    "        self.W1 -= self.learning_rate * self.A1.T.dot(self.dZ2)\n",
    "        self.b1 -= self.learning_rate * self.dZ2\n",
    "        \n",
    "        self.W0 -= self.learning_rate * self.A0.T.dot(self.dZ1)\n",
    "        self.b0 -= self.learning_rate * self.dZ1\n",
    "```\n",
    "\n",
    "Let's mimic this as closely as possible. Since we are not using the class, the syntax would end up lacking the `self.` part. We will mock the instance variables with a dataclass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5673508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is just for mockery purposes\n",
    "# Focus on understanding the next cell!\n",
    "@dataclass\n",
    "class MockedInstanceVariables:\n",
    "    A0: np.ndarray\n",
    "    A1: np.ndarray\n",
    "    A2: np.ndarray\n",
    "    W1: np.ndarray\n",
    "    dZ1: np.ndarray | None = None\n",
    "    dZ2: np.ndarray | None = None\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "self = MockedInstanceVariables(\n",
    "    A0=A0.detach().numpy(),\n",
    "    A1=A1.detach().numpy(), # type: ignore\n",
    "    A2=A2.detach().numpy(),\n",
    "    W1=W1.detach().numpy())\n",
    "\n",
    "# --- This was a given argument for the backward pass ---\n",
    "target = y_sample.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ed4986",
   "metadata": {},
   "source": [
    "## Backpropagation (manual)\n",
    "\n",
    "Compute gradients layer by layer (output → input).\n",
    "\n",
    "The LaTeX formulas below have been generated with Sonnet 4.5 from Anthropic.\n",
    "\n",
    "### Step 1: Output Layer Error\n",
    "\n",
    "Compute the gradient at the output layer. For Binary Cross-Entropy loss with sigmoid activation:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial Z_2} = A_2 - y$$\n",
    "\n",
    "This is the \"error signal\" at the output layer, giving a value for: how far our prediction is from the target.\n",
    "\n",
    "### Step 2: Backpropagate to Hidden Layer\n",
    "\n",
    "Propagate the error backward through the network using the chain rule:\n",
    "\n",
    "1. First, propagate through the weights: $\\frac{\\partial L}{\\partial A_1} = \\frac{\\partial L}{\\partial Z_2} \\cdot W_1$\n",
    "2. Then apply the activation derivative: $\\frac{\\partial L}{\\partial Z_1} = \\frac{\\partial L}{\\partial A_1} \\odot \\sigma'(A_1)$\n",
    "\n",
    "where $\\sigma'(x) = x(1-x)$ for sigmoid and $\\odot$ denotes element-wise multiplication.\n",
    "\n",
    "### Step 3: Compute Output Layer Weight Gradients\n",
    "\n",
    "Calculate how much each weight and bias in the output layer contributed to the loss:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial W_1} = A_1^T \\cdot \\frac{\\partial L}{\\partial Z_2}$$\n",
    "$$\\frac{\\partial L}{\\partial b_1} = \\frac{\\partial L}{\\partial Z_2}$$\n",
    "\n",
    "### Step 4: Compute Hidden Layer Weight Gradients\n",
    "\n",
    "Calculate how much each weight and bias in the hidden layer contributed to the loss:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial W_0} = A_0^T \\cdot \\frac{\\partial L}{\\partial Z_1}$$\n",
    "$$\\frac{\\partial L}{\\partial b_0} = \\frac{\\partial L}{\\partial Z_1}$$\n",
    "\n",
    "Note: We transpose the results to match PyTorch's weight matrix shape convention (out_features, in_features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734593e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "self.dZ2 = self.A2 - target\n",
    "\n",
    "# Step 2\n",
    "dA1 = np.dot(self.dZ2, self.W1)\n",
    "self.dZ1 = dA1 * self.sigmoid_derivative(self.A1)\n",
    "\n",
    "# Step 3\n",
    "dW1_manual = self.A1.T.dot(self.dZ2)\n",
    "dW1_manual = dW1_manual.T # Transpose to match PyTorch shape\n",
    "db1_manual = self.dZ2\n",
    "\n",
    "# Step 4\n",
    "dW0_manual = self.A0.T.dot(self.dZ1) # type: ignore\n",
    "dW0_manual = dW0_manual.T # Transpose to match PyTorch shape\n",
    "db0_manual = self.dZ1\n",
    "\n",
    "# Step N\n",
    "# If we had more layers, we would continue propagating dZ backwards\n",
    "# through the network.\n",
    "\n",
    "print(\"\\n=== MANUAL GRADIENTS (should match PyTorch) ===\")\n",
    "print(f\"\\ndW1 (dL/dW1): \\n{dW1_manual}\")\n",
    "print(f\"\\ndb1 (dL/db1): \\n{db1_manual}\")\n",
    "print(f\"\\ndW0 (dL/dW0): \\n{dW0_manual}\")\n",
    "print(f\"\\ndb0 (dL/db0): \\n{db0_manual}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5014f2a",
   "metadata": {},
   "source": [
    "### Step 5: Compare PyTorch vs Manual Gradients\n",
    "\n",
    "Let's verify that PyTorch's automatic gradients match our manual calculations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fd0cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare fc2 (W1, b1)\n",
    "pytorch_dW1 = inspect_model.fc2.weight.grad.numpy()\n",
    "pytorch_db1 = inspect_model.fc2.bias.grad.numpy()\n",
    "\n",
    "print(\"fc2.weight gradient:\")\n",
    "print(f\"  PyTorch:  {pytorch_dW1}\")\n",
    "print(f\"  Manual:   {dW1_manual}\")\n",
    "print(f\"  Match: {np.allclose(pytorch_dW1, dW1_manual)}\")\n",
    "\n",
    "print(\"\\nfc2.bias gradient:\")\n",
    "print(f\"  PyTorch:  {pytorch_db1}\")\n",
    "print(f\"  Manual:   {db1_manual.flatten()}\")\n",
    "print(f\"  Match: {np.allclose(pytorch_db1, db1_manual.flatten())}\")\n",
    "\n",
    "# Compare fc1 (W0, b0)\n",
    "pytorch_dW0 = inspect_model.fc1.weight.grad.numpy()\n",
    "pytorch_db0 = inspect_model.fc1.bias.grad.numpy()\n",
    "\n",
    "print(\"\\nfc1.weight gradient (flatted for readability):\")\n",
    "print(f\"  PyTorch:  {pytorch_dW0.flatten()}\")\n",
    "print(f\"  Manual:   {dW0_manual.flatten()}\")\n",
    "print(f\"  Match: {np.allclose(pytorch_dW0, dW0_manual)}\")\n",
    "\n",
    "print(\"\\nfc1.bias gradient:\")\n",
    "print(f\"  PyTorch:  {pytorch_db0}\")\n",
    "print(f\"  Manual:   {db0_manual}\")\n",
    "print(f\"  Match: {np.allclose(pytorch_db0, db0_manual)}\") # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6ac06b",
   "metadata": {},
   "source": [
    "### Summary: What We Learned\n",
    "\n",
    "This summary has been written with Sonnet 4.5 from Anthropic.\n",
    "\n",
    "**PyTorch automates backpropagation, but we can verify it matches our manual calculations!**\n",
    "\n",
    "1. **Capturing intermediate activations**: By storing values (Z1, A1, Z2) as instance variables in our custom `forward()` method, we can inspect what happens during PyTorch's forward pass without manual computation.\n",
    "\n",
    "2. **Accessing gradients**: After calling `loss.backward()`, PyTorch stores gradients in each parameter's `.grad` attribute:\n",
    "   - `model.fc1.weight.grad` → gradient with respect to W0\n",
    "   - `model.fc1.bias.grad` → gradient with respect to b0\n",
    "   - `model.fc2.weight.grad` → gradient with respect to W1\n",
    "   - `model.fc2.bias.grad` → gradient with respect to b1\n",
    "\n",
    "3. **Verification**: We manually computed the same gradients using NumPy (exactly like our previous implementation) and confirmed PyTorch's automatic gradients match perfectly.\n",
    "\n",
    "4. **The key insight**: PyTorch's `loss.backward()` performs the exact same chain rule calculations we coded manually:\n",
    "   - `dZ2 = A2 - y` (for BCE + Sigmoid)\n",
    "   - `dA1 = dZ2 @ W1.T`\n",
    "   - `dZ1 = dA1 * sigmoid'(A1)`\n",
    "   - Then computes weight/bias gradients from these\n",
    "\n",
    "**The power of PyTorch: It handles all gradient calculations automatically and correctly, no matter how complex your architecture becomes!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ae1fab",
   "metadata": {},
   "source": [
    "## Bonus: GraphViz Visualization of the Model\n",
    "\n",
    "Note: this requires `graphviz` to be installed on your system. On macOS, you can install it via Homebrew:\n",
    "\n",
    "```bash\n",
    "brew install graphviz\n",
    "```\n",
    "\n",
    "On Windows, you would need to download and install it from the [Graphviz website](https://graphviz.org/download/) and make sure to add it to your system PATH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f95c9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchviz import make_dot\n",
    "\n",
    "# After forward pass\n",
    "output = inspect_model(x_sample)\n",
    "loss = criterion(output, y_sample)\n",
    "\n",
    "# Create visualization\n",
    "dot = make_dot(loss, params=dict(inspect_model.named_parameters()))\n",
    "# dot.render(\"computation_graph\", format=\"png\")\n",
    "dot  # Display in Jupyter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Syvaoppiminen I",
   "language": "python",
   "name": "nb-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
