{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be9d4d91",
   "metadata": {},
   "source": [
    "# Naive MLP for MNIST with PyTorch\n",
    "\n",
    "This notebook implements a simple MLP for MNIST digit classification using PyTorch. The architecture mirrors the reference implementation (784-256-128-10) from PyImageSearch's Adrian Rosebrock (Deep Learning 4 Computer Vision with Python Volume 1). Key difference is that this uses PyTorch, not the old TensorFlow + Keras kombo. Also, this implementation uses both two different ways to visualize training progress:\n",
    "\n",
    "* Option A: Plot training history using matplotlib (stored in local variables)\n",
    "* Option B: Log training progress to TensorBoard (stored in files in `runs/` folder)\n",
    "\n",
    "This is nowhere near a state-of-the-art model, but serves as a good starting point for learning PyTorch basics.\n",
    "\n",
    "## Imports\n",
    "\n",
    "Note that it is a good idea to keep code organized even if you work with Notebook Cells. Instead of importing libraries all over the place, keep all imports in one cell at the top of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ffc2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "from pathlib import Path\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c8620a",
   "metadata": {},
   "source": [
    "## Teacher's setup\n",
    "\n",
    "This Notebook has been initially run with a Macbook Pro with M2 Max chip. The code should support MPS, CUDA and CPU execution. If you have some other backend, adjust the code accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcde1f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"MPS available:\", torch.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95db99cc",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbe38f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "USE_GPU = True  # Toggle this to False to use CPU instead\n",
    "\n",
    "# Hyperparameters\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# Device selection\n",
    "if USE_GPU and torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(f\"Using device: {device}\")\n",
    "elif USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using device: {device}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "# Option B: TensorBoard setup for logging metrics to disk\n",
    "device_name = str(device)\n",
    "run_name = f\"mnist_mlp_{device_name}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "writer = SummaryWriter(f'runs/{run_name}')\n",
    "print(f\"TensorBoard logging to: runs/{run_name}\")\n",
    "\n",
    "# Option B: Log configuration information to TensorBoard\n",
    "writer.add_text('config/device', device_name)\n",
    "writer.add_text('config/hyperparameters', \n",
    "                f'LR: {LEARNING_RATE}, Batch: {BATCH_SIZE}, Epochs: {EPOCHS}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7ce3d6",
   "metadata": {},
   "source": [
    "### Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae7e3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # MNIST mean and std\n",
    "])\n",
    "\n",
    "# Download and load the training data\n",
    "print(\"[INFO] accessing MNIST...\")\n",
    "trainset = datasets.MNIST('./data', download=True, train=True, transform=transform)\n",
    "testset = datasets.MNIST('./data', download=True, train=False, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {len(trainset)}\")\n",
    "print(f\"Test samples: {len(testset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b24e939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify class labels\n",
    "# MNIST labels are digits 0-9, where the label directly corresponds to the digit\n",
    "print(\"MNIST classes:\", trainset.classes)\n",
    "print(\"Class to index mapping:\", trainset.class_to_idx)\n",
    "print(\"\\nVerification: Labels are digits 0-9 in order\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0291110d",
   "metadata": {},
   "source": [
    "### Define the Model\n",
    "\n",
    "The model architecture is 784-256-128-10, matching the reference implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4feb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        # 784-256-128-10 architecture\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Flatten the input\n",
    "        x = x.view(-1, 784)\n",
    "        # First layer with sigmoid activation\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        # Second layer with sigmoid activation\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        # Output layer with softmax (will use cross entropy loss which includes softmax)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model and move to device\n",
    "model = MLP().to(device)\n",
    "print(model)\n",
    "\n",
    "# Option B: Log model architecture graph to TensorBoard\n",
    "dummy_input = torch.randn(1, 1, 28, 28).to(device)\n",
    "writer.add_graph(model, dummy_input)\n",
    "print(\"Model graph logged to TensorBoard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b9ab8f",
   "metadata": {},
   "source": [
    "### Train the Model\n",
    "\n",
    "The loss function is Cross Entropy Loss. You may see some materials using `NLLLoss` instead. Difference is that `NLLLoss` expects log-probabilities as input, while `CrossEntropyLoss` expects raw logits. Since our model outputs raw logits, we use `CrossEntropyLoss`. If we swapped to `NLLLoss`, we would need to add a `LogSoftmax` layer at the end of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376fc930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Option A: Initialize local history dictionary to store metrics in memory\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': []\n",
    "}\n",
    "\n",
    "print(\"[INFO] training network...\")\n",
    "for epoch in range(EPOCHS):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    for batch_idx, (inputs, labels) in enumerate(trainloader):\n",
    "        # Move data to device\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Option B: Log batch-level loss to TensorBoard every 50 batches\n",
    "        if batch_idx % 50 == 0:\n",
    "            global_step = epoch * len(trainloader) + batch_idx\n",
    "            writer.add_scalar('Loss/train_batch', loss.item(), global_step)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    # Calculate averages\n",
    "    train_loss = train_loss / len(trainloader)\n",
    "    train_acc = train_correct / train_total\n",
    "    val_loss = val_loss / len(testloader)\n",
    "    val_acc = val_correct / val_total\n",
    "    \n",
    "    # Option A: Append epoch metrics to local history lists\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    # Option B: Log comparative metrics (train vs val) to TensorBoard\n",
    "    writer.add_scalars('Loss/train_vs_val', {\n",
    "        'train': train_loss,\n",
    "        'val': val_loss\n",
    "    }, epoch)\n",
    "    writer.add_scalars('Accuracy/train_vs_val', {\n",
    "        'train': train_acc,\n",
    "        'val': val_acc\n",
    "    }, epoch)\n",
    "    \n",
    "    # Print progress every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}] - \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "print(\"[INFO] training complete!\")\n",
    "\n",
    "# Option B: Log final hyperparameters with results for comparison across runs\n",
    "writer.add_hparams(\n",
    "    {'lr': LEARNING_RATE, 'batch_size': BATCH_SIZE, 'epochs': EPOCHS, 'device': device_name},\n",
    "    {'hparam/final_train_acc': train_acc, 'hparam/final_val_acc': val_acc,\n",
    "     'hparam/final_train_loss': train_loss, 'hparam/final_val_loss': val_loss}\n",
    ")\n",
    "\n",
    "# Option B: Close the TensorBoard writer to flush all remaining data\n",
    "writer.close()\n",
    "print(\"TensorBoard logs saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51507a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory if it doesn't exist\n",
    "Path('models').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save the model as state dict only. \n",
    "# We will use the runname to differentiate models.\n",
    "model_path = f'models/{run_name}_model.pth'\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e01133f",
   "metadata": {},
   "source": [
    "### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127e1411",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] evaluating network...\")\n",
    "model.eval()\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in testloader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "# Convert to numpy arrays\n",
    "all_predictions = np.array(all_predictions)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(all_labels, all_predictions,\n",
    "                          target_names=[str(x) for x in range(10)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6324a0",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7b600e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Define class names (MNIST digits 0-9)\n",
    "# In MNIST, the class labels are the actual digit values (0-9)\n",
    "# and they are already in the correct order\n",
    "class_names = [str(i) for i in range(10)]\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "# Option 1: Text-based confusion matrix (most compact)\n",
    "print(\"Confusion Matrix (rows=true, cols=predicted):\")\n",
    "print(\"     \", \"  \".join(f\"{i:4}\" for i in range(10)))\n",
    "print(\"    \" + \"-\" * 65)\n",
    "for i, row in enumerate(cm):\n",
    "    print(f\"{i:2} | \", \"  \".join(f\"{val:4}\" for val in row))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbca155",
   "metadata": {},
   "source": [
    "### Visualize Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965c2a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Plot training history from local variables using matplotlib\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.arange(0, EPOCHS), history[\"train_loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, EPOCHS), history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, EPOCHS), history[\"train_acc\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, EPOCHS), history[\"val_acc\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dc035e",
   "metadata": {},
   "source": [
    "### View TensorBoard\n",
    "\n",
    "**Option B: View TensorBoard logs in your browser**\n",
    "\n",
    "To view the training results in TensorBoard, run the following command in a terminal:\n",
    "\n",
    "```bash\n",
    "tensorboard --logdir=runs\n",
    "```\n",
    "\n",
    "Then open your browser to [http://localhost:6006](http://localhost:6006)\n",
    "\n",
    "TensorBoard will show:\n",
    "- **Scalars**: Training and validation loss/accuracy over time\n",
    "- **Graphs**: Model architecture visualization\n",
    "- **HParams**: Comparison of different runs with different hyperparameters\n",
    "- **Text**: Configuration information"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Syvaoppiminen I",
   "language": "python",
   "name": "nb-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
