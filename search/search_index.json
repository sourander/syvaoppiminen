{"config":{"lang":["fi"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Syv\u00e4oppiminen I","text":"<p>Warning</p> <p>T\u00e4m\u00e4 sivusto on kokonaisuudessaan ty\u00f6n alla. Valmistuu kev\u00e4\u00e4ll\u00e4 2026.</p> <p>Kurssinmateriaalin kirjoittanut opettaja ei ole matematiikan opettaja, mutta kurssilla esiintyy silti pieni m\u00e4\u00e4r\u00e4 matematiikkaa. Matematiikka on selitetty ns. data engineerin n\u00e4k\u00f6kulmasta. Kurssilla pysyt\u00e4\u00e4n intuition tasolla. Mit\u00e4 matematiikkaa sitten AI:n kontekstissa tarvitaan? Yleisesti ottaen differentiaali- ja integraalilaskentaa, lineaarialgebraa, todenn\u00e4k\u00f6isyyslaskentaa ja tilastotiedett\u00e4. Joissakin AI:n osa-alueissa voi tulla tarvetta my\u00f6s diskreetin matematiikan, optimoinnin perusteisiin, peliteorian tai matriisilaskennan perusteisiin. T\u00e4m\u00e4n kurssin matematiikka on kuitenkin hyvin kevytt\u00e4. On kuitenkin suositeltavaa heijastella t\u00e4m\u00e4n kurssin sis\u00e4lt\u00f6j\u00e4 opetussuunnitelman mukaisiin matematiikan opintoihin. Tutki, kuinka matematiikan harjoituksissa esiintyneet k\u00e4sitteet n\u00e4kyv\u00e4t t\u00e4ss\u00e4 kurssissa.</p>"},{"location":"exercises/","title":"Teht\u00e4v\u00e4kooste","text":"<p>T\u00e4ss\u00e4 tiedostossa on listattuna kaikki materiaalin teht\u00e4v\u00e4t. Osa niist\u00e4 ei v\u00e4ltt\u00e4m\u00e4tt\u00e4  kuulu sinun kurssitoteutukseesi. Huomaa, ett\u00e4 t\u00e4m\u00e4 tiedosto luodaan automaattisesti parsimalla  kaikki repositorion Markdown-tiedostot l\u00e4pi. Mik\u00e4li huomaat puuttuvia teht\u00e4vi\u00e4, ilmoita opettajalle. Otsikon per\u00e4ss\u00e4 suluissa oleva numero on j\u00e4rjestysprioriteetti: se vaikuttaa vain t\u00e4m\u00e4n listan j\u00e4rjestykseen.</p> <p>Lista hy\u00f6dynt\u00e4\u00e4 Material for MkDocs -teeman Tasklist -ominaisuutta.</p> <p>Kopioi teht\u00e4v\u00e4lista leikep\u00f6yd\u00e4lle ja muokkaa se sinun k\u00e4ytt\u00f6\u00f6si sopivaksi.</p> <pre><code>## Neuroverkot 101 (100)\n\n- [ ] Teht\u00e4v\u00e4: TensorFlow Playground\n- [ ] Teht\u00e4v\u00e4: Quick, Draw!\n- [ ] Teht\u00e4v\u00e4: UDLbook Shallow\n- [ ] Teht\u00e4v\u00e4: ANN ja el\u00e4inkunta\n- [ ] Teht\u00e4v\u00e4: BERT Large\n\n## Syv\u00e4t neuroverkot (110)\n\n- [ ] Teht\u00e4v\u00e4: UDLbook Deep\n- [ ] Teht\u00e4v\u00e4: Valitse kehitysymp\u00e4rist\u00f6si\n- [ ] Teht\u00e4v\u00e4: Aja MNIST MLP koodi\n- [ ] Teht\u00e4v\u00e4: TensorBoard\n- [ ] Teht\u00e4v\u00e4: Mallin tarkkuus CPU vs MPS vs CUDA\n\n## Vektorointi (200)\n\n- [ ] Teht\u00e4v\u00e4: Tutustu vektorointiin\n- [ ] Teht\u00e4v\u00e4: Tutustu NumpyNNwithBCE -malliin\n</code></pre>"},{"location":"aikasarjat/ideat/","title":"Aikasarjat","text":"<p>T\u00e4m\u00e4 tullaan k\u00e4ym\u00e4\u00e4n l\u00e4pi l\u00e4hinn\u00e4 loppukaneettina kurssille.</p> <p>TODO! T\u00e4h\u00e4n tulee ainakin seuraavat asiat:</p> <ul> <li>Mit\u00e4 ovat aikasarjat (time series)</li> <li>Miten aikasarjoja mallinnetaan (esim. RNN, LSTM, GRU)</li> <li>Case: Aikasarjamallinnus jollain datalla</li> </ul>"},{"location":"kieli/nlp/","title":"Luonnollinen kieli (NLP)","text":"<p>TODO! T\u00e4h\u00e4n tulee ainakin seuraavat asiat:</p> <ul> <li>Mik\u00e4 on luonnollinen kieli?</li> <li>Luonnollisen kielen k\u00e4sittelyn perusteet (NLP, Natural Language Processing)</li> <li>Haasteita (eli miksei FC/MLP toimi hyvin)</li> <li>Esimerkkej\u00e4 NLP-malleista<ul> <li>RNN</li> <li>Transformers</li> </ul> </li> <li>Sanavektorit (word embeddings)</li> <li>Kenties transfer learning siten ett\u00e4 fine tunataan Gemma 3 270M tai vastaava pieni malli?</li> </ul> <p>Ty\u00f6kaluna tutustutaan kirjastoon SpaCy.</p>"},{"location":"konvoluutio/cnn/","title":"Konvoluutioverkot (CNN)","text":"<p>TODO! T\u00e4h\u00e4n tulee ainakin seuraavat asiat:</p> <ul> <li>Kuvantunnistuksen perusteet</li> <li>Mik\u00e4 on konvoluutio ja miten se liittyy kuviin</li> <li>Quickdraw -esimerkki</li> <li>Tutustutaan my\u00f6s generatiivisiin malleihin, esim. GAN ja Diffusion pintapuoleisesti</li> <li>Esimerkkin\u00e4 ehk\u00e4 Deep Dream tai Style Transfer?</li> <li>K\u00e4yd\u00e4\u00e4n MNIST + CNN l\u00e4pi. T\u00e4h\u00e4n l\u00f6ytyy gh:pytorch/examples/blob/main/mnist/main.py</li> </ul>"},{"location":"mallinnus/todo/","title":"Ideatasolla","text":"<p>T\u00e4m\u00e4n osion rakenne vaatii miettimist\u00e4. Tulee selitt\u00e4\u00e4 ainakin:</p> <ul> <li>Essential Math for AI kirjasta luvun 3 alusta struktuuri.</li> <li>Mallinnuksen periaatteet (luvun 4 alku)<ul> <li>Training function<ul> <li>Linearly combine, add bias, then activate</li> <li>Activation functions</li> <li>Vanishing gradient problem</li> </ul> </li> <li>The loss function</li> <li>Optimization </li> </ul> </li> <li>Kouluttamisen k\u00e4yt\u00e4nn\u00f6t:<ul> <li>Datan m\u00e4\u00e4r\u00e4 (Machine Learning Yearning sivu 11)</li> <li>Train/validation/test split</li> <li>Over ja underfitting</li> <li>Regularisointi (dropout, weight decay)</li> <li>Error analysis (Machine Learning Yearning sivu 32)</li> <li>Tuotanto (saving and loading models, versioning)</li> <li>Mallin kevent\u00e4minen (kvantisointi, pruning)</li> </ul> </li> <li>Yksitt\u00e4inen kokonaisuus alusta loppuun (esim. Fashion-MNIST)</li> </ul>"},{"location":"neuroverkot/neuroverkot_101/","title":"Neuroverkot 101","text":""},{"location":"neuroverkot/neuroverkot_101/#maaritelma","title":"M\u00e4\u00e4ritelm\u00e4","text":""},{"location":"neuroverkot/neuroverkot_101/#koneoppimista","title":"Koneoppimista","text":"<p>Neuroverkot (engl. neural networks) ovat koneoppimisen malleja, jotka on inspiroitu ihmisen aivojen rakenteesta ja toiminnasta. Neuroverkot pystyv\u00e4t oppimaan monimutkaisia kuvioita datasta, ja niit\u00e4 k\u00e4ytet\u00e4\u00e4n laajalti erilaisissa sovelluksissa, kuten kuvantunnistuksessa, puheentunnistuksessa ja luonnollisen kielen k\u00e4sittelyss\u00e4. Koneoppimisen (mukaan lukien sen alaisuuteen kuuluvan syv\u00e4oppimisen) voi tiiviisti m\u00e4\u00e4ritell\u00e4 seuraavasti:</p> <p>\"Fit a given set of data points into an appropriate function (mapping an input to an output) that picks up on the important signals in the data and ignores the noise, then make sure this function performs well on new data.\"</p> <p>\u2014 Hala Nelson <sup>1</sup></p> <p>Kannattaa kerrata omista Johdatus koneoppimiseen -kurssin muistiinpanoista, kuinka teko\u00e4ly, koneoppiminen ja syv\u00e4oppiminen liittyv\u00e4t toisiinsa. Voit my\u00f6s kerrata sielt\u00e4 muita m\u00e4\u00e4ritelmi\u00e4 koneoppimisesta.</p> <p>Se, kuinka neuroverkot eroavat Johdatus koneoppimiseen -kurssin malleista, on:</p> <ul> <li>Neuroverkot pystyv\u00e4t oppimaan itse piirteet (feature learning). T\u00e4m\u00e4 ei tarkoita, ett\u00e4 feature engineering vaihe olisi turha, mutta t\u00e4t\u00e4 ty\u00f6t\u00e4 voi ulkoistaa neuroverkolle.</li> <li>Neuroverkot pystyv\u00e4t mallintamaan monimutkaisempia, ep\u00e4lineaarisia suhteita datassa.</li> </ul>"},{"location":"neuroverkot/neuroverkot_101/#epalineaarista","title":"Ep\u00e4lineaarista","text":"<p>Varmistetaan jo heti kurssin alussa, ett\u00e4 on ymm\u00e4rrys siit\u00e4, mit\u00e4 lineaarisuus tarkoittaa t\u00e4ss\u00e4 kontekstissa. Lineaariset kaavat tai funktiot on helppo tunnistaa siit\u00e4, ett\u00e4 ne on helppo kirjoittaa. Piirteet esiintyv\u00e4t funktiossa omassa luonnollisessa muodossaan. Funktiossa ei esiinny esimerkiksi: neli\u00f6juuria, potensseja, logaritmisa, sini\u00e4, cosinia tai muuta selke\u00e4sti ei-lineaarista. <sup>1</sup> Esimerkki Pythonilla olisi:</p> <pre><code>def linear_function(x1, x2, x3):\n    return w1 * x1 + w2 * x2 - w3 * x3 + w0\n</code></pre> <p>Ei-lineaariset funktiot ovat monimutkaisempia. Sy\u00f6tteen ja tuloksen v\u00e4lill\u00e4 on ep\u00e4lineaarinen suhde. <sup>1</sup> Esimerkki Pythonilla:</p> <pre><code>import math\n\ndef nonlinear_function(x1, x2, x3):\n    return w1 * math.sqrt(x1) + w2 * (x2 / x3) + w3 * math.sin(x3) + w0\n</code></pre> <p>Neuroverkot kykenev\u00e4t mallintamaan ep\u00e4lineaarisia funktioita, koska ne k\u00e4ytt\u00e4v\u00e4t ep\u00e4lineaarisia aktivointifunktioita (kuten ReLU, sigmoid tai tanh) piilotetuissa kerroksissaan. T\u00e4m\u00e4 mahdollistaa monimutkaisten kuvioiden oppimisen datasta.</p>"},{"location":"neuroverkot/neuroverkot_101/#historia","title":"Historia","text":""},{"location":"neuroverkot/neuroverkot_101/#all-or-nothing","title":"All or nothing","text":"<p>Syv\u00e4oppiminen ei suinkaan ole syntynyt ChatGPT:n my\u00f6t\u00e4 2020-luvulla. Vuonna 1943 Warren McCulloch ja Walter Pitts julkaisivat artikkelin \"A Logical Calculus of the Ideas Immanent in Nervous Activity\", jossa he esittiv\u00e4t yksinkertaisen mallin keinotekoisesta neuronista. Malli perustui \"all-or-nothing\" -periaatteeseen, jossa neuronin aktivaatio tapahtuu, kun sy\u00f6tteiden painotettu summa ylitt\u00e4\u00e4 tietyn kynnyksen. Kyseess\u00e4 olivat siis bin\u00e4\u00e4riset neuronit, jotka toimivat loogisina portteina. <sup>2</sup></p> <p>\"Pitts was self-taught, and by age 12, had received an offer to study at Cambridge University with the great Bertrand Russell. He did not take up this invitation, and indeed throughout his life did not accept any offers of advanced degrees or positions of authority. Most of his famous work was done while he was homeless.\"</p> <p>\u2014 Gugger &amp; Howard, Deep Learning for Coders with fastai and PyTorch <sup>2</sup></p>"},{"location":"neuroverkot/neuroverkot_101/#dartmouth-ja-ain-synty","title":"Dartmouth ja AI:n synty","text":"<p>Vaikka McCulloch ja Pitts olivat jo 1943 luoneet teoreettisen pohjan keinotekoisille neuroneille, AI:n syntym\u00e4hetki tieteenalana on kes\u00e4ll\u00e4 1956 Dartmouth Collegessa j\u00e4rjestetty ty\u00f6paja Dartmouth Summer Research Project \u2013 ainakin yliopiston itsens\u00e4 mukaan <sup>3</sup>. John McCarthy toimi kokoonpanijana ja muita j\u00e4rjest\u00e4ji\u00e4 olivat Marvin Minsky, Claude Shannon ja Nathaniel Rochester.</p> <p>\"We propose that a 2-month, 10-man study of artificial intelligence be carried out during the summer of 1956 at Dartmouth College in Hanover, New Hampshire. [...] An attempt will be made to find how to make machines use language, form abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves.\"</p> <p>\u2014 A Proposal for the Dartmouth Summer Research Project on Artificial Intelligence, 1955 <sup>12</sup></p> <p>Ty\u00f6pajan aivoriihiss\u00e4 k\u00e4siteltiin jo tuolloin monia alueita, jotka ovat edelleen keskeisi\u00e4. Ty\u00f6pajasta on jo yli 70 vuotta, mutta silti siihen t\u00f6rm\u00e4\u00e4 yh\u00e4: John K. Thompson esitteli Helsinki Data Week 2025:ssa t\u00e4m\u00e4n kes\u00e4projektin AI:n ensimm\u00e4isen\u00e4 askeleena Keynote-puheessaan.</p>"},{"location":"neuroverkot/neuroverkot_101/#kissakokeet","title":"Kissakokeet","text":"<p>50-luvun lopulla David Hubel ja Torsten Wiesel tutkivat, kuinka aivokuoressa k\u00e4sitell\u00e4\u00e4n visuaalista informaatiota. He havaitsivat, ett\u00e4 tietyt neuronit reagoivat spesifisiin visuaalisiin \u00e4rsykkeisiin, kuten viivoihin ja reunoihin <sup>4</sup>. He saivat t\u00e4st\u00e4 Nobelin fysiologian ja l\u00e4\u00e4ketieteen palkinnon vuonna 1981. <sup>5</sup> Hubel ja Wiesel suorittivat kokeensa n\u00e4ytt\u00e4m\u00e4ll\u00e4 visuaalisia \u00e4rsykkeit\u00e4 anestesioiduille kissoille. N\u00e4iden 24 kissaraudan hermosolujen aktivaatiota tarkkailtiin mittalaitteilla aivokuoresta. <sup>6</sup> Lieneek\u00f6 kirja (ja elokuva) Kellopeliappelsiini ottanut t\u00e4st\u00e4 vaikutteita?</p> <p>Hubel ja Wiesel yrittiv\u00e4t aluksi stimuloida kissojen n\u00e4k\u00f6aivokuoren neuroneja yksinkertaisilla muodoilla kuten pisteill\u00e4, mutta eiv\u00e4t saaneet mit\u00e4\u00e4n vastetta. Sitten sattuma ja vahinko astuivat peliin: \"And then, as with many of the great discoveries, from X-rays to penicillin to the microwave oven, Hubel and Wiesel made a serendipitous observation: As they removed one of their slides from the projector, its straight edge elicited the distinctive crackle of their recording equipment to alert them that a primary visual cortex neuron was firing.\" <sup>4</sup> N\u00e4m\u00e4 neuronit, joihin signaali tulee ensimm\u00e4isen\u00e4 silmien suunnasta, saivat nimen \"simple cells\". N\u00e4m\u00e4 solut kytkeytyv\u00e4t edelleen monimutkaisempiin \"complex cells\" -soluihin, jotka reagoivat viivoihin ja reunoihin eri kulmissa. Kun n\u00e4it\u00e4 verkkoja kasataan useita kerroksia, meill\u00e4 on syv\u00e4 neuroverkko, joka pystyy tunnistamaan monimutkaisia kuvioita, kuten kasvoja, esineit\u00e4 ja maisemia.</p> <p></p> <p>Kuva 1: Otos Hubelin ja Wieselin tutkimuksesta. Kuva n\u00e4ytt\u00e4\u00e4, miten primaari n\u00e4k\u00f6aivokuori (V1) reagoi eri kohtiin verkkokalvon n\u00e4k\u00f6kent\u00e4ss\u00e4 osuvista pistem\u00e4isist\u00e4 valoista reseptiivisen kent\u00e4n kautta. <sup>6</sup></p> <p>Tip</p> <p>Aivokuori on evoluution my\u00f6h\u00e4inen kehitystuote, joka selitt\u00e4\u00e4 nis\u00e4kk\u00e4iden monimutkaista k\u00e4ytt\u00e4ytymist\u00e4 verrattuna vanhempiin el\u00e4inryhmiin. <sup>4</sup></p> <p>Aivoja kutsutaan \"harmaaksi aineeksi\", koska ulkopinta (aivokuori) on harmaata. Suurin osa aivoista on kuitenkin valkoista ainetta, joka kuljettaa tietoa pitki\u00e4 matkoja. Sen hermosolut on p\u00e4\u00e4llystetty valkoisella rasvaisella kalvolla, joka nopeuttaa signaalien johtumista. <sup>4</sup></p>"},{"location":"neuroverkot/neuroverkot_101/#perceptron","title":"Perceptron","text":"<p>1950-luvun tapahtui samanaikaisesti my\u00f6s muuta. Frank Rosenblatt jalosti McCulloch ja Pittsin ajatusta kehitt\u00e4en tavan kouluttaa neuroverkkoja. N\u00e4in syntyi Mark I Perceptron, joka oli yksi ensimm\u00e4isist\u00e4 neuroverkkopohjaisista malleista. <sup>2</sup> Kyseinen malli ei ollut teoriaa vaan se saatiin toteutettua siten, ett\u00e4 se tunnisti yksinkertaisia kuvioita, kuten neli\u00f6n ja ympyr\u00e4n. Voit tutustua alkuper\u00e4iseen artikkeliin l\u00e4hteiden kautta l\u00f6ytyv\u00e4st\u00e4 PDF-tiedostosta. <sup>7</sup></p> <p></p> <p>Kuva 2: Lehtileike Research Trends -lehden kes\u00e4n 1958 numerosta. Huomaa ingressi: \"Introducing the perceptron \u2014 A machine which senses, recognizes, remembers, and responds like the human mind.\" <sup>7</sup></p>"},{"location":"neuroverkot/neuroverkot_101/#ai-talven-alku","title":"AI-talven alku","text":"<p>MIT:n tohtori Marvin Minsky ja Seymour Papert julkaisivat vuonna 1969 kirjan Perceptrons, jossa he osoittivat, ett\u00e4 yksitt\u00e4inen kerros ei pysty ratkaisemaan tiettyj\u00e4 ongelmia, kuten XOR-ongelmaa. Howard ja Gugger <sup>2</sup> toteavat, ett\u00e4 samassa Perceptrons-kirjassa esiteltiin my\u00f6s ratkaisuja ongelmaan, kuten useiden kerrosten k\u00e4ytt\u00f6. Heid\u00e4n mukaansa vain rajoitukset saivat huomiota, ja n\u00e4in alkoi parin vuosikymmenen \"talvi\", jolloin neuroverkkojen tutkimus hiipui.</p>"},{"location":"neuroverkot/neuroverkot_101/#80-luvun-kevat","title":"80-luvun kev\u00e4t","text":"<p>AI-talven j\u00e4lkeen alkoi taas tapahtua. 1986 David Rumelhart, James McClelland ja PDP Research Group julkaisivat moniosaisin artikkelin Parallel Distributed Processing (PDP). Howard ja Gugger nostavat t\u00e4m\u00e4n viimeisen 50 vuoden k\u00e4\u00e4nteentekevimm\u00e4ksi julkaisuksi. PDP:n asettamat vaatimukset, kuten \"joukko prosessointiyksik\u00f6it\u00e4\" ja \"ulostulon funktio\", ovat edelleen keskeisi\u00e4 neuroverkkojen m\u00e4\u00e4ritelm\u00e4ss\u00e4. Skaala on toki kasvanut: 80-luvulla verkoissa oli yleens\u00e4 2 kerrosta. Jo t\u00e4ll\u00f6in, 80- ja 90-luvuilla, koneoppimisella oli jo oikeita k\u00e4ytt\u00f6tarkoituksia, mutta AI-tutkimus koki yh\u00e4 tietynlaista AI-talvea. Vasta \u00e4skett\u00e4in, 2010-luvun lopulla, alkoi nykyinen AI-kev\u00e4t. <sup>2</sup> Avainsanoja t\u00e4ss\u00e4 nykyisen kev\u00e4\u00e4n alussa ovat esimerkiksi AlexNet, AlphaGo, GAN, GPU.</p> <p>Video 1: Stanfordin tohtori Jay McClelland vastaa Podcastiss\u00e4 kysymykseen \"What is Parallel Distributed Processing?\" eli PDP. Videolla keskustellaan siit\u00e4, miksi ihminen muistaa helpommin h\u00e4nelle esitetyn kirjaimen, jos esitetyt kirjaimet muodostavat sanan.</p> <p>Video 2: Yann LeCun esiintyy vuonna 1989 videolla esittelem\u00e4ss\u00e4 LeNEt 1 -verkkoa (9760 parametria), joka kykenee tunnistamaan k\u00e4sinkirjoitettuja numeroita. Datasetti tunnetaan nimell\u00e4 MNIST. T\u00e4m\u00e4 kyseinen video on ConvNet-verkon avulla kuvanlaadultaan paranneltu versio alkuper\u00e4isest\u00e4. Videolla esiintyy siis konvoluutioverkkojen esi-is\u00e4, ja videota on korjailtu vuosikymmeni\u00e4 my\u00f6hemmin sen kunnioittamiseksi.</p> <p>80-luvulla vaikuttivat my\u00f6s 2024 Nobelilla palkitut John J. Hopfield ja Geoffrey Hinton. <sup>8</sup> Daniel Crevier nostaa Hopfieldin ty\u00f6n merkityksen esilleen esiin kirjassaan AI: The Tumultuous Search for Artificial Intelligence. H\u00e4n kirjoittaa, ett\u00e4 AI-talvi loppui osaltaan 80-luvulla Hopfieldin julkaisuun, mainiten my\u00f6s PDP:n sek\u00e4 backpropagation-algoritmin, joihin kumpaankin liittyy Rummelhart. <sup>9</sup> Tai siis, t\u00e4m\u00e4 on kenties se tunnetuin l\u00e4hde. Ensimm\u00e4inen lienee Seppo Linnainmaa pro gradu -ty\u00f6ss\u00e4\u00e4n vuonna 1970, joskaan ei neuroverkon kontekstissa. T\u00e4st\u00e4 huolimatta: \"As of 2020, all modern software packages for NNs (such as Google's Tensorflow) are based on Linnainmaa's method of 1970.\" <sup>10</sup></p> <p></p> <p>Kuva 3: Hopfield network -mallin toimintaa graafisesti kuvattuna. Vastaava maisemassa vaeltaminen lienee Johdatus koneoppimiseen -kurssin Gradient Descent -osiosta tuttu. Kuva: \u00a9Johan Jarnestad/The Royal Swedish Academy of Sciences</p>"},{"location":"neuroverkot/neuroverkot_101/#matalat-neuroverkot","title":"Matalat neuroverkot","text":""},{"location":"neuroverkot/neuroverkot_101/#viittaus-koneoppimiseen","title":"Viittaus koneoppimiseen","text":"<p>Ennen kuin tutustumme aiheen syv\u00e4\u00e4m p\u00e4\u00e4tyyn eli syviin neuroverkkoihin (engl. deep neural networks), on hyv\u00e4 tarkistella matalia neuroverkkoja (engl. shallow neural networks). Kertaa alkuun Johdatus koneoppimiseen kurssilta Normaaliyht\u00e4l\u00f6, Gradient Descent sek\u00e4 Logistinen regressio. Kyseisell\u00e4 kurssilla sinulle on kerrottu, ett\u00e4 n\u00e4iss\u00e4 aiheissa on pohja neuroverkkojen ymm\u00e4rrykselle. Nyt on siihen paneutumisen aika.</p> <p>Tavallisen 1D-regressiomallin rajoituksia ovat <sup>11</sup>, ett\u00e4 se voi mallintaa:</p> <ul> <li>vain viivan</li> <li>yhden inputin</li> <li>yhden outputin</li> </ul> <p>N\u00e4it\u00e4 rajoituksia kierrettiin Johdatus koneoppimiseen kurssilla osin k\u00e4ytt\u00e4m\u00e4ll\u00e4 logistista regressiota, SGD:t\u00e4 ja polynomeja. Jos j\u00e4lkimm\u00e4inen ei her\u00e4t\u00e4 muistikuvia, kertaa scikitin dokumentaatiosta PolynomialFeatures, jonka avulla muuttujista <code>[a, b]</code> voi muodostaa toisen asteen polynomifunktion <code>[1, a, b, a^2, ab, b^2]</code>. Malli on yh\u00e4 lineaarinen parametrien suhteen, mutta muunnettu piirreavaruus mahdollistaa ep\u00e4lineaaristen kuvioiden mallintamisen alkuper\u00e4isess\u00e4 sy\u00f6teavaruudessa. T\u00e4m\u00e4 ei ehk\u00e4 ole tieteellisesti t\u00e4ysin p\u00e4tev\u00e4 vertaus, mutta voi auttaa: kuvittele, ett\u00e4 piirr\u00e4t logaritmiseen taulukkoon suoran viivan. Viiva on lineaarinen logaritmisessa avaruudessa, mutta alkuper\u00e4isess\u00e4 mittakaavassa (\"todellisuudessa\") se kuvaa eksponentiaalista k\u00e4yr\u00e4\u00e4.</p>"},{"location":"neuroverkot/neuroverkot_101/#maaritelma_1","title":"M\u00e4\u00e4ritelm\u00e4","text":"<p>Kuva 4: Matala neuroverkko koostuu kerroksista: sy\u00f6te (input), piilotettu (hidden) ja tuloste (output). Kerrokset yhdist\u00e4v\u00e4t eteenp\u00e4in suunnatut yhteydet (nuolet), joten n\u00e4it\u00e4 kutsutaan eteenp\u00e4in sy\u00f6tt\u00e4viksi verkoiksi (feed-forward networks). Kun jokainen muuttuja yhdistyy kaikkiin seuraavan kerroksen muuttujiin, kyseess\u00e4 on t\u00e4ysin yhdistetty verkko. Yhteydet edustavat painokertoimia, piilokierroksen muuttujia kutsutaan neuroneiksi tai piiloyksikk\u00f6iksi (hidden units). (CC-BY-NC-ND) <sup>11</sup></p> <p>Yll\u00e4 n\u00e4kyv\u00e4n kuvan verkosta tekee matalan se, ett\u00e4 siin\u00e4 on vain yksi piilokerros. Jos kerroksia olisi useita, kyseess\u00e4 olisi syv\u00e4 neuroverkko. N\u00e4ihin tutustumme my\u00f6hemmin.</p> <p>Nyt pyyd\u00e4n sinua palaamaan takaisin Johdatus Koneoppimiseen kurssin Logistinen regressio osioon. Siell\u00e4 on esitelty logistinen regressio, joka on k\u00e4yt\u00e4nn\u00f6ss\u00e4 yksi neuroni. Jos kytket sy\u00f6tteen useisiin neuroneihin, saat piilokerroksen. Jos kytket piilokerroksen useisiin neuroneihin, saat tulostekerroksen (output). N\u00e4in sinulla on neuroverkko luotuna. Muista, ett\u00e4 tuloja ja l\u00e4ht\u00f6j\u00e4 voi olla useita.</p>"},{"location":"neuroverkot/neuroverkot_101/#mita-se-tekee","title":"Mit\u00e4 se tekee?","text":"<p>K\u00e4sitell\u00e4\u00e4n t\u00e4m\u00e4n otsikon alla seuraavanlaista verkkoa:</p> <p></p> <p>Kuva 5: Yksinkertainen neuroverkko, jossa on vain yksi sy\u00f6te x, kolme piilotettua neuronia ja yksi tulos. Vasemmanpuoleiseen versioon on lis\u00e4ttyn\u00e4 vakiotermi (intercept, bias), joka yleens\u00e4 j\u00e4tet\u00e4\u00e4n kuvaajista pois. (CC-BY-NC-ND) <sup>11</sup></p> <p>Kaikki kuvan nuolet ovat painoja (weights). Lineaarialgebrassa n\u00e4it\u00e4 kutsuttaisiin kulmakertoimiksi (slope), mutta neuroverkoissa termi on paino. Koska meill\u00e4 on 1 sis\u00e4\u00e4ntulo ja 3 neuronia, n\u00e4iden v\u00e4lill\u00e4 on <code>1 x 3</code> eli kolme painoa. Lis\u00e4ksi kutakin vakiotermi\u00e4 (bias) kohden on yksi paino, joten niit\u00e4 on kolme lis\u00e4\u00e4. Yhteens\u00e4 painoja on siis kuusi. Toivon mukaan t\u00e4m\u00e4 alkaa kuulostaa tutulta, kun mietit Johdatus koneoppimiseen kurssin normaaliyht\u00e4l\u00f6n matriisiesityst\u00e4, joka k\u00e4siteltiin Hill Climbing osiossa. Kuvaa tutkimalla huomaat, ett\u00e4 esimerkiksi \\(\\theta_{10}\\) ja \\(\\theta_{11}\\) vastaavat painoja, jotka yhdist\u00e4v\u00e4t sy\u00f6tteen \\(x\\) ja vakiotermin \\(1\\) piilotetun kerroksen ensimm\u00e4iseen neuroniin \\(h_1\\). Theta on siis 3x2 matriisi, joka n\u00e4ytt\u00e4\u00e4 t\u00e4lt\u00e4:</p> \\[ \\Theta = \\begin{bmatrix} \\theta_{10} &amp; \\theta_{11} \\\\ \\theta_{20} &amp; \\theta_{21} \\\\ \\theta_{30} &amp; \\theta_{31} \\end{bmatrix} \\] <p>Eli siis \\(h_1\\), \\(h_2\\) ja \\(h_3\\), tai tarkemmin niiden esiasteet (pre-activation), lasketaan seuraavasti:</p> \\[ \\begin{align*} h_{pre1} &amp;= \\theta_{10} + \\theta_{11} x \\\\ h_{pre2} &amp;= \\theta_{20} + \\theta_{21} x \\\\ h_{pre3} &amp;= \\theta_{30} + \\theta_{31} x \\end{align*} \\] <p>Yll\u00e4 olevissa lukee pienell\u00e4 <code>pre</code>, koska kyseess\u00e4 ovat esiasteet (pre-activations). N\u00e4ist\u00e4 saa varsinaiset piilotetun yksik\u00f6n aktivoinnit (activations) aktivointifunktion avulla. K\u00e4sittelemme aktivointifunktiot my\u00f6hemmin kattavammin, mutta t\u00e4ss\u00e4 v\u00e4liss\u00e4 riitt\u00e4\u00e4 hyv\u00e4ksy\u00e4, ett\u00e4 kunkin piilotetun kerroksen neuronin laskema arvo sy\u00f6tet\u00e4\u00e4n tyypillisesti ReLu-aktivointifunktioon, joka palauttaa nollan, jos sy\u00f6te on negatiivinen, ja sy\u00f6tteen itsens\u00e4, jos se on positiivinen.</p> <p></p> <p>Kuva 6: ReLu-aktivointifunktio. (CC-BY-NC-ND) <sup>11</sup></p> <p>Kun t\u00e4m\u00e4 aktivointifunktio, \\(f(z) = max(0, z)\\), joka tunnetaan jatkossa a-merkkin\u00e4, on otettu huomioon, piilotetun kerroksen arvot ovat siis:</p> \\[ \\begin{align*} h_1 &amp;= a(\\theta_{10} + \\theta_{11} x) \\\\ h_2 &amp;= a(\\theta_{20} + \\theta_{21} x) \\\\ h_3 &amp;= a(\\theta_{30} + \\theta_{31} x) \\end{align*} \\] <p>Yll\u00e4 olevassa kaavassa <code>x</code> on sy\u00f6te, \\(\\theta\\) on painot ja \\(h\\) on piilotetun kerroksen aktivoinnit eli varsinaiset hidden unit eli piiloyksik\u00f6t. N\u00e4iden lineaarinen yhdistelm\u00e4 antaa tuloksen <code>y</code>:</p> \\[ y = \\phi_0 + \\phi_1 h_1 + \\phi_2 h_2 + \\phi_3 h_3 \\] <p>Kaiken kaikkiaan mallin opittua parametrej\u00e4 ovat siis:</p> \\[ \\begin{align*} \\phi_0 &amp;= \\text{tuloskerroksen vakiotermi (bias)} \\\\ \\phi_1, \\phi_2, \\phi_3 &amp;= \\text{tuloskerroksen painot} \\\\ \\theta_{10}, \\theta_{20}, \\theta_{30} &amp;= \\text{piilotetun kerroksen muuttujien painot} \\\\ \\theta_{11}, \\theta_{21}, \\theta_{31} &amp;= \\text{piilotetun kerroksen vakioiden painot} \\end{align*} \\] <p>N\u00e4m\u00e4 nelj\u00e4 vaihetta, eli esiasteet, aktivoinnit, piilokerroksen l\u00e4ht\u00f6 ja viimeisen kerroksen tulos n\u00e4kyv\u00e4t alla olevassa kuvassa.</p> <p></p> <p>Kuva 7: Neuroverkon laskennan vaiheet <code>a-j</code>. Viimeisen kuvaajan varjostetussa alueessa \\(h_2\\) on passiivinen (leikattu), mutta \\(h_1\\) ja \\(h_3\\) ovat molemmat aktiivisia. (CC-BY-NC-ND) <sup>11</sup></p> <ul> <li>Esiasteet (a-c): Sy\u00f6te x sy\u00f6tet\u00e4\u00e4n kolmeen lineaarifunktioon, joista jokaisella on eri y-leikkauspiste ja kulmakerroin.</li> <li>Aktivoinnit (d-f): Jokainen lineaarifunktio sy\u00f6tet\u00e4\u00e4n ReLU-aktivointifunktioon, joka leikkaa negatiiviset arvot nollaan.</li> <li>Painotus (g-i): Kolmea leikattu funktiota painotetaan (skaalataan) kertoimilla \\(\\phi_1\\), \\(\\phi_2\\) ja \\(\\phi_3\\).</li> <li>Yhteenlasku (j): Leikatut ja painotetut funktiot summataan yhteen ja lis\u00e4t\u00e4\u00e4n offset-arvo \\(\\phi_0\\), joka kontrolloi korkeutta.</li> </ul> <p>Huomaa, ett\u00e4 kuvaajassa on kolme \"nivelt\u00e4\". T\u00e4st\u00e4 tulee termi piecewise linear function (suom. paloittain m\u00e4\u00e4ritelty lineaarinen funktio). Mik\u00e4li ennustettava ilmi\u00f6 on monimutkainen, tarvitaan useampia piilokerroksia, jotta t\u00e4m\u00e4 paloittain m\u00e4\u00e4ritelty funktio saadaan taiteltua haluttuun muotoon. Alla t\u00e4st\u00e4 viel\u00e4 havainnollistava kuva.</p> <p></p> <p>Kuva 8: Katkoviivalla n\u00e4kyv\u00e4\u00e4 todellista ilmi\u00f6t\u00e4 voi yritt\u00e4\u00e4 mallintaa eri piiloverkon kokoisilla malleilla. Vasemmanpuoleinen malli on selke\u00e4sti liian yksinkertainen, oikea on tarkka (joskin kenties liian tarkka.) (CC-BY-NC-ND) <sup>11</sup></p> <p>Jos haluat tutustua aiheeseen syvemmin, tutustu Understanding Deep Learning kirjaan, joka on ilmainen ja avoin verkossa. Kirjaan liittyv\u00e4 Qatarin yliopiston kurssi l\u00f6ytyy my\u00f6s ilmaiseksi YouTube: Deep Learning Fall 2024.</p>"},{"location":"neuroverkot/neuroverkot_101/#koulutus-ja-inferenssi","title":"Koulutus ja inferenssi","text":"<p>Kurssilla k\u00e4sitell\u00e4\u00e4n neuroverkkojen koulutusta ja inferenssi\u00e4 (eli mallin k\u00e4ytt\u00f6\u00e4). Esitell\u00e4\u00e4n termit jo kuitenkin t\u00e4ss\u00e4 alkuvaiheessa, koska ne ovat keskeisi\u00e4 neuroverkkojen ymm\u00e4rt\u00e4misess\u00e4, ja pohjustavat vastavirta (backpropagation) algoritmia, jota k\u00e4sitell\u00e4\u00e4n parin seuraavan osion aikana.</p> <p>Aiheesta l\u00f6ytyy my\u00f6s pidempi Nvidian artikkeli, jos haluat tutustua: What\u2019s the Difference Between Deep Learning Training and Inference?</p>"},{"location":"neuroverkot/neuroverkot_101/#koulutus","title":"Koulutus","text":"<p>Koulutusvaiheessa mallille sy\u00f6tet\u00e4\u00e4n koulutusdataa. T\u00e4m\u00e4 on Johdatus koneoppimisesta tuttu <code>X_train</code>-osuus datasetist\u00e4. Malli laskee ennusteet \\(\\hat{y}\\) ja vertaa niit\u00e4 todellisiin arvoihin \\(y\\). N\u00e4iden erotus lasketaan h\u00e4vi\u00f6funktiolla (loss function). H\u00e4vi\u00f6funktio palauttaa yhden luvun, joka kertoo kuinka hyvin malli suoriutui. T\u00e4m\u00e4n luvun perusteella mallin parametrej\u00e4 s\u00e4\u00e4det\u00e4\u00e4n, jotta h\u00e4vi\u00f6 pienenee. T\u00e4t\u00e4 toistetaan useita kertoja, kunnes malli on oppinut halutun tason tarkkuuden. T\u00e4m\u00e4n pit\u00e4isi olla kertausta Johdatus koneoppimiseen -kurssilta. Kannattaa vilkaista omia muistiinpanoja ja omaa oppimisp\u00e4iv\u00e4kirjaa.</p> <p>Neuroverkkojen koulutukseen liittyy yksi hyvinkin keskeinen ero perinteisiin malleihin verrattuna: Neuroverkot oppivat itse piirteet (feature learning). Perinteisiss\u00e4 malleissa piirteet piti usein valita k\u00e4sin, mutta neuroverkot pystyv\u00e4t oppimaan hy\u00f6dylliset piirteet suoraan datasta.</p> <p>Koulutuksen aikana tarvittu muistin m\u00e4\u00e4r\u00e4 riippuu monesta tekij\u00e4st\u00e4. Jos haluat tutustua asiaan klikkailemalla, k\u00e4y kurkkaamassa interaktiivista Hugging Facen blogikirjoitusta Visualize and understand GPU memory in PyTorch.</p>"},{"location":"neuroverkot/neuroverkot_101/#inferenssi","title":"Inferenssi","text":"<p>Inferenssi on mallin k\u00e4ytt\u00f6\u00e4. Kun malli on koulutettu, se kirjoitetaan levylle: tai siis tarkemmin sanottuna sen parametrit tallennetaan. Jatkossa parametrit voidaan ladata k\u00e4ytt\u00f6\u00f6n, jopa useille eri laitteille samanaikaisesti rinnakkain, ja mallia voidaan k\u00e4ytt\u00e4\u00e4 ennustamiseen. T\u00e4t\u00e4 kutsutaan inferenssiksi.</p> <p>Neuroverkkojen inferenssi vaatii v\u00e4hemm\u00e4n muistia (ja laskentatehoa) kuin koulutus, koska mallin parametrej\u00e4 ei en\u00e4\u00e4 s\u00e4\u00e4det\u00e4. Malli vain suorittaa eteenp\u00e4in sy\u00f6tt\u00e4misen (feed-forward) laskennan. Mallia voidaan my\u00f6s eri tekniikoin pienent\u00e4\u00e4 ilman ett\u00e4 suorituskyky k\u00e4rsii liikaa. N\u00e4it\u00e4 tekniikoita ovat esimerkiksi kvantisointi (quantization), karsinta (pruning) ja \"tislaus tai tiivistys\" (distillation). N\u00e4it\u00e4 k\u00e4sitell\u00e4\u00e4n my\u00f6hemmin kurssilla ainakin pintapuolisesti. On hyv\u00e4 kuitenkin jo tunnistaa, ett\u00e4 mallin k\u00e4ytt\u00f6kulut (inferenssi) ja koulutuskulut (training) eroavat toisistaan merkitt\u00e4v\u00e4sti. K\u00e4yt\u00e4nn\u00f6ss\u00e4 voit t\u00f6rm\u00e4t\u00e4 vaikkapa BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding-julkaisun malliin BERT sivustolla Hugging Face siten, ett\u00e4 osa vaatii enemm\u00e4n ja osa v\u00e4hemm\u00e4n suorituskyky\u00e4. Alla taulukkona suuntaa-antava vertailu.</p> Malli n parameteria Tensor tyyppi optimoinnin taso Muistin tarve painoille bert-base-uncased 110M float32 Alkuper\u00e4inen koulutettu malli ~440 MB distilbert-base-uncased 66M float32 40% pienempi, 60% nopeampi ~264 MB distilbert-base-uncased-distilled-squad 66M float32 QA-teht\u00e4viin hienos\u00e4\u00e4detty ~264 MB distilbert-base-uncased-distilled-squad-int8-static-inc 66M int8 Kvantisointi ~66 MB <p>Muistin tarpeen voi laskea helposti: 32-bittinen liukuluku vaatii 4 tavua muistia. N\u00e4it\u00e4 on 110 miljoonaa, joten 110M * 4B = 440MB. Kvantisoinnissa mallin painot muunnetaan 8-bittisiksi kokonaisluvuiksi, jolloin muistin tarve on vain nelj\u00e4sosa alkuper\u00e4isest\u00e4. Huomaa, ett\u00e4 inferenssiss\u00e4 muistia tarvitsee my\u00f6s muita asioita, kuten sy\u00f6tteet, v\u00e4liarvot ja mahdolliset v\u00e4limuistit. Todellisen muistin tarve voi siis olla esimerkiksi 20-50 % enemm\u00e4n kuin pelkkien painojen vaatima muisti.</p>"},{"location":"neuroverkot/neuroverkot_101/#mihin-kaytetaan","title":"Mihin k\u00e4ytet\u00e4\u00e4n","text":"<p>Tutustu n\u00e4ihin:</p> <ul> <li>Dilmegani: Top 50 Deep Learning Use Case &amp; Case Studies</li> <li>Lex Fridman: Deep Learning Basics: Introduction and Overview (YouTube-video)</li> </ul>"},{"location":"neuroverkot/neuroverkot_101/#tehtavat","title":"Teht\u00e4v\u00e4t","text":"<p>Teht\u00e4v\u00e4: TensorFlow Playground</p> <p>Tutustu online-ty\u00f6kaluun TensorFlow Playground. Kokeile eri asetuksia ja yrit\u00e4 ymm\u00e4rt\u00e4\u00e4, miten ne vaikuttavat mallin oppimiseen, ja kuinka t\u00e4m\u00e4 liittyy yll\u00e4 kirjoitettuun teoriaan (ja historiaan). Dokumentoi omat havaintosi oppimisp\u00e4iv\u00e4kirjaasi \u2013 jatka t\u00e4t\u00e4 dokumentointia jatkossa kaikkien teht\u00e4vien yhteydess\u00e4.</p> <ol> <li> <p>Mallinna ty\u00f6kalulla Perceptron-paperin mukainen malli. Sinulla pit\u00e4isi olla siis \\(x_1\\) ja \\(x_2\\) sy\u00f6ttein\u00e4, ei yht\u00e4\u00e4n piilotettua kerrosta ja tulos. K\u00e4yt\u00e4 ReLU-aktivointifunktiota, vaikka se ei olekaan aivan 1958 ajan mukainen.</p> <p>Tunnista eri datasetit sivustolta. Ne ovat j\u00e4rjestyksess\u00e4:</p> <ul> <li><code>Circle</code></li> <li><code>Exclusive or</code> (XOR)</li> <li><code>Gaussian</code></li> <li><code>Spiral</code></li> </ul> </li> <li> <p>0 piilotettua: Todista, ett\u00e4 on mahdotonta ratkaista XOR-ongelma ilman piilotettua kerrosta. Todista Gauss mahdolliseksi.</p> </li> <li>1 piilotettu: Kokeile ratkaista XOR-ongelma yhdell\u00e4 piilotetulla kerroksella, jossa on 2-4 neuronia. Ratkaise my\u00f6s Gauss ja Circle.</li> <li>4 piilotettua: Kokeile ratkaista Spiral-ongelma nelj\u00e4ll\u00e4 piilotetulla kerroksella, joissa on kussakin 2-8 neuronia. Aloita pienemm\u00e4st\u00e4 mallista ja monimutkaista sit\u00e4 v\u00e4hitellen. Voi olla kannattavaa yritt\u00e4\u00e4 pit\u00e4\u00e4 inputin puolella olevat kerrokset suurempina (neuronim\u00e4\u00e4r\u00e4n osalta) kuin outputin puolella olevat kerrokset. Eli mallista tulee siis suppilo, joka piennee kohti outputtia.</li> </ol> <p>Teht\u00e4v\u00e4: Quick, Draw!</p> <p>Pelaa er\u00e4 (tai useampi) Googlen Quick, Draw! peli\u00e4.</p> <p>Tutustu t\u00e4m\u00e4n j\u00e4lkeen sen taustalla olevaan dataan domain/data.</p> <p>Tutustu viimeisen\u00e4 viel\u00e4 Kaggle-sivultolta l\u00f6ytyv\u00e4\u00e4n haasteeseen, jossa datan perusteella tulee pyrki\u00e4 tehd\u00e4 mahdollisimman tarkka luokittelija: Quick, Draw! Doodle Recognition Challenge.</p> <p>Teht\u00e4v\u00e4: UDLbook Shallow</p> <p>Lataa PDF-muodossa koneellesi Understanding Deep Learning kirja. Tutustu sen lukuun kolme, \"Shallow neural networks\" ja tee lopulta seuraavat teht\u00e4v\u00e4t:</p> <ul> <li>Problem 3.4 Draw a version of figure 3.3 where the y-intercept and slope of the third hidden unit have changed as in figure 3.14c. Assume that the remaining parameters remain the same.</li> <li>Problem 3.11 How many parameters does the model in figure 3.6 have?</li> </ul> <p>Voit k\u00e4ytt\u00e4\u00e4 ensimm\u00e4iseen teht\u00e4v\u00e4\u00e4n UDL-kirjan Interactive Figures-ty\u00f6kalua.</p> <p>Saat toki ratkaista halutessasi my\u00f6s muita ongelmia. Kurssilla siirryt\u00e4\u00e4n kuitenkin seuraavaksi teoriasta PyTorchin kautta k\u00e4yt\u00e4nt\u00f6\u00f6n.</p> <p>Teht\u00e4v\u00e4: ANN ja el\u00e4inkunta</p> <p>Kannattaa tutustua Wikipedian Cerebral cortex sek\u00e4 Neuron-artikkeleihin. Aivokuoren (engl. cerebral cortex) osalta aiheeseen liittyv\u00e4\u00e4 sis\u00e4lt\u00f6\u00e4 on ainakin \"Layers of neocortex\"-otsikkoon liittyv\u00e4 teksti ja kuvat. T\u00e4m\u00e4 ei ole biologian kurssi, joten silm\u00e4ily riitt\u00e4\u00e4. Neuroverkot (ANN, Artificial Neural Network) perustuvat ideatasolla siihen, kuinka aivojen synapsien ja neuronien verkosto toimii \u2013 tai ainakin, kuinka sen kuviteltiin toimivan viel\u00e4 1900-luvun puoliv\u00e4liss\u00e4.</p> <p>Voi olla my\u00f6s mielenkiintoista verrata nykyisi\u00e4 koneoppimismalleja List of animals by number of neurons -listaan. Jos verrataan melko naiivisti parametrim\u00e4\u00e4ri\u00e4, niin mit\u00e4 el\u00e4imi\u00e4 seuraavat mallit vastaavat:</p> Malli Vuosi desimaali potenssiesitys LeNet-5 1998 60 000 6.0 x 10^4 AlexNet 2012 60 000 000 6.0 x 10^7 GPT-3 2020 175 000 000 000 1.75 \u00d7 10^11 Grok-1 (open source) 2024 314 000 000 000 3.14 \u00d7 10^11 GPT-4 (spekulaatio) 2024 1 760 000 000 000 1.76 \u00d7 10^12 <p>Vinkki: mieti tarkkaan, kumpaa \"parametrien m\u00e4\u00e4r\u00e4\" vastaa paremmin aivoissa: neuronien vai synapsien m\u00e4\u00e4r\u00e4?    </p> <p>Teht\u00e4v\u00e4: BERT Large</p> <p>Yll\u00e4 on esiteltyn\u00e4 BERT-mallin <code>base</code>-versio ja sen kevennetty <code>distilbert</code>-versio. Tutustu, kuinka BERT Large liittyy t\u00e4h\u00e4n. Kuinka monta parametria siin\u00e4 on? Kuinka paljon enemm\u00e4n muistia se vaatii verrattuna <code>base</code>-versioon? Miten se tai sen koulutus eroaa <code>base</code>-versiosta?</p>"},{"location":"neuroverkot/neuroverkot_101/#lahteet","title":"L\u00e4hteet","text":"<ol> <li> <p>Nelson, H. Essential Math for AI. O'Reilly Media. 2023.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>Gugger, J. &amp; Howard, J. Deep Learning for Coders with fastai and PyTorch. O'Reilly Media. 2020.\u00a0\u21a9\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>Dartmouth University. Artificial Intelligence Coined at Dartmouth. https://home.dartmouth.edu/about/artificial-intelligence-ai-coined-dartmouth\u00a0\u21a9</p> </li> <li> <p>Krohn, J., Beyleveld, G. &amp; Bassens, A. Deep Learning Illustrated: A Visual, Interactive Guide to Artificial Intelligence. Addison-Wesley Professional. 2019.\u00a0\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>The Nobel Prize. Nobel Prize in Physiology or Medicine 1981. https://www.nobelprize.org/prizes/medicine/1981/summary/\u00a0\u21a9</p> </li> <li> <p>Hubel, D.H. &amp; Wiesel, T.N. *Receptive fields of single neurones in the cat's striate cortex. The Journal of Physiology, 1959. https://doi.org/10.1113/jphysiol.1968.sp008455\u00a0\u21a9\u21a9</p> </li> <li> <p>Rosenblatt, F. The Design of an Intelligent Automaton. Research Trends, Cornell Aeronautical Laboratory. Summer 1958, Issue 2. https://www.informationphilosopher.com/solutions/scientists/rosenblatt/Rosenblatt_Research_Trends.pdf\u00a0\u21a9\u21a9</p> </li> <li> <p>The Nobel Prize. Nobel Prize in Physics 2024. https://www.nobelprize.org/prizes/physics/2024/summary/\u00a0\u21a9</p> </li> <li> <p>Crevier, D. AI: The Tumultuous Search for Artificial Intelligence. Basic Books. 1993. https://www.researchgate.net/profile/Daniel-Crevier/publication/233820788_AI_The_Tumultuous_History_of_the_Search_for_Artificial_Intelligence/links/63fe3d9457495059454f87ca/AI-The-Tumultuous-History-of-the-Search-for-Artificial-Intelligence.pdf\u00a0\u21a9</p> </li> <li> <p>Schmidhuber, J. Who Invented Backpropagation?. 2014 (p\u00e4ivitetty 2025). https://people.idsia.ch/~juergen/who-invented-backpropagation.html\u00a0\u21a9</p> </li> <li> <p>Prince, S. Understanding Deep Learning. The MIT Press. 2023. https://udlbook.github.io/udlbook/\u00a0\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>McCarthy, J., Minsky, M.L., Rochester, N. &amp; Shannon, C.E. A Proposal for the Dartmouth Summer Research Project on Artificial Intelligence. 1955. https://raysolomonoff.com/dartmouth/boxa/dart564props.pdf\u00a0\u21a9</p> </li> </ol>"},{"location":"neuroverkot/syvaoppiminen_FC/","title":"Syv\u00e4t neuroverkot","text":""},{"location":"neuroverkot/syvaoppiminen_FC/#syvaoppiminen","title":"Syv\u00e4oppiminen","text":""},{"location":"neuroverkot/syvaoppiminen_FC/#maaritelma","title":"M\u00e4\u00e4ritelm\u00e4","text":"<p>M\u00e4\u00e4ritelm\u00e4n osalta Understanding Deep Learning kirjan luvun 3 alku on hyv\u00e4:</p> <p>\"The last chapter described shallow neural networks, which have a single hidden layer. This chapter introduces deep neural networks, which have more than one hidden layer. With ReLU activation functions, both shallow and deep networks describe piecewise linear mappings from input to output.\" <sup>1</sup></p> <p>Prince toteaa, ett\u00e4 matalan neuroverkkojen kyky kuvata monimutkaisia funktioita kasvaa piilokerroksen neuronien m\u00e4\u00e4r\u00e4n lis\u00e4\u00e4ntyess\u00e4. Riitt\u00e4v\u00e4n suurella neuronim\u00e4\u00e4r\u00e4ll\u00e4 matalat verkot pystyv\u00e4t mallintamaan mielivaltaisen monimutkaisia funktioita. K\u00e4yt\u00e4nn\u00f6ss\u00e4 t\u00e4m\u00e4 on kuitenkin usein mahdotonta, sill\u00e4 tarvittava neuronien m\u00e4\u00e4r\u00e4 voi kasvaa kohtuuttoman suureksi. <sup>1</sup></p> <p>Syv\u00e4t neuroverkot tarjoavat t\u00e4h\u00e4n ratkaisun: ne pystyv\u00e4t tuottamaan huomattavasti enemm\u00e4n lineaarisia alueita kuin matalat verkot samalla parametrim\u00e4\u00e4r\u00e4ll\u00e4. Kerrosten m\u00e4\u00e4r\u00e4n ja niiden neuronien m\u00e4\u00e4r\u00e4 on siis jotakin, mit\u00e4 pit\u00e4\u00e4 optimoida mallia suunniteltaessa. Palaamme t\u00e4h\u00e4n kohta t\u00e4ss\u00e4 samassa luvussa.</p>"},{"location":"neuroverkot/syvaoppiminen_FC/#yleiskatsaus","title":"Yleiskatsaus","text":"<p>T\u00e4ysin yhdistetyt kerrokset (engl. fully connected layers) ovat syv\u00e4oppimisen peruskomponentteja. Niiss\u00e4 jokainen neuroni on yhteydess\u00e4 kaikkiin edellisen kerroksen neuroneihin. T\u00e4m\u00e4 mahdollistaa monimutkaisempien suhteiden oppimisen sy\u00f6tteiden ja ulostulojen v\u00e4lill\u00e4. Tosiel\u00e4m\u00e4n mallit ovat 2020-luvulla siirtyneet yh\u00e4 enemm\u00e4n erilaisiin konvoluutio- ja toistoverkkoihin, mutta FC-kerrokset ovat edelleen keskeisi\u00e4 monissa arkkitehtuureissa. T\u00e4ss\u00e4 luvussa keskitymme verkkoihin, joissa on pelkki\u00e4 FC-kerroksia.</p> <p>Konsepti on helppo ja tulee toivon mukaan selv\u00e4ksi seuraavaa kuvaa katsomalla. Huomaat, ett\u00e4 olet toteuttanut n\u00e4it\u00e4 verkkoja jo edellisen luvun TensorFlow Playground -teht\u00e4v\u00e4ss\u00e4.</p> <p>Kuvassa kerroksen yksi neuronit ovat \\(h_1\\), \\(h_2\\) ja \\(h_3\\). Kunkin niiden tuloste p\u00e4\u00e4tyy seuraavan kerroksen kunkin neuronin sy\u00f6tteeksi.</p> <p></p> <p>Kuva 1: Kaksi kerroksinen syv\u00e4verkko, jossa on kaksi piilotettua kerrosta, joissa kussakin on kolme neuronia. Jokainen piilotettu kerros on t\u00e4ysin yhdistetty (fully connected) edelliseen kerrokseen.</p>"},{"location":"neuroverkot/syvaoppiminen_FC/#laskutoimitukset","title":"Laskutoimitukset","text":"<p>Selvyyden vuoksi k\u00e4yd\u00e4\u00e4n l\u00e4pi, miten verkko toimii. Oletetaan, ett\u00e4 sy\u00f6tevektori on \\(x = [x_1, x_2]\\). Ensimm\u00e4isen piilotetun kerroksen neuronit laskevat seuraavasti: </p> \\[ \\begin{align*} h_1 &amp;= a(w_{11} x_1 + w_{12} x_2 + b_1) \\\\ h_2 &amp;= a(w_{21} x_1 + w_{22} x_2 + b_2) \\\\ h_3 &amp;= a(w_{31} x_1 + w_{32} x_2 + b_3) \\end{align*} \\] <p>miss\u00e4 \\(w_{ij}\\) ovat painot ja \\(b_i\\) on bias-termi. Funktio \\(a\\) on aktivointifunktio ReLU. Toinen kerros on monimutkaisempi, koska siin\u00e4 on kolme sy\u00f6tett\u00e4 ja kolme vastaanottavaan neuronia, joten painoja tulee olemaan \\(3 \\times 3 = 9\\). Toisen kerroksen neuronit laskevat seuraavasti:</p> \\[ \\begin{align*} h_4 &amp;= a(w_{41} h_1 + w_{42} h_2 + w_{43} h_3 + b_4) \\\\ h_5 &amp;= a(w_{51} h_1 + w_{52} h_2 + w_{53} h_3 + b_5) \\\\ h_6 &amp;= a(w_{61} h_1 + w_{62} h_2 + w_{63} h_3 + b_6) \\end{align*} \\] <p>N\u00e4m\u00e4 merkinn\u00e4t alkavat olla kohtalaisen sekavia, joten on parempi k\u00e4ytt\u00e4\u00e4 vektori- ja matriisimerkint\u00f6j\u00e4. Kummankin kerroksen ja ulostulon laskenta voidaan ilmaista seuraavasti:</p> \\[ \\begin{align*} h^{(1)} &amp;= a(W^{(1)} x + b^{(1)}) \\\\ h^{(2)} &amp;= a(W^{(2)} h^{(1)} + b^{(2)}) \\\\ y &amp;= W^{(3)} h^{(2)} + b^{(3)} \\end{align*} \\] <p>Yll\u00e4 olevassa kaavassa \\(y\\) on laskettu ilman aktivointifunktiota. T\u00e4m\u00e4 tekee mallista regressiomallin, joka sopii hyvin jatkuvien arvojen ennustamiseen. Jos mallia halutaan k\u00e4ytt\u00e4\u00e4 bin\u00e4\u00e4riseen luokitteluun, siihen lis\u00e4t\u00e4\u00e4n sigmoid-aktivointifunktio. Huomaa, ett\u00e4 t\u00e4m\u00e4 on kertausta Johdatus koneoppimiseen -kurssin logistisesta regressiosta.</p> <p>Kurssin aikana tulemme k\u00e4ytt\u00e4m\u00e4\u00e4n PyTorch-kirjastoa, joka hoitaa paljon laskuja puolestamme. T\u00e4ss\u00e4 on jo esimakua siit\u00e4, kuinka yll\u00e4 olevat laskut voidaan toteuttaa PyTorchilla:</p> <pre><code># Esimerkin vuoksi W1 voisi n\u00e4ytt\u00e4\u00e4 t\u00e4lt\u00e4\nW1 = torch.tensor(\n    [\n    [0.11, 0.12],\n    [0.21, 0.22],\n    [0.31, 0.32]\n], dtype=torch.float32)\n\n# Lasketaan ensimm\u00e4inen kerros\nh1 = torch.relu(torch.matmul(W1, x) + b1)\n\n# Lasketaan toinen kerros\nh2 = torch.relu(torch.matmul(W2, h1) + b2)\n\n# Lasketaan ulostulo (ilman aktivointifunktiota)\ny = torch.matmul(W3, h2) + b3\n</code></pre> <p>Huomaa, ett\u00e4 t\u00e4ss\u00e4 on kyseess\u00e4 pelkk\u00e4 inferenssi eli ennustaminen. Koko mallin kouluttaminen vaatii viel\u00e4 paljon enemm\u00e4n koodia, ja t\u00e4m\u00e4 esitell\u00e4\u00e4n kurssilla my\u00f6hemmin.</p>"},{"location":"neuroverkot/syvaoppiminen_FC/#hyperparametrit","title":"Hyperparametrit","text":"<p>Syv\u00e4verkkojen suunnittelussa on useita hyperparametreja. Hyperparametrit ovat malliin liittyv\u00e4t asetukset, jotka valitetaan ennen sen kouluttamista \u2013 eli niit\u00e4 ei siis opita koulutusvaiheessa. T\u00e4ss\u00e4 luvussa keskitymme vain niihin hyperparametreihin, jotka liittyv\u00e4t FC-verkon kokoon:</p> <ul> <li>Kerrosten m\u00e4\u00e4r\u00e4 \\(K\\)</li> <li>Neuronien m\u00e4\u00e4r\u00e4 kussakin kerroksessa \\(D_k\\)</li> </ul> <p>Tutustumme my\u00f6s muihin hyperparametreihin kurssin edetess\u00e4. Hyperparametrien oikeita arvoja ei voi yksinkertaisesti tarkistaa jostakin Maolin taulukkokirjasta. Ne on l\u00f6ydett\u00e4v\u00e4 kokeilemalla.</p> <p></p> <p>Kuva 2: Syv\u00e4verkko, jossa on \\(K\\) piilotettua kerrosta, joissa kussakin on \\(D_k\\) neuronia. Jokainen piilotettu kerros on t\u00e4ysin yhdistetty (fully connected) edelliseen kerrokseen. T\u00e4h\u00e4n kuvaan on piirretty mukaan my\u00f6s vakiotermit (bias) \\(b_k\\), jotka ovat \\(D_k\\)-ulotteisia vektoreita.</p> <p>Huomaa, ett\u00e4 jos meid\u00e4n budjetti GPU-muistille sallii vain \\(N = 1000\\) painoa, voimme valita esimerkiksi luoda \\(K=2\\) kerrosta, joissa kummassakin \\(D_k = 500\\) neuronia. Tai voimme tehd\u00e4 \\(K=5\\) kerrosta, joissa kussakin on \\(D_k = 200\\) neuronia. Tai voimme luoda suppilon, jossa ensimm\u00e4isess\u00e4 kerroksessa on \\(D_1 = 400\\) neuronia, toisessa \\(D_2 = 300\\), kolmannessa \\(D_3 = 200\\) ja nelj\u00e4nness\u00e4 \\(D_4 = 100\\). Kaikki n\u00e4m\u00e4 vaihtoehdot k\u00e4ytt\u00e4v\u00e4t saman verran muistia, mutta niill\u00e4 on erilaiset kyvyt oppia erilaisia funktioita. Jos tutkit vanhoja malleja, huomaat, ett\u00e4 suppilo oli ennen hyvinkin suosittu arkkitehtuuri. Nyky\u00e4\u00e4n on tavallisempaa k\u00e4ytt\u00e4\u00e4 saman kokoisia kerroksia. G\u00e9ron antaa nyrkkis\u00e4\u00e4nn\u00f6n, ett\u00e4 paremman hy\u00f6dyn saa tyypillisesti lis\u00e4\u00e4m\u00e4ll\u00e4 kerrosten m\u00e4\u00e4r\u00e4\u00e4 kuin neuronien m\u00e4\u00e4r\u00e4\u00e4 kerroksessa. <sup>2</sup></p> <p>\"A typical neural network for MNIST might have 3 hidden layers, the first with 300 neurons, the second with 200, and the third with 100. However, this practice has been largely abandoned because it seems that using the same number of neurons in all hidden layers performs just as well in most cases, or even better; plus, there is only one hyperparameter to tune, instead of one per layer. That said, depending on the dataset, it can sometimes help to make the first hidden layer bigger than the others.\" <sup>2</sup></p>"},{"location":"neuroverkot/syvaoppiminen_FC/#case-mlp-ja-mnist","title":"Case: MLP ja MNIST","text":""},{"location":"neuroverkot/syvaoppiminen_FC/#tehtavan-yleiskuvaus","title":"Teht\u00e4v\u00e4n yleiskuvaus","text":"<p>Aloitamme kurssin tutustumalla yksinkertaiseen syv\u00e4verkkoon, joka tunnetaan nimell\u00e4 monikerroksinen perceptroni (MLP, multi-layer perceptron). MLP on syv\u00e4verkko, joka koostuu useista t\u00e4ysin yhdistetyist\u00e4 kerroksista (FC-kerrokset), joissa on aktivointifunktio kunkin kerroksen j\u00e4lkeen. Valitsemamme ongelma, tai dataset, on MNIST. Lyhenteen NIST tulee sanoista National Institute of Standards and Technology, joka on Yhdysvaltain hallituksen virasto. Kirjain M tulee sanasta Modified. Dataa on esiprosessoitu siten, ett\u00e4 se on sopivassa muodossa koneoppimiseen, tehden siit\u00e4 hyv\u00e4n Hello World -esimerkin koneoppimiselle. Tyypillisess\u00e4 koneoppimisteht\u00e4v\u00e4ss\u00e4 saat hyvin harvoin n\u00e4in valmista dataa k\u00e4siisi: kuvat on valmiiksi rajattu, skaalattu, keskitetty ja muunnettu vektoreiksi. MNIST on kuitenkin erinomainen aloitus, koska se on pieni, helppo ymm\u00e4rt\u00e4\u00e4 ja sill\u00e4 on helppo kokeilla erilaisia malleja.</p> <p>MNIST-datassa on k\u00e4sinkirjoitettuja numeroita (0-9), jotka on skaalattu 28x28 pikselin harmaas\u00e4vykuviksi. Kuvat on esitetty alla olevassa kuvassa. PyTorchin MNNIST tarjoaa 60 000 koulutuskuvaa ja 10 000 testikuvaa. Kuvat ovat tavallisia yksikanavaisia harmaas\u00e4vykuvia PIL-formaatissa eli niiden pikseliarvot ovat v\u00e4lill\u00e4 <code>0-255</code>. Jos kaikki n\u00e4m\u00e4 kuvat lataisi yhteen tensoriin, sen koko olisi <code>[70000, 1, 28, 28]</code>.</p> <p></p> <p>Kuva 3: MNIST-datan esimerkkikuvia. Kuvat on poimittu PyTorch:n torchvision.datasets.MNIST-luokasta.</p> <p>Tavoite on siis: kouluttaa moniluokkainen luokittelija, joka pystyy tunnistamaan k\u00e4sinkirjoitetut numerot. T\u00e4m\u00e4 on 10-luokkainen luokitteluteht\u00e4v\u00e4, jossa jokainen luokka vastaa yht\u00e4 numeroa (0-9). Ty\u00f6 on jo tehty sinun puolestasi, koska t\u00e4m\u00e4 on kurssin aloitus, ja sinulle ei ole viel\u00e4 edes opetettu PyTorchin k\u00e4ytt\u00f6\u00e4. Koodi l\u00f6ytyy Notebookista <code>notebooks/nb/100/110_first_model.ipynb</code> t\u00e4m\u00e4n kurssimateriaalin repositoriota eli gh:sourander/syvaoppiminen. Aloitusluennolla sinulle on esitelty, mist\u00e4 mit\u00e4kin materiaalia l\u00f6ytyy, ja kuinka kurssi kannattaa suorittaa.</p> <p>Tip</p> <p>MLP:t ovat perusarkkitehtuuri, josta monet muut neuroverkot on johdettu. Huomaa, ett\u00e4 arkkitehtuureita on useita erilaisia. Aloitamme yksinkertaisimmasta mahdollisesta, jossa on vain FC-kerroksia, mutta etenemme kurssin aikana monimutkaisempiin arkkitehtuureihin, kuten konvoluutio- ja toistoverkkoihin (RNN). Tulet luomaan kurssin aikana konvoluutioverkon, joka istuu valittuun teht\u00e4v\u00e4\u00e4n paremmin kuin MLP.</p>"},{"location":"neuroverkot/syvaoppiminen_FC/#tulokset","title":"Tulokset","text":"<p>Aloitetaan k\u00e4\u00e4nteisesti tulosten esittelemisest\u00e4. Kuten on sanottu, MNIST on hyvinkin Hello World-tason teht\u00e4v\u00e4 n\u00e4in 2020-luvulla. Yksinkertainen MLP pystyy saavuttamaan yli 95 % tarkkuuden (accuracy) jo muutamassa minuutissa. Tulet huomaamaan, ett\u00e4 vaikeampien teht\u00e4vien kanssa koulutusajat kasvavat merkitt\u00e4v\u00e4sti, ja tarkkuudet j\u00e4\u00e4v\u00e4t usein vaatimattomaksi, varsinkin jos MLP-arkkitehtuuria k\u00e4ytet\u00e4\u00e4n vasarana, joka sopii teht\u00e4v\u00e4\u00e4n kuin teht\u00e4v\u00e4\u00e4n. Vasaralla viittaan sanontaan: \"it is tempting, if the only tool you have is a hammer, to treat everything as if it were a nail\" <sup>3</sup>. Tutustu alla olevaan kuvaajaan:</p> <p></p> <p>Kuva 4: MNIST-datalla koulutetun mallin tarkkuus (accuracy) ja h\u00e4vi\u00f6 (loss) koulutuksen aikana epookki epookilta.</p> <p>Kuvaajassa n\u00e4kyy nelj\u00e4 k\u00e4yr\u00e4\u00e4: <code>train_loss</code>, <code>train_acc</code>, <code>val_loss</code> ja <code>val_acc</code>. N\u00e4m\u00e4 kuvaavat mallin suoriutumista koulutusdatalla (train) ja validaatiodatalle (val). Koulutusdata on se data, jolla malli on koulutettu, ja validaatiodata on erillinen osajoukko datasta, jota ei ole k\u00e4ytetty mallin kouluttamiseen. Validaatiodataa k\u00e4ytet\u00e4\u00e4n mallin arviointiin koulutuksen aikana, jotta n\u00e4hd\u00e4\u00e4n, kuinka hyvin malli yleistyy n\u00e4kem\u00e4tt\u00f6m\u00e4\u00e4n dataan. Ideaalitilanteessa lopullinen arviointi tehd\u00e4\u00e4n testidatalla, joka on t\u00e4ysin erillinen sek\u00e4 koulutus- ett\u00e4 validaatiodatasta. T\u00e4ss\u00e4 meid\u00e4n yksinkertaisessa esimerkiss\u00e4 emme kuitenkaan tee erillist\u00e4 testidataa. Eli siis kaikki 60 000 kuvaa ovat koulutusdataa ja 10 000 kuvaa ovat validaatiodataa. Hyv\u00e4ksymme validaatiotuloksen lopulliseksi tulokseksi.</p> <p>Kuvaajan vaakasuuntainen akseli on epookkien m\u00e4\u00e4r\u00e4. Kuvan koulutuksessa on ajettu 100 epookkia. Yksi epookki tarkoittaa, ett\u00e4 koko koulutusdata on k\u00e4yty l\u00e4pi kerran. Koska koulutusdata on jaettu pienempiin eriin (batch), yksi epookki koostuu useammasta batchista. T\u00e4ss\u00e4 tapauksessa batch-koko on 128, joten yhdess\u00e4 epookissa on \\(60000 / 128 \\approx 469\\) askelta (batches). Malli p\u00e4ivitt\u00e4\u00e4 painojaan jokaisen batchin j\u00e4lkeen.</p>"},{"location":"neuroverkot/syvaoppiminen_FC/#koulutuksen-suoritusaika","title":"Koulutuksen suoritusaika","text":"<p>Koulutuksen tulokset eri raudalla:</p> Rauta Aika/epookki Aika/koko koulutus Lopputarkkuus (val) MacBook Pro (CPU) 2.9 s 4 min 48 s 95.35 % MacBook Pro (MPS) 3.7 s 6 min 12 s 95.18 % PC (CPU) 3.2 s 5 min 22 s 95.21 % PC (CUDA) 3.3 s 5 min 30 s 95.14 % Google Colab (CPU) 10.8 s 18 min 00 s 95.35 % Google Colab (CUDA) 13.8 s 23 min 00 s 95.19 % Jupyter Hub (CPU) 20.4 s 33 min 24 s 95.35 % Jupyter Hub (0.33x GPU) 20.5 s 34 min 10 s 95.33 % <p>Macbook Pro on tarkemmalta malliltaan M2 Max (32 GB muistia). MPS (Metal Performance Shaders) on Apple Siliconin GPU-kiihdytys. PC on p\u00f6yt\u00e4kone i7-12700F suorittimella, 32 GB keskusmuistilla ja NVIDIA RTX 3060 Ti -n\u00e4yt\u00f6nohjaimella, jossa on 8 GB muistia. Verrokkina listalla on Google Colab:n ilmainen CPU runtime sek\u00e4 GPU runtime (Tesla T4). Jupyter Hubissa koodi on ajettu tavallisena torstai-iltanap\u00e4iv\u00e4n\u00e4 ilman tietoa siit\u00e4, onko alustalla muita samanaikaisia k\u00e4ytt\u00e4ji\u00e4.</p>"},{"location":"neuroverkot/syvaoppiminen_FC/#epookkien-maara","title":"Epookkien m\u00e4\u00e4r\u00e4","text":"<p>Miksik\u00f6 juuri 100 epookkia? Koska se on mukavan py\u00f6re\u00e4 summa. Esimerkin mallin rakenne on lainattu Adrian Rosebrockin kirjasta Deep Learning for Computer Vision with Python Volume 1. Kukaan ei luonnollisesti pakota meit\u00e4 pys\u00e4htym\u00e4\u00e4n juuri 100 epookkiin, joten tutkitaan, mit\u00e4 tapahtu, jos jatkamme koulutusta 500 epookkiin asti.</p> <p>Alla n\u00e4kyy kuvaajissa, mit\u00e4 tapahtuu 100-500 epookin aikana. Kuvaajat on otettu ty\u00f6kalusta nimelt\u00e4\u00e4n TensorBoard, joka tulee sinulle tutuksi kurssin aikana. Lopputarkkuudet 500. epookin kohdalla ovat 99.79 % (train) ja 98.15 % (val). Alemmasta kuvasta (Kuva 6) n\u00e4kee, kuinka accuracy ei ole parantunut en\u00e4\u00e4 400. epookin j\u00e4lkeen laisinkaan validaatiodatalla, mutta koulutusdataa vasten se on edelleen parantunut. T\u00e4m\u00e4 on malliesimerkki ylikoulutumisesta (overfitting).</p> <p></p> <p>Kuva 5: TensorBoardissa on mahdollista zoomata kuvaajaan sis\u00e4\u00e4n. T\u00e4ss\u00e4 kuvaajassa n\u00e4kyy suunnilleen alue 100-500. Mallin virhe v\u00e4henee epookista 100 eteenp\u00e4in, mutta <code>train</code> ja <code>val</code> k\u00e4yr\u00e4t alkavat erkaantua toisistaan, mik\u00e4 viittaa ylikoulutukseen.</p> <p></p> <p>Kuva 6: T\u00e4ss\u00e4 kuvaajassa on zoomattu alueelle 400-500 accuracy-k\u00e4yr\u00e4ss\u00e4. Kuten n\u00e4kyy, validaatiotarkkuus ei ole parantunut 400. epookin j\u00e4lkeen, mutta koulutustarkkuus jatkaa parantumistaan. T\u00e4m\u00e4 on selke\u00e4 merkki ylikoulutumisesta (overfitting). Malli k\u00e4yt\u00e4nn\u00f6ss\u00e4 oppii ulkoa koulutusdatan.</p>"},{"location":"neuroverkot/syvaoppiminen_FC/#nostoja-koodista","title":"Nostoja koodista","text":"<p>Alempana on teht\u00e4v\u00e4, jossa sinua k\u00e4sket\u00e4\u00e4n ajaa Notebook, joka kouluttaa mallin. T\u00e4m\u00e4 toimii samalla kertaa sek\u00e4 \"Opi PyTorchia lukemalla\" ett\u00e4 \"Testaa kehitysymp\u00e4rist\u00f6si toimivuus\" -teht\u00e4v\u00e4n\u00e4. Teht\u00e4v\u00e4n lomassa sinun tulee k\u00e4yd\u00e4 koodi l\u00e4pi, mutta t\u00e4ss\u00e4 on joitakin nostoja, jotka kannattaa huomioida.</p>"},{"location":"neuroverkot/syvaoppiminen_FC/#datan-lataus","title":"Datan lataus","text":"<p>Mallin data ladataan n\u00e4in:</p> <pre><code># Define transforms\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))\n])\n\n# Download and load the training data\nprint(\"[INFO] accessing MNIST...\")\ntrainset = datasets.MNIST(*args, train=True, **kwargs)\ntestset = datasets.MNIST(*args, train=False, **kwargs)\n</code></pre> <p>Koodisnippetist\u00e4 on p\u00e4\u00e4telt\u00e4viss\u00e4 ainakin, kuinka MNIST-datan voi normalisoida k\u00e4ytt\u00e4en <code>Normalize()</code>-metodia. Normalisoinnissa annetaan kaksi argumenttia: sy\u00f6tteiden keskiarvo ja keskihajonta. MNIST-tapauksen kohdalla n\u00e4m\u00e4 ovat hyvin tunnettuna arvoja ja ne on laskettu nimenomaan training-datalla. Kyseinen torchvision.transforms.Normalize itsess\u00e4\u00e4n on Johdatus koneoppimiseen -kurssilta tuttu juttu. Se on tarkalleen ottaen sama asia kuin sklearnin <code>StandardScaler</code>: \"Normalize a tensor image with mean and standard deviation. This transform does not support PIL Image.\"</p>"},{"location":"neuroverkot/syvaoppiminen_FC/#batchien-lataus","title":"Batchien lataus","text":"<p>Datan lataus tapahtuu <code>DataLoader</code>-luokan avulla. Kyseinen luokka on iteraattori-tyylinen olio, joka on wrapper datan ymp\u00e4rille. Se mahdollistaa datan k\u00e4sittelyn eriss\u00e4 (batches) ja se osaa sekoittaa datan (shuffle) sek\u00e4 ladata dataa rinnakkaisesti useammalla s\u00e4ikeell\u00e4 (num_workers). Alla on esimerkki koulutusdatan lataamisesta:</p> <pre><code># Create data loaders\ntrainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\ntestloader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False)\n</code></pre> <p>My\u00f6hemmin koulutusloopissa dataa haetaan n\u00e4in:</p> <pre><code>for epoch in range(EPOCHS):\n    model.train()\n    for batch_idx, (inputs, labels) in enumerate(trainloader):\n        # Training code here\n</code></pre>"},{"location":"neuroverkot/syvaoppiminen_FC/#batchin-koko","title":"Batchin koko","text":"<p>Miksi <code>BATCH_SIZE</code> on juuri 128? Alla taulukko, josta voit lukea tyypillisi\u00e4 eri er\u00e4koon vaikutuksia koulutukseen, tulokseen, muistink\u00e4ytt\u00f6\u00f6n ja niin edelleen:</p> Pienet (1-16) Medium (32-128) Large (256-512) Koulutusnopeus Hidas Tasapainoinen Nopea jos muisti riitt\u00e4\u00e4 Muistink\u00e4ytt\u00f6 Pieni Kohtalainen Suuri Lopputarkkuus Hyv\u00e4 Eritt\u00e4in hyv\u00e4 Voi heikenty\u00e4 Konvergenssi Todenn\u00e4k\u00f6isesti 100- 100 oli ok Todenn\u00e4k\u00f6isesti 100+ <p>Palaanne kurssin aikana t\u00e4h\u00e4n aiheeseen, mutta on hyv\u00e4 muistaa, ett\u00e4 er\u00e4koon valinta on t\u00e4rke\u00e4 hyperparametri, joka vaikuttaa merkitt\u00e4v\u00e4sti mallin suorituskykyyn ja koulutusprosessiin. Pieni er\u00e4 koko voi johtaa ep\u00e4vakaampiin p\u00e4ivityksiin, mutta se voi my\u00f6s auttaa mallia yleistym\u00e4\u00e4n paremmin. Suuremmat er\u00e4t voivat hy\u00f6dynt\u00e4\u00e4 GPU:n rinnakkaisprosessointia tehokkaammin, mutta ne voivat my\u00f6s johtaa huonompaan yleistymiseen.</p> <p>Selvyyden vuoksi sanottakoon viel\u00e4, ett\u00e4:</p> <ul> <li>Batch on siis N joukko kuvia, jotka sy\u00f6tet\u00e4\u00e4n malliin kerralla. Niiden pit\u00e4\u00e4 mahtua muistiin.</li> <li>Epookissa on useita askelia (steps). Tarkka askelten m\u00e4\u00e4r\u00e4 riippuu kuvien m\u00e4\u00e4r\u00e4st\u00e4 ja er\u00e4koosta. Eli <code>60 000 / 128 = 469</code> askelta per epookki. Tai <code>60 000 / 1 = 60 000</code> askelta per epookki, jos batch-koko on 1.</li> </ul>"},{"location":"neuroverkot/syvaoppiminen_FC/#mallin-maarittely","title":"Mallin m\u00e4\u00e4rittely","text":"<p>Malli m\u00e4\u00e4ritell\u00e4\u00e4n <code>MLP</code>-luokassa, joka perii <code>nn.Module</code>-luokan ominaisuudet.</p> <pre><code>class MLP(nn.Module):\n    def __init__(self):\n        super(MLP, self).__init__()\n        self.fc1 = nn.Linear(784, 256)\n        self.fc2 = nn.Linear(256, 128)\n        self.fc3 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = x.view(-1, 784)\n        x = torch.sigmoid(self.fc1(x))\n        x = torch.sigmoid(self.fc2(x))\n        x = self.fc3(x)\n        return x\n</code></pre> <p>Mieti, mit\u00e4 forward-metodi tarkalleen palauttaa. Tuleeko sielt\u00e4 raakatulokset (logits) vai softmaxin l\u00e4pi k\u00e4yneet todenn\u00e4k\u00f6isyydet? Jos olet sit\u00e4 mielt\u00e4, ett\u00e4 raakatulokset, niin miss\u00e4 kohtaa koodia softmax tehd\u00e4\u00e4n? Tulet huomaamaan, ett\u00e4 PyTorchissa on useita tapoja tehd\u00e4 samoja asioita \u2013 aivan kuten ohjelmoinnissa yleens\u00e4kin.</p> Vaihtoehtoinen mallin m\u00e4\u00e4rittely ilman luokkaa <p>Huomaa, ett\u00e4 mallin voisi m\u00e4\u00e4ritell\u00e4 my\u00f6s ilman <code>nn.Module</code>-perint\u00e4\u00e4. T\u00e4ll\u00f6in koodi n\u00e4ytt\u00e4isi t\u00e4lt\u00e4:</p> <pre><code>model = nn.Sequential(\n    nn.Flatten(),\n    nn.Linear(784, 256),\n    nn.Sigmoid(),\n    nn.Linear(256, 128),\n    nn.Sigmoid(),\n    nn.Linear(128, 10),\n    nn.LogSoftmax(dim=1)\n)\n</code></pre> <p>T\u00e4m\u00e4 on kuitenkin PyTorchin kohdalla harvinaisempi tapa. Emme k\u00e4yt\u00e4 sit\u00e4 t\u00e4ll\u00e4 kurssilla.</p> <p>Huomaa, ett\u00e4 MLP on luokka. T\u00e4m\u00e4 mahdollistaa, ett\u00e4 sin\u00e4 voit itse lis\u00e4t\u00e4 luokkaan tarpeen mukaan metodeja. Ainoat pakolliset metodit ovat <code>__init__</code> ja <code>forward</code>. Kukaan ei est\u00e4 sinua tekem\u00e4st\u00e4 esimerkiksi <code>def initialize_weights(self):</code> -metodia, joka alustaa painot haluamallasi tavalla. Mallin <code>super()</code>-kutsussa saadut metodit ja ominaisuudet selvi\u00e4v\u00e4t PyTorchin dokumentaatiosta: https://pytorch.org/docs/stable/generated/torch.nn.Module.html.</p>"},{"location":"neuroverkot/syvaoppiminen_FC/#termistoa","title":"Termist\u00f6\u00e4","text":"<p>Alla on t\u00e4h\u00e4n asti kurssilla k\u00e4ytettyj\u00e4 termej\u00e4, jotka tulee jo nyt laittaa korvan taakse. HUOM! T\u00e4ss\u00e4 l\u00e4hinn\u00e4 vain nimet\u00e4\u00e4n kurssilla k\u00e4yt\u00e4vi\u00e4 asioita. Syv\u00e4llisemp\u00e4\u00e4 k\u00e4sittely\u00e4 tulee PyTorchin kautta my\u00f6hemmiss\u00e4 luvuissa.</p> Termi Selitys Painot (weights) Mallin parametrit, jotka oppivat datasta. Jokaisella yhteydell\u00e4 on oma painonsa. Bias Mallin parametrit, jotka oppivat datasta. Jokaisella neuronilla on oma bias-termi. Aktivaatiofunktio Funktio, joka lis\u00e4t\u00e4\u00e4n neuronin ulostuloon. Aktivointifunktio tekee mallista ei-lineaarisen. Loss-funktio Funktio, joka mittaa mallin virheen. Koulutuksen aikana pyrit\u00e4\u00e4n minimoimaan loss-funktio. Optimointi Prosessi, jossa mallin painot ja bias-termit p\u00e4ivitet\u00e4\u00e4n loss-funktion minimoimiseksi. Epoch Yksi l\u00e4pik\u00e4ynti koko koulutusdatasta. Batch Pieni osa koulutusdatasta, joka sy\u00f6tet\u00e4\u00e4n malliin kerralla. Learning rate Hyperparametri, joka m\u00e4\u00e4ritt\u00e4\u00e4, kuinka suuria p\u00e4ivityksi\u00e4 mallin painoihin ja biaseihin tehd\u00e4\u00e4n."},{"location":"neuroverkot/syvaoppiminen_FC/#tehtavat","title":"Teht\u00e4v\u00e4t","text":"<p>Teht\u00e4v\u00e4: UDLbook Deep</p> <p>Lue Understanding Deep Learning kirjasta v\u00e4hint\u00e4\u00e4n luvu 4.1 \"Composing neural networks\" sek\u00e4 4.2 \"From composing networks to deep networks\". Kirjoita itsellesi lyhyt yhteenveto aiheesta omin sanoinesi, jotta ymm\u00e4rr\u00e4t asian.</p> <p>Tutustu my\u00f6s UDL-kirjan kylki\u00e4isen\u00e4 tuleviin Interactive Figures-ty\u00f6kaluihin. Erityisesti Concatenating networks sek\u00e4 Deep network computation-kuvaajiin.</p> <p>Teht\u00e4v\u00e4: Valitse kehitysymp\u00e4rist\u00f6si</p> <p>Valitse itsellesi sopiva kehitysymp\u00e4rist\u00f6, jossa aiot tehd\u00e4 kurssin harjoitukset. Vaihtoehtoja on useita:</p> <ul> <li>Lokaali kone:<ul> <li><code>uv</code>-ymp\u00e4rist\u00f6 ja VS Code Notebooks (t\u00e4t\u00e4 opettaja k\u00e4ytt\u00e4\u00e4 macOS:ll\u00e4 ja Ubuntussa)</li> <li><code>docker</code>-ymp\u00e4rist\u00f6 ja Jupyter Lab (Windows + CUDA GPU suositus)</li> </ul> </li> <li>Jupyter Hub (DC-labran yll\u00e4pit\u00e4m\u00e4)</li> <li>Coder (DC-labran yll\u00e4pit\u00e4m\u00e4)</li> <li>Google Colab</li> <li>Joku muu pilvipalvelu, jossa on GPU</li> </ul> <p>HUOM! Opettaja ei voi realistisesti kokeilla kaikkia vaihtoehtoja, jotka syntyv\u00e4t <code>(\"Win\", \"Mac\", \"Linux\") x (\"uv\", \"docker\", \"jupyterhub\", \"colab\")</code> -ristikkona. Valitse siis sellainen, joka sinulle on tuttu tai jonka opit helposti. Opettaja tarjoaa tukea, mutta \u00e4l\u00e4 odota, ett\u00e4 sinulle annetaan tasan yksi koodirimpsu, jolla kaikki toimii. Hallitse omat ymp\u00e4rist\u00f6si!</p> <p>P.S. Tarkista aloitusluennon tallenne! Siell\u00e4 on mit\u00e4 varmimmin neuvottu ainakin yksi tapa asentaa tarvittava ymp\u00e4rist\u00f6!</p> <p>Teht\u00e4v\u00e4: Aja MNIST MLP koodi</p> <p>Koodi l\u00f6ytyy Notebookista <code>notebooks/nb/100/110_first_model.ipynb</code> t\u00e4m\u00e4n kurssimateriaalin repositoriota eli gh:sourander/syvaoppiminen.</p> <ol> <li>Lataa Notebook koneellesi.</li> <li>Aja Notebook kokonaisuudessaan. Varmista, ett\u00e4 saat mallin koulutettua ja kaikki solut ajettua.</li> <li>Lue koodi kokonaisuudessaan l\u00e4pi! Emme ole viel\u00e4 opiskelleet PyTorchin k\u00e4ytt\u00f6\u00e4, mutta yrit\u00e4 konseptitasolla ymm\u00e4rt\u00e4\u00e4, mit\u00e4 kukin koodirivi tekee.</li> </ol> <p>Samalla n\u00e4et benchmarkkia siihen, kuinka sinun rautasi suhtautuu opettajan rautaan (ks. yll\u00e4 oleva taulukko).</p> <p>Teht\u00e4v\u00e4: TensorBoard</p> <p>Yll\u00e4 oleva koodi k\u00e4ytt\u00e4\u00e4 TensorBoardia koulutuksen seurantaan. Aja TensorBoard omassa ymp\u00e4rist\u00f6ss\u00e4si. Ohjeita t\u00e4h\u00e4n on Notebookin lopussa ja mahdollisisssa kurssivideoissa. Varautu ottamaan my\u00f6s itsen\u00e4isesti selv\u00e4\u00e4: tutki, mit\u00e4 tiedostoja Notebook loi ja mihin (ks. <code>runs/</code>-hakemisto). Lyhyimmill\u00e4\u00e4n komento on kuitenkin:</p> <pre><code>cd notebooks/\nuv run tensorboard --logdir nb/100/runs\n</code></pre> <p>Tutustu TensorBoardin k\u00e4ytt\u00f6liittym\u00e4\u00e4n ja sen tarjoamiin visualisointeihin. Tutki, mik\u00e4 rivi Notebookissa on vastuussa kunkin metriikan kirjaamisesta TensorBoardiin.</p> <p>Teht\u00e4v\u00e4: Mallin tarkkuus CPU vs MPS vs CUDA</p> <p>Tutustu yll\u00e4 olevaan \"Koulutuksen suoritusaika\"-osioon. Osiossa on taulukko, josta selvi\u00e4\u00e4, miten eri rauta vaikuttaa koulutuksen suoritusaikaan. Taulukkoon on asetettu esille my\u00f6s lopputarkkuus validaatiodatalla. Pohdi, ett\u00e4:</p> <ul> <li>Miksi Macbook (CPU ja MPS) sek\u00e4 Linux PC (CPU sek\u00e4 CUDA) eiv\u00e4t p\u00e4\u00e4sseet t\u00e4ysin samaan lopputarkkuuteen?</li> <li>Vaihteleeko t\u00e4m\u00e4 joka kerta kun koulutat mallin uudestaan?</li> <li>Miten ihmeess\u00e4 CPU voi olla hitaampi kuin GPU? Huijataanko meit\u00e4 ostamaan n\u00e4yt\u00f6nohjaimia turhaan?</li> </ul>"},{"location":"neuroverkot/syvaoppiminen_FC/#lahteet","title":"L\u00e4hteet","text":"<ol> <li> <p>Prince, S. Understanding Deep Learning. The MIT Press. 2023. https://udlbook.github.io/udlbook/\u00a0\u21a9\u21a9</p> </li> <li> <p>G\u00e9ron, A. Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 3rd Edition. O'Reilly Media. 2022.\u00a0\u21a9\u21a9</p> </li> <li> <p>Wikipedia. Law of the instrument. https://en.wikipedia.org/wiki/Law_of_the_instrument\u00a0\u21a9</p> </li> </ol>"},{"location":"siirtovaikutus/transferlearning/","title":"Siirtovaikutus (Transfer Learning)","text":"<p>TODO! T\u00e4h\u00e4n tulee ainakin seuraavat asiat:</p> <ul> <li>Mik\u00e4 on siirtovaikutus?</li> <li>Sivutuotteena t\u00e4ss\u00e4 tutustutaan GPU:n k\u00e4ytt\u00f6\u00f6n, koska mallit alkavat olla t\u00e4ss\u00e4 luvussa suurehkoja.</li> </ul>"},{"location":"tensorit/pytorch/","title":"PyTorch 101","text":"<p>T\u00e4ss\u00e4 otetaan PyTorch tutuksi. Perusjuttuja. Muutama harjoitus, kuten Sigmoid ja ReLu implementointi k\u00e4ytt\u00e4m\u00e4tt\u00e4 PyTorchin valmiita funktioita.</p>"},{"location":"tensorit/vektorointi/","title":"Vektorointi","text":"<p>Ennen kuin hypp\u00e4\u00e4mme PyTochin k\u00e4ytt\u00f6\u00f6n, varmistetaan viel\u00e4, ett\u00e4 ymm\u00e4rr\u00e4mme, miksi vektorointi on t\u00e4rke\u00e4\u00e4 syv\u00e4oppimisessa. Neuroverkkojen opetus vaatii valtavia m\u00e4\u00e4ri\u00e4 laskentatehoa. T\u00e4ten ei ole aivan sama, kuinka data esitet\u00e4\u00e4n ja kuinka laskenta toteutetaan. Ennen kuin nyrpist\u00e4t nen\u00e4\u00e4si, ett\u00e4 miksi syv\u00e4oppimiskurssilla on k\u00e4yt\u00f6ss\u00e4 Numpy, niin mainittaakoon heti alkuun: PyTorchin Tensor on hyvinkin samankaltainen kuin Numpyn ndarray. Molemmat ovat siis vektoreita ja matriiseja. Jotta osaamme my\u00f6hemmin arvostaa PyTorchin Tensoreita, kurkataan pintapuolisesti, kuinka neuroverkkoja voisi toteuttaa Numpyll\u00e4. Itse asiassa aivan ensimm\u00e4isen\u00e4 unohdetaan jopa Numpy ja kurkataan Python-only -toteutusta.</p>"},{"location":"tensorit/vektorointi/#nn-ilman-vektorointia","title":"NN ilman vektorointia","text":""},{"location":"tensorit/vektorointi/#verkon-rakenne","title":"Verkon rakenne","text":"<p>K\u00e4yt\u00e4mme t\u00e4m\u00e4n luvun esimerkkin\u00e4 verkkoa, joka tunnetaan nimell\u00e4 <code>NumpyNNwithBCE</code>. L\u00f6yd\u00e4t sen tiedostosta <code>200_numpy_nn.ipynb</code>. Verkko on juuri sen verran pelkk\u00e4\u00e4 Perceptronia monimutkaisempi, ett\u00e4 se kykenee yhden piilotetun kerroksen ansiosta ratkaisemaan XOR-ongelman. Selvyyden vuoksi verkon arkkitehtuuri on esitetty alla olevassa kuvassa</p> <p></p> <p>Kuva 1: Kuvassa on 2-2-1 arkkitehtuuria edustava verkko, <code>NumpyNNwithBCE</code>, jota k\u00e4yt\u00e4mme t\u00e4m\u00e4n luvun aikana esimerkkin\u00e4. Toteutuksesta l\u00f6ytyy Jupyter Notebook, jossa verkko on toteuttuna NumPy:lla.</p>"},{"location":"tensorit/vektorointi/#toteutus-ilman-vektorointia","title":"Toteutus ilman vektorointia","text":"<p>Tutustumme ensin siihen, milt\u00e4 malli n\u00e4ytt\u00e4isi, jos emme k\u00e4ytt\u00e4isi Numpy-kirjastoa. T\u00e4t\u00e4 mallia ei l\u00f6ydy mist\u00e4\u00e4n kokonaisuuteena, mutta saat toki koodata sen itse jos haluat. Olkoon sen nimi: <code>PythonNN</code>. Tutustu alla olevassa kuvassa esitettyyn koodiin. Koodi on v\u00e4rikoodattu siten, ett\u00e4 v\u00e4rit t\u00e4sm\u00e4\u00e4v\u00e4t Kuvan 1 painojen v\u00e4reihin.</p> <p></p> <p>Kuva 2: Kuvassa on <code>PythonNN</code>-verkon <code>__init__</code>- ja <code>forward</code>-metodit ilman vektorointia. Koodissa on paljon rautakoodattua toistoa.</p> <p>Esimerkki on tarkoituksella j\u00e4tetty t\u00e4ysin rautakoodatuksi. Ongelmaa voisi koodin yll\u00e4pidett\u00e4vyyden ja uudelleenk\u00e4ytett\u00e4vyyden kannalta parantaa k\u00e4ytt\u00e4m\u00e4ll\u00e4 silmukoita ja listoja. Koodista tulisi rakenteeltaan dynaamisempaa, mutta se suoritettaisiin yh\u00e4 yksi kerta per sy\u00f6te, yksi paino kerrallaan - eli siis sekventiaalisesti.</p> <p>T\u00e4t\u00e4 ongelmaa ratkaistaan vektoroinnilla. Numpy</p>"},{"location":"tensorit/vektorointi/#silmukka-toisen-peraan","title":"Silmukka toisen per\u00e4\u00e4n","text":"<p>Jos k\u00e4yt\u00f6ss\u00e4 ei ole mink\u00e4\u00e4n sortin vektorointia, koodia loopataan n\u00e4in:</p> <pre><code>EPOCHS = 10_000\ndataset = [(1, 0), (0, 1), (1, 1), (0, 0)]  # XOR dataset\nmodel = PythonNN()\n\nfor epoch in range(EPOCHS):\n    for x1, x2 in dataset:\n        model.forward(x1, x2)\n        model.backward(x1, x2)\n        model.update_weights(lr=0.1)\n</code></pre>"},{"location":"tensorit/vektorointi/#numpy-to-the-rescue","title":"Numpy to the rescue","text":"<p>Huomaa, ett\u00e4 Numpy-vektorointi tiivist\u00e4\u00e4 saman koodin hyvinkin lyhyeksi. Jos j\u00e4tet\u00e4\u00e4n pari yksityiskohtaa pois, <code>forward()</code>-metodin koodi n\u00e4ytt\u00e4\u00e4 t\u00e4lt\u00e4:</p> <pre><code>def forward(self, x):\n\n    # Inputs -&gt; Hiddens\n    Z1 = self.A0.dot(self.W0) + self.b0\n    self.A1 = self.sigmoid(Z1)\n\n    # Hiddens -&gt; Output\n    Z2 = self.A1.dot(self.W1) + self.b1\n    self.A2 = self.sigmoid(Z2)\n    return self.A2\n</code></pre> <p>Numpy ei kuitenkaan pelk\u00e4st\u00e4\u00e4n typist\u00e4 syntaksia lyhyemm\u00e4ksi. Se my\u00f6s suorittaa laskennan paljon tehokkaammin. T\u00e4m\u00e4 johtuu siit\u00e4, ett\u00e4 Numpy on ohjelmoitu C-kielell\u00e4, mik\u00e4 mahdollistaa C array-rakenteen k\u00e4yt\u00f6n ep\u00e4tehokkaan Python listan sijasta <sup>1</sup>. Lis\u00e4ksi Numpy hy\u00f6dynt\u00e4\u00e4 SIMD (Single Instruction, Multiple Data) -laskentaa, joka mahdollistaa useiden arvojen k\u00e4sittelyn yhdell\u00e4 k\u00e4skyll\u00e4 <sup>2</sup>. Eli siis yksitt\u00e4inen k\u00e4sky (single instruction) voidaan suorittaa yht\u00e4aikaisesti rinnakkaisesti usealle datalle (multiple data).</p> <p>My\u00f6hemmin kurssilla k\u00e4ytett\u00e4v\u00e4t TensorFlow ja PyTorch viev\u00e4t t\u00e4m\u00e4n viel\u00e4 askeleen pidemm\u00e4lle hy\u00f6dynt\u00e4m\u00e4ll\u00e4 GPU:ita, jotka on suunniteltu erityisesti rinnakkaislaskentaan. Esimerkiksi CUDA on SIMT (Single Instruction, Multiple Threads) -arkkitehtuuri, joka mahdollistaa tuhansien s\u00e4ikeiden samanaikaisen suorittamisen. Jos k\u00e4sitteet SISD, MDSI, SIMD ja MIMD eiv\u00e4t ole entuudestaan tuttuja, kannattaa pikaisesti tutustus Flynnin luokittelu-Wiki-artikkeliin. SIMT on lis\u00e4ys t\u00e4h\u00e4n luokitteluun.</p> <p>Neuroverkkojen kanssa s\u00e4\u00e4st\u00f6 on suuri, koska verkon koulutuksessa tehd\u00e4\u00e4n useita kertoja $ W \\cdot X + b $ -tyyppisi\u00e4 laskuja. N\u00e4m\u00e4 ovat juuri niit\u00e4 laskuja, jotka hy\u00f6tyv\u00e4t vektoroinnista ja rinnakkaislaskennasta. Pelk\u00e4ss\u00e4 <code>forward()</code>-metodissa on yksi t\u00e4llainen operaatio per kerros. Ilman SIMD-laskentaa t\u00e4m\u00e4 pistetulo suoritettaisiin yksi parametri kerrallaan. Kuinka monta kertaa siis? T\u00e4ss\u00e4 meid\u00e4n 2-2-1 verkossamme koulutettavia parametreja on yhteens\u00e4 9 (6 painoa ja 3 biasia). Muistellaan vertaiun vuoksi meid\u00e4n aiempaa MNIST-malliamme, jossa oli 784 sy\u00f6tett\u00e4, 128 piilotettua solmua ja 10 ulostuloa. Yhteens\u00e4 parametreja on:</p> Layer item Shape Count fc1.weight (256, 784) 200,704 fc1.bias (256,) 256 fc2.weight (128, 256) 32,768 fc2.bias (128,) 128 fc3.weight (10, 128) 1,280 fc3.bias (10,) 10 Total 235,146"},{"location":"tensorit/vektorointi/#lyhyet-opit","title":"Lyhyet opit","text":"<p>\u00c4l\u00e4 koskaan sekoita <code>for</code>-silmukoita ja Python-natiiveja ei-vektoroituja funktioita hajautetun tai vektoroidun koodin kanssa sekaisin. Pythonin <code>for</code>-silmukkaa voi k\u00e4ytt\u00e4\u00e4 ajurina (engl. driver), kuten vaikkapa epookkien tai erien (engl. batch) l\u00e4pik\u00e4ymiseen, mutta \u00e4l\u00e4 koskaan k\u00e4yt\u00e4 sit\u00e4 datan l\u00e4pik\u00e4ymiseen. K\u00e4yt\u00e4 sen sijaan vektoroituja funktioita ja operaatioita.</p> <p>Seuraavassa osassa t\u00e4t\u00e4 kurssia opit PyTorchin ja siihen liittyvien aputy\u00f6kalujen kuten Datasetin ja Dataloaderin k\u00e4yt\u00f6n. N\u00e4m\u00e4 ovat tehokkaita ty\u00f6kaluja: k\u00e4yt\u00e4 niit\u00e4 hyv\u00e4ksesi.</p>"},{"location":"tensorit/vektorointi/#tehtava","title":"Teht\u00e4v\u00e4","text":"<p>Teht\u00e4v\u00e4: Tutustu vektorointiin</p> <p>Katso Vectorization in PYTHON by Prof. Andrew NG -video. Videolla AI-kontekstissa hyvinkin tunnettu tekij\u00e4 selitt\u00e4\u00e4 vektoroinnin perusteet ja havainnollistaa, miten vektorointi nopeuttaa laskentaa. Kaikki 8 minuutissa.</p> <p>Teht\u00e4v\u00e4: Tutustu NumpyNNwithBCE -malliin</p> <p>Avaa <code>200_numpy_nn.ipynb</code>-tiedosto ja tutustu <code>NumpyNNwithBCE</code>-malliin. Aja koodi ja tutki mit\u00e4 tapahtuu. Varmista, ett\u00e4 ymm\u00e4rr\u00e4t, kuinka mik\u00e4kin rivi koodia liittyy t\u00e4h\u00e4n menness\u00e4 kurssilla opittuun.</p> <p>P.S. Voit j\u00e4tt\u00e4\u00e4 <code>backward()</code>-metodin pienemm\u00e4lle huomiolle. Tutustumme my\u00f6hemm\u00e4ll\u00e4 luennolla vastavirta-algoritmiin (engl. backpropagation), joka on <code>backward()</code>-metodin ydin.</p>"},{"location":"tensorit/vektorointi/#lahteet","title":"L\u00e4hteet","text":"<ol> <li> <p>VanderPlas, J. Why is Python slow? https://jakevdp.github.io/blog/2014/05/09/why-python-is-slow/\u00a0\u21a9</p> </li> <li> <p>Numpy. Why is NumPy fast? https://numpy.org/doc/stable/user/whatisnumpy.html#why-is-numpy-fast\u00a0\u21a9</p> </li> </ol>"},{"location":"vastavirta/backpropagation/","title":"Vastavirta (Backprop)","text":"<p>TODO! T\u00e4h\u00e4n tulee ainakin seuraavat asiat:</p> <ul> <li>Mik\u00e4 on backpropagation</li> <li>Miten backpropagation toimii (perusasiat)</li> <li>Gradient descent</li> </ul> <p>L\u00e4hteen\u00e4 tulee olemaan p\u00e4\u00e4asiassa:</p> <ul> <li>Understanding Deep Learning e-kirja</li> <li>3Blue1Brown video Backpropagation, intuitively | Deep Learning Chapter 3</li> <li>StatQuest Neural Networks Pt. 2: Backpropagation Main Ideas</li> </ul>"}]}