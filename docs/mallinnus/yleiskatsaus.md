---
priority: 400
---

# Yleiskatsaus

## Mallinnus

T√§m√§ luku on osin koostetta aiemmista luvuista, mutta aiheisiin sukelletaan hieman syvemm√§lle. Osa termeist√§, kuten optimointifunktiot, ovat olleet vain sivulauseina. Leikkis√§sti sanottuna aihe on: *"Mist√§ on pienet ‚Äì tai suuret ‚Äì neuroverkot tehty?"* ü§ñ

Neuroverkkojen luomisen prosessista k√§ytet√§√§n termi√§ *mallinnus* (*engl. modeling, building or developing a model*). Mallinnus tarkoittaa prosessia, jossa suunnitellaan, rakennetaan ja koulutetaan neuroverkko, joka ratkaisee tietyn ongelman. Mallinnus kattaa siis kaikki vaiheet datan hankinnasta ja esik√§sittelyst√§ mallin arkkitehtuurin suunnitteluun, kouluttamiseen ja arviointiin. Keskeisin vaihe on se, kun matemaattinen funktio sovitetaan dataan, mutta se on mekaanisin ja kenties helpoin osa-alue.

### Ty√∂vaiheet lyhyesti

Aiemmin Johdatus koneoppimiseen -kurssilla tutustuit jo yleiseen koneoppimisen ty√∂nkulkuun: datan ker√§√§miseen, esik√§sittelyyn, mallin valintaan ja arviointiin. T√§ss√§ luvussa tarkennamme n√§it√§ vaiheita nimenomaan neuroverkon n√§k√∂kulmasta ‚Äì eli siihen, millainen funktio valitaan ja miten sen muotoa (arkkitehtuuria) s√§√§det√§√§n. Alla oleva taulukko on yhdistelm√§ Essential Math for AI -kirjan luvun 3 alun ty√∂nkulusta [^mathforai] sek√§ DEep Learning with Python (3rd ed) kirjan luvusta 6 [^dlwithpython]. Se kokoaa yhteen mallintamisen t√§rkeimm√§t vaiheet. Datan hankinta (tai l√§hinn√§ sen k√§ytt√∂ PyTorchissa) k√§sitell√§√§n tarkemmin seuraavassa luvussa. Datan louhintaa on k√§sitelty muilla kursseilla.


| Ty√∂vaihe                        | Kuvaus                                                                                                                              |
| ------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------- |
| Tunnista **ongelma**            | M√§√§rittele, onko kyse luokittelusta, regressiosta, generoinnista tai esimerkiksi anomalioiden havaitsemisesta.                      |
| Hanki sopiva **data**           | Varmista, ett√§ dataa on oikeanmallista, sit√§ on riitt√§v√§sti ja se on laadukasta. T√§m√§ vaihe on usein aikaa vievin.                  |
| Valitse **virhefunktio**        | Virhefunktio (eng. loss function, error function, cost function, objective function) kertoo, kuinka ohi tavoitteesta ennuste menee. |
| Luo **malli**                   | Suunnittele tai valitse hypoteesifunktio.                                                                                           |
| Valitse **optimointimenetelm√§** | Gradienttimenetelm√§ (eng. gradient descent) on keskeinen ty√∂kalu minimointiin. Se perustuu virhefunktion derivaatan laskemiseen.    |
| **Regularisoi**                 | Regularisointi tekee funktiosta tasaisemman ja v√§hent√§√§ ylisovittamista.                                                            |
| **Kouluta**                     | Optimoi eli minimoi virhe . T√§m√§ on CPU/GPU-intensiivinen *fit*-vaihe eli mallin koulutus.                                          |

## Ty√∂vaiheet yksityiskohtaisemmin

### Ongelma

> "You can‚Äôt do good work without a deep understanding of the context of what you‚Äôre doing. Why is your customer trying to solve this particular problem? What value will they derive from the solution? How will your model be used? How will it fit into your customer‚Äôs business processes? What kind of data is available or could be collected? What kind of machine learning task can be mapped to the business problem?" [^dlwithpython]
> 

T√§m√§ on Johdatus koneoppimiseen -kurssilta tuttua. Esimerkiksi logistinen regressio soveltuu luokitteluun, kun taas lineaarinen regressio on tarkoitettu jatkuvien arvojen ennustamiseen, ja k-Means soveltuu klusterointiin. Neuroverkoissa t√§m√§ muuttuu sin√§ns√§, ett√§ sama arkkitehtuuri voi soveltua monenlaisiin ongelmiin, kunhan mallin ulostulokerros ja virhefunktio valitaan oikein. My√∂hemmin kurssilla k√§sittelemme kuviin, tekstiin tai aikasarjoihin erikoistuneita arkkitehtuureita; t√§m√§n viikon tarkoituksena on tunnistaa, kuinka ==eteenp√§in kytketyn verkon (feedforward neural network)== arkkitehtuuri soveltuu erilaisiin ongelmiin.

Tulet huomaamaan, ett√§ sama Dense-kerrosten verkko soveltuu muiden muassa seuraaviin ongelmiin: regressio, bin√§√§riluokittelu, monen luokan luokittelu (joko yksi luokka tai useita luokkia kerrallaan) ja jopa ep√§varmuuden mallintaminen. Erona on vain ulostulokerros ja virhefunktio.

Ongelmaa pohtiessa on hyv√§ pyrki√§ tunnistaa my√∂s alin l√§ht√∂kohta (engl. baseline). Luokittelussa alin l√§ht√∂kohta on satunnainen arvaus: bin√§√§riluokittelussa 50 %, k-luokassa 1/k. Regressiossa alin l√§ht√∂kohta tulee tunnistaa bisneskontekstissa: kuinka tarkka ennusteen tulee olla, jotta siit√§ on hy√∂ty√§?

### Data

T√§h√§n palaamme seuraavassa luvussa. 

!!! tip

    T√§ss√§ vaiheessa on hyv√§ huomioida, ett√§ neuroverkon tulevat toimeen ns. raakadatan kanssa. Oletkin jo n√§hnyt, kuinka neuroverkko luokittelee kuvia pikseleiden intensiteettien perusteella ilman, ett√§ kuvia on erikseen muunnettu piirteiksi (kuten reunat, kulmat, muodot). Raakadata esitet√§√§n kuitenkin aina vektoroidussa numeromuodossa. My√∂s kielimallit toimivat numeroidulla datalla.

### Virhefunktio

T√§ss√§ on t√§rke√§√§ erottaa kaksi asiaa: virhefunktio ja evaluointimittarit. Virhefunktion tulee soveltua optimointiin, eli sen tulee olla derivoituva. Evaluointimittarit taas voivat olla mit√§ tahansa, kunhan ne mittaavat haluttua ominaisuutta ‚Äì mieluiten luotettavasti ja helposti tulkittavasti.

**Virhefunktio** (*engl. loss function, error function, cost function, objective function*) mittaa, kuinka hyvin malli suoriutuu annetusta teht√§v√§st√§.

**Evaluointimittarit** (*engl. evaluation metrics*) mittaavat mallin suorituskyky√§, mutta niit√§ ei k√§ytet√§ optimointiin. Esimerkiksi ROC AUC on suosittu bin√§√§riluokittelun arviointimittari, mutta sit√§ ei voi k√§ytt√§√§ virhefunktiona, koska se ei ole derivoituva. [^dlwithpython]

Alla taulukko yleisimmist√§ teht√§v√§tyypeist√§, niiden ulostuloaktivoinneista ja virhefunktioista PyTorchissa. Taulukko on kirjoitettu englanniksi, koska en l√∂yd√§ vakiintuneita suomennoksia, jotka erottaisivat toisistaan moniluokkaisen ennustuksen kaksi eri tyyppi√§ (multi-class vs. multi-label classification). Pidemmilt√§ nimilt√§√§n n√§m√§ ovat ==multiclass, single-label classification== ja ==multilabel, multi-class classification==. Ensimm√§isess√§ voi olla vain yksi luokka kerrallaan (esim. koira TAI kissa TAI lintu), kun taas j√§lkimm√§isess√§ voi olla useita luokkia samanaikaisesti (esim. koira JA kissa).

| **Task type**                     | **Output activation**                | **PyTorch loss function** | **Task type explained**   | **Human metric**             |
| --------------------------------- | ------------------------------------ | ------------------------- | ------------------------- | ---------------------------- |
| **Regression**                    | None                                 | `MSELoss`, `L1Loss`       | `0.123`                   | MAE                          |
| **Binary classification**         | None (Sigmoid) :one:                 | `BCEWithLogitsLoss`       | `a` or not                | binary accuracy, F1, ROC AUC |
| **Multi-label classification**    | None (Sigmoid) :one:                 | `BCEWithLogitsLoss`       | `a` and/or `b` and/or `c` | binary accuracy, F1, ROC AUC |
| **Multi-class classification**    | None (Softmax) :two:                 | `CrossEntropyLoss`        | `a` or `b` or `c`         | top-k accuracy, ROC AUC      |
| **Gaussian regression** :three:   | None (mean), <br>Softplus (variance) | `GaussianNLLLoss`         | mean and variance         | MAE (mean only)              |
| **Poisson count modeling** :four: | None                                 | `PoissonNLLLoss`          | `0` or `1` or `2` ‚Ä¶ `n`   | MAE                          |

!!! note "Selitykset"

    :one: Saatat n√§hd√§ yhdistelm√§n `Sigmoid + BCELoss`, mutta `BCEWithLogitsLoss` on suositeltavampi, koska se on numeerisesti vakaampi (kuten [PyTorchin dokumentaatiossa: BCEWithLogitsLoss](https://docs.pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html) todetaan.)

    :two: Saatat n√§hd√§ yhdistelm√§n `LogSoftmax + NLLLoss` moniluokkaisessa luokittelussa, mutta `CrossEntropyLoss` sis√§lt√§√§ jo molemmat vaiheet ja v√§ltt√§√§ turhan ty√∂n.

    :three: Gaussinen regressiomalli antaa kaksi lukua: arvauksen (keskiarvo) ja ep√§varmuuden (varianssi). Malli voi esimerkiksi sanoa: *"Arvioin, ett√§ l√§mp√∂tila on 21 ¬∞C, mutta ep√§varmuus on noin ¬±3 ¬∞C."* Varianssin positiivisuus voidaan varmistaa eksponentti- tai Softplus-aktivoinnilla. Softplus lis√§√§ numeerista vakautta, mutta tuo my√∂s pienen harhan tai lattian varianssiin.

    :four: Poisson-malli olettaa, ett√§ ilmi√∂ noudattaa Poisson-jakaumaa tavallisen normaalijakauman (kuten regressiossa) sijaan. Malli ennustaa ei-negatiivisen luvun: *"Annettujen piirteiden perusteella ennustan, ett√§ asiakkaita tulee 10."*


### Malli

Nyt sinulla pit√§isi olla data vektoroituna, ongelma m√§√§riteltyn√§ ja virhefunktio valittuna. Seuraava askel on hypoteesifunktion eli mallin valinta. T√§t√§ varten sinun on k√§yt√§nn√∂ss√§ pakko pilkkoa datasetti kolmeen osaan:

* Koulutusdata (training set)
* Validointidata (validation set)
* Testidata (test set)

Hypoteesifunktiolla eli mallilla on monta nime√§: *"We use the terms hypothesis function, learning function, prediction function, training function, and model interchangeably."* [^mathforai] Johdatus koneoppimiseen -kurssilla mallin valinta oli yksi vaiheista; neuroverkkojen kohdalla mallin valinta tarkoittaa arkkitehtuurin m√§√§rittely√§. K√§yt√§nn√∂ss√§ mallin kerrosten m√§√§r√§ ja muoto ovat hyperparametreja. "A deep learning model is like a sieve for data processing, made of a succession of increasingly refined data filters‚Äîthe layers [^dlwithpython]".

Watson ja Chollet kirjoittavat, ett√§ "To figure out how big a model you‚Äôll need, you must develop a model that overfits." [^dlwithpython] T√§m√§ tarkoittaa, ett√§ et k√§yt√§nn√∂ss√§ voi etuk√§teen tiet√§√§, mink√§ kokoinen malli soveltuu teht√§v√§√§n. Aloita pienest√§ ja skaalaa sit√§ suuremmaksi kunnes l√∂yd√§t ylisovittamisen rajan. T√§m√§n j√§lkeen voit k√§ytt√§√§ regularisointia ja etsi√§ tasapainoa alisovittamisen ja ylisovittamisen v√§lill√§. Malli monimutkaisuus m√§√§rittyy seuraavilla:

* Kerrosten lukum√§√§r√§
* Kunkin kerroksen solmujen (neuroneiden) lukum√§√§r√§
* Epookkien lukum√§√§r√§

Lis√§ksi voit muuttaa esimerkiksi aktivointifunktioita, optimointimenetelmi√§ ja oppimisnopeutta (learning rate). N√§ihin kuitenkin l√∂ytyy yleens√§ hyv√§t oletusarvot, joilla p√§√§see pitk√§lle. Tutustumme n√§ihin tarkemmin [Kouluttamisen k√§yt√§nn√∂t](kaytannot.md)-luvussa.

### Regularisointi

T√§h√§n palataan [Kouluttamisen k√§yt√§nn√∂t](kaytannot.md)-luvussa.

### Koulutus

Mallin varsinainen koulutus tapahtuu optimointimenetelm√§ll√§, joka minimoi virhefunktion arvon. Yleisin menetelm√§ on gradienttimenetelm√§ (engl. gradient descent). Gradienttimenetelm√§ perustuu virhefunktion derivaatan laskemiseen ja sen hy√∂dynt√§miseen mallin painojen p√§ivitt√§misess√§ siten, ett√§ virhe pienenee jokaisella askeleella. T√§m√§n pit√§isi olla sinulle jo tuttua.

Muutoin t√§m√§ vaihe on jossain m√§√§rin mekaaninen. K√§yt√§t valmista kirjastoa, GPU alkaa tuottaa l√§mp√∂√§ ja sin√§ menet kahville. ‚òï

## Teht√§v√§t

!!! question "Teht√§v√§: Mallinna Fashion MNIST"

    Kouluta yksinkertainen neuroverkko [FashionMNIST](https://docs.pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html) -datasetill√§. Datasetti on siit√§ tuttu, ett√§ se noudattaa MNIST:n tavoin samaista rakennetta: 28x28 pikselin harmaas√§vykuvat, joissa on vaatekappaleita luokiteltavana. Luokkia on 10 ja kyseess√§ on *multi-class, single-label classification* -ongelma. Datasetti l√∂ytyy suoraan `torchvision.datasets`-moduulista jaettuna koulutus- ja testidataseteiksi. Niiss√§ on yhteens√§ 70 000 kuvaa (60 000 koulutukseen, 10 000 testaukseen). T√§m√§ kaikki on hyvin, hyvin tuttua MNIST:st√§. Se, mik√§ muuttuu, on ett√§ kuvat ovat vaikeampia tulkita kuin numerot (sandaali vs. lenkkari on vaikeampi erottaa kuin 3 vs. 7). Jos k√§yt√§t samaa 784-256-128-10 -arkkitehtuuria kuin aiempi MLP, ja Sigmoid-kerroksia, koulutus kest√§√§ jotakuinkin yht√§ kauan kuin edellinenkin. Saavutettu tarkkuus tulee olemaan heikompi (n. 85 %) sadalla epookilla. 

    Huomaa, ett√§ *"oikea vastaus"* on v√§hemm√§n merkityksellinen kuin sinun oppimismatka ja ymm√§rryksen syventyminen. Kokeile erilaisia arkkitehtuureita, optimointimenetelmi√§ ja hyperparametreja. Dokumentoi oppimiskokemuksesi. Oikean vastauksen sinulle antaa tuore kielimalli hetkess√§, tai voit jopa kaivaa sen netist√§, kuten [PyTorch Learn the Basics: Datasets & Dataloaders](https://docs.pytorch.org/tutorials/beginner/basics/data_tutorial.html) tai AI and ML for Coders in PyTorch -kirjan [repositoriosta](https://github.com/lmoroney/PyTorch-Book-FIles/blob/main/Chapter02/PyTorchChapter2.ipynb). Huijaamalla huijaat l√§hinn√§ vain itse√§si.

    √Ñl√§ siis huijaa vaan ota ensimm√§isen viikon `110_first_model.py`-tiedosto esimerkiksi. Voit k√§ytt√§√§ pohjana `400_fashion_mnist.py`-tiedostoa tai luoda kokonaan oman Notebookisi. 
    
    * Aloita pienest√§ mallista ja kasvata sit√§, kunnes l√∂yd√§t ylisovittamisen rajan.
    * K√§yt√§ Tensorboardia prosessin seurantaan ja dokumentointiin.
    * Tallenna malli seuraavaa teht√§v√§√§ varten!

    Kun tallennat arvot, tallenna mukana my√∂s mallin arkkitehtuuriin liittyv√§t tiedot, joiden avulla saat alustettua t√§ysin saman mallin uusiksi.

    !!! tip "Mallin tallennus"

        Tallenna malli PyTorchin `torch.save()`-funktiolla. Tallenna mukaan my√∂s mallin arkkitehtuuriin liittyv√§t tiedot, kuten kerrosten lukum√§√§r√§ ja solmujen lukum√§√§r√§ kussakin kerroksessa. N√§in voit alustaa t√§ysin saman mallin uudelleen lataamisen yhteydess√§. T√§m√§n voi tehd√§ monin eri tavoin, mutta alla on yksi esimerkki:

        ```python
        model_checkpoint = {
            'input_size': 784,
            'output_size': 10,
            'hidden_layers': [256, 128, 64],
            'state_dict': model.state_dict()
        }

        torch.save(model_checkpoint, 'mnist_mlp_checkpoint.pth')
        ```

        Jos haluat haastaa itse√§si, kokeile k√§√§nt√§√§ malli TorchScript-muotoon `torch.jit.script()`-funktiolla. L√∂yd√§t t√§h√§n hyv√§n esimerkin **Hands-On Machine Learning with Scikit-Learn and PyTorch** -kirjan luvun 10 lopusta otsikon *"Compiling and Optimizing a PyTorch Model"* alta.

        !!! tip "Relu vs. Sigmoid"

            Kannattaa kokeilla ainakin ReLU-aktivointia Sigmoidin sijasta, mink√§ jo itsess√§√§n pit√§isi nostaa tarkkuutta muutaman prosenttiyksik√∂n verran.

!!! question "Teht√§v√§: Lataa Fashion MNIST -malli"

    K√§yt√§ edellisen teht√§v√§n tallennettua mallia ennustamaan luokkia Fashion MNIST -datasetin testidatalle. T√§m√§n Notebookin kanssa sinulla ei ole sit√§ ongelmaa, ett√§ vahingossa ajaisit useita minuutteja viev√§n solun uudestaan. Toisin sanoen t√§ss√§ sinun on helppo harjoitella seuraavia asioita:

    * Mallin lataaminen tiedostosta.
    * Mallin k√§ytt√§minen ennustamiseen.
    * Ennusteiden evaluointi sopivilla mittareilla (kuten top-k accuracy).
    * Output-kerroksen numeroiden tutkiminen
        * Aktivaatio on None, joten ulostulo on raakaa logit-arvoa.
        * Tulosta arvot sin√§ll√§√§n ja todenn√§k√∂isyydet (softmaxin avulla).

    Voit k√§ytt√§√§ pohjana `401_fashion_mnist_eval.py`-tiedostoa tai luoda kokonaan oman Notebookisi.

!!! question "Teht√§v√§: Tutustu aktivointifunktioihin"

    Aja Marimo Notebook `413_activation_functions.ipynb` ja tutustu eri aktivointifunktioihin.

## L√§hteet

[^mathforai]: Nelson, H. *Essential Math for AI*. O'Reilly Media. 2023.
[^dlwithpython]: Watson, M & Chollet, F. *Deep Learning with Python, Third Edition*. Manning. 2025.